# Generated on: 2025-12-08 12:19:24
# by: combine_files.py

----------------------------------PROJECT TREE----------------------------------
project-root/
├── templates/
│   ├── base.html
│   ├── index.html
│   └── includes/
│       ├── toolbar.html
│       ├── upload_card.html
│       ├── xtts_row.html
│       ├── ace_step_row.html
│       ├── chatbot_row.html
│       ├── fish_row.html
│       ├── kokoro_row.html
│       ├── production_row.html
│       └── stable_audio_row.html
├── venv/
├── voices/
├── audio_post.py
├── audio_post_FISH.py
├── audio_post_KOKORO.py
├── audio_post_XTTS.py
├── config.py
├── logger.py
├── main.py
├── ace_deps/
├── ACE-Step/
├── bin/
│   ├── ffmpeg
│   ├── rubberband
│   └── espeak-ng
├── brain/
│   ├── context_history
│   ├── chat_mode.txt
│   └── system_prompt.json
├── fish-speech/
├── models/
│   ├── medium.en.pt
│   ├── base.en.pt
│   ├── __pycache__/
│   ├── kokoro-82m/
│   ├── transformers/
│   ├── safetensors/
│   ├── tokenizers/
│   ├── fish-speech-1.5/
│   ├── fish-speech/
│   ├── clap-htsat-unfused/
│   ├── ace_step/
│   ├── stable-audio-open-1.0/
│   ├── XTTS-v2/
│   ├── kokoro.py
│   ├── fish.py
│   ├── xtts.py
│   ├── whisper.py
│   ├── __init__.py
│   ├── lmstudio.py
│   ├── llama.py
│   ├── ace_generate.py
│   ├── ace_step_loader.py
│   ├── stable_audio.py
│   ├── openrouter.py
│   ├── clap.py
│   └── stable_audio_state.py
├── output_tts/
├── projects_output/
├── routes/
│   ├── infer_kokoro.py
│   ├── infer_xtts.py
│   ├── lmstudio.py
│   ├── model.py
│   ├── openrouter.py
│   ├── production.py
│   ├── settings_manager.py
│   ├── stable_audio.py
│   ├── static.py
│   ├── voice.py
│   ├── voice_transcribe.py
│   ├── __init__.py
│   ├── ace_step.py
│   ├── admin.py
│   ├── api_docs.py
│   ├── chatbot.py
│   └── infer_fish.py
├── settings/
├── static/
│   ├── favicon.ico
│   ├── css/
│   │   ├── style.css
│   │   └── bootstrap.min.css
│   ├── icons/
│   └── js/
│       ├── production.js
│       ├── app.js
│       ├── bootstrap.bundle.min.js
│       └── modules/
│           ├── backend-lmstudio.js
│           ├── chatbot.js
│           ├── backend-local.js
│           ├── backend-openrouter.js
│           ├── model-kokoro.js
│           ├── model-fish.js
│           ├── model-ace-step.js
│           ├── model-whisper.js
│           ├── model-xtts.js
│           ├── model-stable-audio.js
│           ├── upload.js
│           ├── generate-ace-step.js
│           ├── generate-stable-audio.js
│           ├── generate-kokoro.js
│           ├── generate-fish.js
│           ├── generate-xtts.js
│           ├── ui-helpers.js
│           ├── settings.js
│           └── chatbot-core.js 
    
Special instructions: When sending the user updated code, always send full code, or
at least the full methods or functions. Don't send small confusing to place snippets
that never work hoenstly. Full code.

--------------------------------------------------------------------------------

FILE: E:\tts_0\audio_post_FISH.py

================================================================================

# audio_post_FISH.py

import os
import json
import time
import numpy as np
import soundfile as sf
import pyloudnorm as pyln
from pydub import AudioSegment
from pydub.silence import detect_silence
import pyrubberband as pyrb
import noisereduce as nr
from scipy.signal import butter, sosfiltfilt, hilbert
from scipy.ndimage import gaussian_filter1d
from difflib import SequenceMatcher
import whisper
from pathlib import Path
from config import (
    FISH_CLIPPING_THRESHOLD, FISH_TARGET_LUFS,
    FISH_TRIM_DB, FISH_MIN_SILENCE,
    FISH_FRONT_PROTECT, FISH_END_PROTECT
)
from text_utils import sanitize_for_whisper, prepare_xtts_text

import models.whisper as whisper_mod


def _ts():
    return time.strftime("%H:%M:%S")

def _apply_de_esser(data: np.ndarray, rate: int, strength: float = 0.0) -> np.ndarray:
    print(f"[{_ts()} FISH_POST] Starting de-esser with strength={strength:.2f}, rate={rate} Hz, data shape={data.shape}")
    if strength <= 0.0:
        print(f"[{_ts()} FISH_POST] De-esser skipped (strength=0)")
        return data
    strength = min(1.0, max(0.0, strength))
    print(f"[{_ts()} FISH_POST] De-esser strength clamped to {strength:.2f}")

    cutoff = 3000
    print(f"[{_ts()} FISH_POST] Applying high-pass filter at {cutoff} Hz")
    sos_high = butter(4, cutoff, 'high', fs=rate, output='sos')
    high = sosfiltfilt(sos_high, data)
    print(f"[{_ts()} FISH_POST] High-pass applied, high shape={high.shape}")

    print(f"[{_ts()} FISH_POST] Computing envelope")
    env = np.abs(hilbert(high))
    sigma = (rate * 5 / 1000) / 2.355
    print(f"[{_ts()} FISH_POST] Gaussian sigma={sigma:.3f}")
    env = gaussian_filter1d(env, sigma)
    print(f"[{_ts()} FISH_POST] Envelope shape={env.shape}")

    print(f"[{_ts()} FISH_POST] Computing gain reduction (thresh=-20dB, ratio=4:1)")
    env_db = 20 * np.log10(env + 1e-10)
    gain_db = np.where(env_db > -20, (env_db + 20) * (1/4 - 1), 0.0)
    gain = 10 ** (gain_db / 20.0)
    print(f"[{_ts()} FISH_POST] Gain min={np.min(gain):.3f}, max={np.max(gain):.3f}")

    high_compressed = high * gain
    print(f"[{_ts()} FISH_POST] Compressed high shape={high_compressed.shape}")

    print(f"[{_ts()} FISH_POST] Applying low-pass at {cutoff} Hz")
    sos_low = butter(4, cutoff, 'low', fs=rate, output='sos')
    low = sosfiltfilt(sos_low, data)
    print(f"[{_ts()} FISH_POST] Low-pass applied, low shape={low.shape}")

    out = (1 - strength) * data + strength * (low + high_compressed)
    print(f"[{_ts()} FISH_POST] De-esser complete, output shape={out.shape}")
    return out

def _trim_silence_fish(wav_path: str):
    print(f"[{_ts()} FISH_POST] Starting trim on {wav_path}")
    print(f"[{_ts()} FISH_POST] Params: thresh={FISH_TRIM_DB}dB, min_sil={FISH_MIN_SILENCE}ms, front_protect={FISH_FRONT_PROTECT}ms, end_protect={FISH_END_PROTECT}ms")
    try:
        audio = AudioSegment.from_wav(wav_path)
        print(f"[{_ts()} FISH_POST] Loaded audio length={len(audio)}ms, rate={audio.frame_rate}, channels={audio.channels}")
    except Exception as e:
        print(f"[{_ts()} FISH_POST] FAILED load audio {wav_path}: {e}")
        return

    print(f"[{_ts()} FISH_POST] Detecting silence...")
    sil = detect_silence(audio, min_silence_len=FISH_MIN_SILENCE, silence_thresh=FISH_TRIM_DB)
    print(f"[{_ts()} FISH_POST] Detected {len(sil)} silence segments")

    start_trim = 0
    if sil and sil[0][0] == 0:
        front_ms = sil[0][1]
        start_trim = max(0, front_ms - FISH_FRONT_PROTECT)
        print(f"[{_ts()} FISH_POST] Front silence {front_ms}ms → trim {start_trim}ms")

    end_trim = 0
    if sil and sil[-1][1] == len(audio):
        tail_ms = len(audio) - sil[-1][0]
        end_trim = max(0, tail_ms - FISH_END_PROTECT)
        print(f"[{_ts()} FISH_POST] End silence {tail_ms}ms → trim {end_trim}ms")

    if start_trim or end_trim:
        print(f"[{_ts()} FISH_POST] Trimming start={start_trim}ms, end={end_trim}ms")
        trimmed = audio[start_trim:len(audio) - end_trim]
        trimmed.export(wav_path, format="wav")
        print(f"[{_ts()} FISH_POST] Trimmed to {len(trimmed)}ms and saved")
    else:
        print(f"[{_ts()} FISH_POST] No trim needed")

def _normalize_loudness(wav_path: str):
    print(f"[{_ts()} FISH_POST] Starting loudness normalize on {wav_path}, target={FISH_TARGET_LUFS} LUFS")
    try:
        data, rate = sf.read(wav_path)
        print(f"[{_ts()} FISH_POST] Loaded data shape={data.shape}, rate={rate} Hz")
    except Exception as e:
        print(f"[{_ts()} FISH_POST] FAILED load {wav_path}: {e}")
        return

    print(f"[{_ts()} FISH_POST] Creating meter")
    meter = pyln.Meter(rate)
    print(f"[{_ts()} FISH_POST] Measuring loudness...")
    loudness = meter.integrated_loudness(data)
    print(f"[{_ts()} FISH_POST] Measured loudness={loudness:.2f} LUFS")

    print(f"[{_ts()} FISH_POST] Normalizing...")
    normalized = pyln.normalize.loudness(data, loudness, FISH_TARGET_LUFS)
    print(f"[{_ts()} FISH_POST] Normalized shape={normalized.shape}")

    print(f"[{_ts()} FISH_POST] Saving normalized audio")
    sf.write(wav_path, normalized, rate, subtype="PCM_16")
    print(f"[{_ts()} FISH_POST] Normalize complete")

def _adjust_tempo(data: np.ndarray, rate: int, speed: float) -> np.ndarray:
    print(f"[{_ts()} FISH_POST] Starting tempo adjust with speed={speed}, rate={rate} Hz, data shape={data.shape}")
    if abs(speed - 1.0) < 1e-6:
        print(f"[{_ts()} FISH_POST] Tempo unchanged (speed=1.0)")
        return data
    try:
        print(f"[{_ts()} FISH_POST] Applying time stretch...")
        stretched = pyrb.time_stretch(data, rate, speed)
        print(f"[{_ts()} FISH_POST] Tempo adjust complete, new shape={stretched.shape}")
        return stretched
    except Exception as e:
        print(f"[{_ts()} FISH_POST] Tempo adjust FAILED: {e}")
        return data

def verify_with_whisper(
    wav_path: str,
    original_text: str,
    language: str = "en",
    tolerance: float = 80.0,
    job_file: Path = None,
    chunk_idx: int = None,
) -> bool:
    print(f"[{_ts()} FISH_WHISPER] Verifying chunk: {Path(wav_path).name}")

    if whisper_mod.whisper_model is None:
        return True

    try:
        data, _ = sf.read(wav_path)
        if np.max(np.abs(data)) > FISH_CLIPPING_THRESHOLD + 1e-10:
            return False
    except:
        return False

    audio = whisper.load_audio(wav_path)
    result = whisper_mod.whisper_model.transcribe(audio, language=language, fp16=False, word_timestamps=False)
    transcribed = result["text"].strip()

    sim = SequenceMatcher(None,
                         sanitize_for_whisper(original_text).split(),
                         sanitize_for_whisper(transcribed).split()).ratio()
    passed = sim >= (tolerance / 100.0)

    if job_file and job_file.exists() and chunk_idx is not None:
        try:
            with open(job_file, "r+", encoding="utf-8") as f:
                j = json.load(f)
                c = j["chunks"][chunk_idx]
                c["whisper_transcript"] = transcribed
                c["verification_passed"] = passed
                c["whisper_similarity"] = round(sim, 4)
                c["processing_error"] = (
                    f"Whisper similarity {sim:.3f} < {tolerance/100:.2f}" if not passed else None
                )
                f.seek(0)
                json.dump(j, f, ensure_ascii=False, indent=2)
                f.truncate()
        except: pass

    print(f"[{_ts()} FISH_WHISPER] Expected : \"{original_text}\"")
    print(f"[{_ts()} FISH_WHISPER] Heard    : \"{transcribed}\"")
    print(f"[{_ts()} FISH_WHISPER] Similarity {sim:.4f} ≥ {tolerance/100:.2f} → {'PASS' if passed else 'FAIL'}")
    return passed



def post_process_fish(wav_path: str, speed: float = 1.0, de_reverb: float = 0.7, de_ess: float = 0.0) -> str:
    print(f"\n[{_ts()} FISH_POST] === START POST-PROCESS {wav_path} ===")
    print(f"[{_ts()} FISH_POST] Params: speed={speed:.2f}, de_reverb={de_reverb:.2f}, de_ess={de_ess:.2f}")
    if not os.path.exists(wav_path):
        print(f"[{_ts()} FISH_POST] File not found: {wav_path} → SKIP")
        return wav_path

    try:
        data, rate = sf.read(wav_path)
        print(f"[{_ts()} FISH_POST] Loaded input: shape={data.shape}, rate={rate} Hz")
    except Exception as e:
        print(f"[{_ts()} FISH_POST] FAILED load {wav_path}: {e}")
        return wav_path

    if len(data) > rate * 0.2:
        print(f"[{_ts()} FISH_POST] Starting de-reverb (clip length > 0.2s)")
        noise_clip = data[:int(rate * 0.2)]
        print(f"[{_ts()} FISH_POST] Noise clip shape={noise_clip.shape}")
        data = nr.reduce_noise(y=data, sr=rate, y_noise=noise_clip, prop_decrease=de_reverb)
        print(f"[{_ts()} FISH_POST] De-reverb complete, new shape={data.shape}")
    else:
        print(f"[{_ts()} FISH_POST] De-reverb skipped (clip too short)")

    print(f"[{_ts()} FISH_POST] Starting high-pass filter (80 Hz)")
    sos = butter(4, 80, 'high', fs=rate, output='sos')
    data = sosfiltfilt(sos, data)
    print(f"[{_ts()} FISH_POST] High-pass complete")

    data = _apply_de_esser(data, rate, de_ess)

    data = _adjust_tempo(data, rate, speed)

    print(f"[{_ts()} FISH_POST] Saving intermediate audio")
    sf.write(wav_path, data, rate, subtype="PCM_16")
    print(f"[{_ts()} FISH_POST] Intermediate saved")

    _trim_silence_fish(wav_path)

    _normalize_loudness(wav_path)

    # FINAL UNIVERSAL PEAK SAFETY — respects config, protects forever
    data, rate = sf.read(wav_path)
    peak = np.max(np.abs(data))
    if peak > FISH_CLIPPING_THRESHOLD:
        data = data * (FISH_CLIPPING_THRESHOLD / peak)
        sf.write(wav_path, data, rate, subtype="PCM_16")
        print(f"[{_ts()} FISH_POST] Peak limited {peak:.6f} → {FISH_CLIPPING_THRESHOLD} (config threshold)")
    else:
        print(f"[{_ts()} FISH_POST] Peak OK: {peak:.6f} ≤ {FISH_CLIPPING_THRESHOLD}")

    print(f"[{_ts()} FISH_POST] === POST-PROCESS COMPLETE ===\n")
    return wav_path

FILE: E:\tts_0\audio_post_KOKORO.py

================================================================================

# audio_post_KOKORO.py
"""
Kokoro-specific audio post-processing pipeline.

This module contains all functions required to clean, enhance, and finalize
raw audio generated by the Kokoro TTS model. It handles:
- De-essing
- Silence trimming with front/end protection
- Loudness normalization to EBU R128 (-23 LUFS by default)
- Final peak limiting (hard cap at 0.89)
- Optional speed adjustment and de-reverb via noisereduce
- Whisper-based transcription verification

All functions are deliberately stateless and operate directly on file paths
so they can be safely used from any inference route.
"""
# audio_post_KOKORO.py
from pathlib import Path
import os, json
import time
import numpy as np
import soundfile as sf
import pyloudnorm as pyln
from pydub import AudioSegment
from pydub.silence import detect_silence
import pyrubberband as pyrb
import noisereduce as nr
from scipy.signal import butter, sosfiltfilt, hilbert
from scipy.ndimage import gaussian_filter1d
from difflib import SequenceMatcher
import whisper
from config import (
    KOKORO_TARGET_LUFS,
    KOKORO_CLIPPING_THRESHOLD,
    KOKORO_TRIM_DB,
    KOKORO_MIN_SILENCE,
    KOKORO_FRONT_PROTECT,
    KOKORO_END_PROTECT
)
from text_utils import sanitize_for_whisper, prepare_xtts_text
import models.whisper as whisper_mod


def _ts():
    return time.strftime("%H:%M:%S")


def _apply_de_esser(data: np.ndarray, rate: int, strength: float = 0.0) -> np.ndarray:
    """Apply a frequency-selective compressor targeting sibilance (de-esser).

    Args:
        data: Audio samples as float32 numpy array (-1.0 .. 1.0).
        rate: Sample rate in Hz.
        strength: De-essing intensity (0.0 = none, 1.0 = maximum).

    Returns:
        Processed audio array with the same shape and dtype as input.
    """
    print(f"[{_ts()} KOKORO_POST] Starting de-esser with strength={strength:.2f}")
    if strength <= 0.0:
        print(f"[{_ts()} KOKORO_POST] De-esser skipped (strength=0)")
        return data
    strength = min(1.0, max(0.0, strength))

    cutoff = 3000
    sos_high = butter(4, cutoff, 'high', fs=rate, output='sos')
    high = sosfiltfilt(sos_high, data)

    env = np.abs(hilbert(high))
    sigma = (rate * 5 / 1000) / 2.355
    env = gaussian_filter1d(env, sigma)

    env_db = 20 * np.log10(env + 1e-10)
    gain_db = np.where(env_db > -20, (env_db + 20) * (1/4 - 1), 0.0)
    gain = 10 ** (gain_db / 20.0)

    high_compressed = high * gain
    sos_low = butter(4, cutoff, 'low', fs=rate, output='sos')
    low = sosfiltfilt(sos_low, data)

    out = (1 - strength) * data + strength * (low + high_compressed)
    print(f"[{_ts()} KOKORO_POST] De-esser complete")
    return out


def _trim_silence_kokoro(wav_path: str):
    """Trim leading/trailing silence from a WAV file while preserving a small protected zone.

    Uses pydub's silence detection with Kokoro-specific thresholds defined in config.

    Args:
        wav_path: Path to the WAV file (modified in-place).
    """
    print(f"[{_ts()} KOKORO_POST] Starting trim → {wav_path}")
    print(f"[{_ts()} KOKORO_POST] Params: thresh={KOKORO_TRIM_DB}dB, min_sil={KOKORO_MIN_SILENCE}ms, "
          f"front_protect={KOKORO_FRONT_PROTECT}ms, end_protect={KOKORO_END_PROTECT}ms")

    audio = AudioSegment.from_wav(wav_path)
    sil = detect_silence(audio, min_silence_len=KOKORO_MIN_SILENCE, silence_thresh=KOKORO_TRIM_DB)

    start_trim = 0
    if sil and sil[0][0] == 0:
        front_ms = sil[0][1]
        start_trim = max(0, front_ms - KOKORO_FRONT_PROTECT)
        print(f"[{_ts()} KOKORO_POST] Front silence {front_ms}ms → trim {start_trim}ms")

    end_trim = 0
    if sil and sil[-1][1] == len(audio):
        tail_ms = len(audio) - sil[-1][0]
        end_trim = max(0, tail_ms - KOKORO_END_PROTECT)
        print(f"[{_ts()} KOKORO_POST] End silence {tail_ms}ms → trim {end_trim}ms")

    if start_trim or end_trim:
        trimmed = audio[start_trim:len(audio) - end_trim]
        trimmed.export(wav_path, format="wav")
        print(f"[{_ts()} KOKORO_POST] Trimmed → {len(trimmed)}ms")
    else:
        print(f"[{_ts()} KOKORO_POST] No trim needed")


def _normalize_loudness(wav_path: str, target_lufs: float = KOKORO_TARGET_LUFS):
    """Normalize integrated loudness of a WAV file to the target LUFS value (EBU R128).

    Args:
        wav_path: Path to the WAV file (overwritten with normalized version).
        target_lufs: Desired integrated loudness in LUFS (default from config).
    """
    print(f"[{_ts()} KOKORO_POST] Normalizing loudness → target {target_lufs} LUFS")
    data, rate = sf.read(wav_path)
    meter = pyln.Meter(rate)
    loudness = meter.integrated_loudness(data)
    print(f"[{_ts()} KOKORO_POST] Measured: {loudness:.2f} LUFS")
    normalized = pyln.normalize.loudness(data, loudness, target_lufs)
    sf.write(wav_path, normalized, rate, subtype="PCM_16")
    print(f"[{_ts()} KOKORO_POST] Normalized & saved")


def verify_with_whisper(
    wav_path: str,
    original_text: str,
    language: str = "en",
    tolerance: float = 80.0,
    job_file: Path = None,
    chunk_idx: int = None,
) -> bool:
    print(f"[{_ts()} KOKORO_WHISPER] Verifying chunk: {Path(wav_path).name}")

    if whisper_mod.whisper_model is None:
        return True

    try:
        data, _ = sf.read(wav_path)
        if np.max(np.abs(data)) > KOKORO_CLIPPING_THRESHOLD + 1e-10:
            print(f"[{_ts()} KOKORO_WHISPER] CLIPPED → REJECT")
            return False
    except Exception as e:
        print(f"[{_ts()} KOKORO_WHISPER] Read failed: {e}")
        return False

    audio = whisper.load_audio(wav_path)
    result = whisper_mod.whisper_model.transcribe(
        audio, language=language, fp16=False, word_timestamps=False
    )
    transcribed = result["text"].strip()

    sim = SequenceMatcher(
        None,
        sanitize_for_whisper(original_text).split(),
        sanitize_for_whisper(transcribed).split()
    ).ratio()

    passed = sim >= (tolerance / 100.0)

    if job_file and job_file.exists() and chunk_idx is not None:
        try:
            with open(job_file, "r+", encoding="utf-8") as f:
                j = json.load(f)
                c = j["chunks"][chunk_idx]
                c["whisper_transcript"] = transcribed
                c["whisper_similarity"] = round(sim, 4)
                c["verification_passed"] = passed
                c["processing_error"] = (
                    f"Whisper similarity {sim:.3f} < {tolerance/100:.2f}"
                    if not passed else None
                )
                f.seek(0)
                json.dump(j, f, ensure_ascii=False, indent=2)
                f.truncate()
        except: pass

    print(f"[{_ts()} KOKORO_WHISPER] Expected : \"{original_text}\"")
    print(f"[{_ts()} KOKORO_WHISPER] Heard    : \"{transcribed}\"")
    print(f"[{_ts()} KOKORO_WHISPER] Similarity {sim:.4f} ≥ {tolerance/100:.2f} → {'PASS' if passed else 'FAIL'}")
    return passed

def post_process_kokoro(wav_path: str, speed: float = 1.0, de_reverb: float = 0.7, de_ess: float = 0.0) -> str:
    """Complete Kokoro post-processing chain applied to a raw generated WAV file.

    The file is modified in-place and the same path is returned.

    Steps performed:
    1. Optional de-re − noisereduce using first 200 ms as noise profile
    2. High-pass filter at 80 Hz
    3. De-essing (if strength > 0)
    4. Tempo/speed adjustment via pyrubberband (if != 1.0)
    5. Silence trimming with protection zones
    6. Loudness normalization to TARGET_LUFS
    7. Final hard peak limit to 0.89 (-1 dBTP equivalent)

    Args:
        wav_path: Path to the raw WAV file.
        speed: Playback speed factor (1.0 = original).
        de_reverb: noisereduce strength (0.0 - 1.0).
        de_ess: De-esser strength (0.0 - 1.0).

    Returns:
        Path to the fully processed file (same as input).
    """
    print(f"\n[{_ts()} KOKORO_POST] === START POST-PROCESS {wav_path} ===")
    print(f"[{_ts()} KOKORO_POST] Params: speed={speed:.2f}, de_reverb={de_reverb:.2f}, de_ess={de_ess:.2f}")

    if not os.path.exists(wav_path):
        print(f"[{_ts()} KOKORO_POST] File not found → SKIP")
        return wav_path

    data, rate = sf.read(wav_path)
    print(f"[{_ts()} KOKORO_POST] Loaded: {len(data)} samples @ {rate} Hz")

    if len(data) > rate * 0.2:
        noise_clip = data[:int(rate * 0.2)]
        print(f"[{_ts()} KOKORO_POST] De-reverb (decrease={de_reverb:.2f})")
        data = nr.reduce_noise(y=data, sr=rate, y_noise=noise_clip, prop_decrease=de_reverb)
    else:
        print(f"[{_ts()} KOKORO_POST] De-reverb skipped (too short)")

    print(f"[{_ts()} KOKORO_POST] High-pass 80 Hz")
    sos = butter(4, 80, 'high', fs=rate, output='sos')
    data = sosfiltfilt(sos, data)

    data = _apply_de_esser(data, rate, de_ess)

    if abs(speed - 1.0) > 1e-6:
        print(f"[{_ts()} KOKORO_POST] Adjusting tempo ×{speed:.2f}")
        data = pyrb.time_stretch(data, rate, speed)
    else:
        print(f"[{_ts()} KOKORO_POST] Tempo unchanged")

    sf.write(wav_path, data, rate, subtype="PCM_16")
    print(f"[{_ts()} KOKORO_POST] Intermediate saved")

    _trim_silence_kokoro(wav_path)
    _normalize_loudness(wav_path)

    # FINAL: Kokoro-specific hard 0.89 cap
    data, rate = sf.read(wav_path)
    peak = np.max(np.abs(data))
    if peak > KOKORO_CLIPPING_THRESHOLD:
        data = data * (KOKORO_CLIPPING_THRESHOLD / peak)
        sf.write(wav_path, data, rate, subtype="PCM_16")
        print(f"[{_ts()} KOKORO_POST] Final amplitude scaled to 0.89 (was {peak:.5f})")
    else:
        print(f"[{_ts()} KOKORO_POST] Peak OK: {peak:.5f} ≤ 0.89")

    print(f"[{_ts()} KOKORO_POST] === POST-PROCESS COMPLETE ===\n")
    return wav_path

FILE: E:\tts_0\audio_post_XTTS.py

================================================================================

# audio_post_XTTS.py
import os
import re, json
import time
import numpy as np
import soundfile as sf
import pyloudnorm as pyln
from pydub import AudioSegment
from pydub.silence import detect_silence
import pyrubberband as pyrb
import noisereduce as nr
from scipy.signal import butter, sosfiltfilt, hilbert
from scipy.ndimage import gaussian_filter1d
from difflib import SequenceMatcher
import whisper
from pathlib import Path
from config import (
    XTTS_CLIPPING_THRESHOLD, XTTS_TARGET_LUFS, XTTS_MIN_SILENCE,
    XTTS_TRIM_DB, XTTS_FRONT_PROTECT, XTTS_END_PROTECT
)
from text_utils import sanitize_for_whisper, prepare_xtts_text
import models.whisper as whisper_mod

def _ts():
    return time.strftime("%H:%M:%S")

def _apply_de_esser(data: np.ndarray, rate: int, strength: float = 0.0) -> np.ndarray:
    """
    Classic multiband de-esser using Hilbert envelope follower on high frequencies.

    Args:
        data: Input audio (numpy float32, mono or stereo)
        rate: Sample rate in Hz
        strength: 0.0 = no effect, 1.0 = full de-essing

    Returns:
        Processed audio with reduced sibilance.
    """
    print(f"[{_ts()} XTTS_POST] Starting de-esser with strength={strength:.2f}, rate={rate} Hz, data shape={data.shape}")
    if strength <= 0.0:
        print(f"[{_ts()} XTTS_POST] De-esser skipped (strength=0)")
        return data
    strength = min(1.0, max(0.0, strength))
    print(f"[{_ts()} XTTS_POST] De-esser strength clamped to {strength:.2f}")

    cutoff = 3000
    print(f"[{_ts()} XTTS_POST] Applying high-pass filter at {cutoff} Hz")
    sos_high = butter(4, cutoff, 'high', fs=rate, output='sos')
    high = sosfiltfilt(sos_high, data)
    print(f"[{_ts()} XTTS_POST] High-pass applied, high shape={high.shape}")

    print(f"[{_ts()} XTTS_POST] Computing envelope")
    env = np.abs(hilbert(high))
    sigma = (rate * 5 / 1000) / 2.355
    print(f"[{_ts()} XTTS_POST] Gaussian sigma={sigma:.3f}")
    env = gaussian_filter1d(env, sigma)
    print(f"[{_ts()} XTTS_POST] Envelope shape={env.shape}")

    print(f"[{_ts()} XTTS_POST] Computing gain reduction (thresh=-20dB, ratio=4:1)")
    env_db = 20 * np.log10(env + 1e-10)
    gain_db = np.where(env_db > -20, (env_db + 20) * (1/4 - 1), 0.0)
    gain = 10 ** (gain_db / 20.0)
    print(f"[{_ts()} XTTS_POST] Gain min={np.min(gain):.3f}, max={np.max(gain):.3f}")

    high_compressed = high * gain
    print(f"[{_ts()} XTTS_POST] Compressed high shape={high_compressed.shape}")

    print(f"[{_ts()} XTTS_POST] Applying low-pass at {cutoff} Hz")
    sos_low = butter(4, cutoff, 'low', fs=rate, output='sos')
    low = sosfiltfilt(sos_low, data)
    print(f"[{_ts()} XTTS_POST] Low-pass applied, low shape={low.shape}")

    out = (1 - strength) * data + strength * (low + high_compressed)
    print(f"[{_ts()} XTTS_POST] De-esser complete, output shape={out.shape}")
    return out

def _trim_silence_xtts(wav_path: str) -> None:
    """
    Intelligently trim leading/trailing silence while protecting natural breaths and endings.

    Uses pydub's detect_silence with configurable thresholds and protection zones
    defined in config.py (FRONT_PROTECT, END_PROTECT).
    Overwrites the original file in-place.
    """
    print(f"[{_ts()} XTTS_POST] Starting trim on {wav_path}")
    print(f"[{_ts()} XTTS_POST] Params: thresh={XTTS_TRIM_DB}dB, min_sil={XTTS_MIN_SILENCE}ms, front_protect={XTTS_FRONT_PROTECT}ms, end_protect={XTTS_END_PROTECT}ms")
    try:
        audio = AudioSegment.from_wav(wav_path)
        print(f"[{_ts()} XTTS_POST] Loaded audio length={len(audio)}ms, rate={audio.frame_rate}, channels={audio.channels}")
    except Exception as e:
        print(f"[{_ts()} XTTS_POST] FAILED load audio {wav_path}: {e}")
        return

    print(f"[{_ts()} XTTS_POST] Detecting silence...")
    sil = detect_silence(audio, min_silence_len=XTTS_MIN_SILENCE, silence_thresh=XTTS_TRIM_DB)
    print(f"[{_ts()} XTTS_POST] Detected {len(sil)} silence segments")

    start_trim = 0
    if sil and sil[0][0] == 0:
        front_ms = sil[0][1]
        start_trim = max(0, front_ms - XTTS_FRONT_PROTECT)
        print(f"[{_ts()} XTTS_POST] Front silence {front_ms}ms → trim {start_trim}ms")

    end_trim = 0
    if sil and sil[-1][1] == len(audio):
        tail_ms = len(audio) - sil[-1][0]
        end_trim = max(0, tail_ms - XTTS_END_PROTECT)
        print(f"[{_ts()} XTTS_POST] End silence {tail_ms}ms → trim {end_trim}ms")

    if start_trim or end_trim:
        print(f"[{_ts()} XTTS_POST] Trimming start={start_trim}ms, end={end_trim}ms")
        trimmed = audio[start_trim:len(audio) - end_trim]
        trimmed.export(wav_path, format="wav")
        print(f"[{_ts()} XTTS_POST] Trimmed to {len(trimmed)}ms and saved")
    else:
        print(f"[{_ts()} XTTS_POST] No trim needed")

def _normalize_loudness(wav_path: str) -> None:
    """
    Normalize integrated loudness to TARGET_LUFS (-23 LUFS) using pyloudnorm.
    Overwrites the file in-place.
    """
    print(f"[{_ts()} XTTS_POST] Starting loudness normalize on {wav_path}, target={XTTS_TARGET_LUFS} LUFS")
    try:
        data, rate = sf.read(wav_path)
        print(f"[{_ts()} XTTS_POST] Loaded data shape={data.shape}, rate={rate} Hz")
    except Exception as e:
        print(f"[{_ts()} XTTS_POST] FAILED load {wav_path}: {e}")
        return

    print(f"[{_ts()} XTTS_POST] Creating meter")
    meter = pyln.Meter(rate)
    print(f"[{_ts()} XTTS_POST] Measuring loudness...")
    loudness = meter.integrated_loudness(data)
    print(f"[{_ts()} XTTS_POST] Measured loudness={loudness:.2f} LUFS")

    print(f"[{_ts()} XTTS_POST] Normalizing...")
    normalized = pyln.normalize.loudness(data, loudness, XTTS_TARGET_LUFS)
    print(f"[{_ts()} XTTS_POST] Normalized shape={normalized.shape}")

    print(f"[{_ts()} XTTS_POST] Saving normalized audio")
    sf.write(wav_path, normalized, rate, subtype="PCM_16")
    print(f"[{_ts()} XTTS_POST] Normalize complete")

def _adjust_tempo(data: np.ndarray, rate: int, speed: float) -> np.ndarray:
    """
    Change playback speed without altering pitch using pyrubberband time-stretch.

    Args:
        data: Input audio
        rate: Sample rate
        speed: Target speed multiplier (e.g., 1.1 = 10% faster)

    Returns:
        Time-stretched audio (same pitch).
    """
    print(f"[{_ts()} XTTS_POST] Starting tempo adjust with speed={speed}, rate={rate} Hz, data shape={data.shape}")
    if abs(speed - 1.0) < 1e-6:
        print(f"[{_ts()} XTTS_POST] Tempo unchanged (speed=1.0)")
        return data
    try:
        print(f"[{_ts()} XTTS_POST] Applying time stretch...")
        stretched = pyrb.time_stretch(data, rate, speed)
        print(f"[{_ts()} XTTS_POST] Tempo adjust complete, new shape={stretched.shape}")
        return stretched
    except Exception as e:
        print(f"[{_ts()} XTTS_POST] Tempo adjust FAILED: {e}")
        return data



def verify_with_whisper(
    wav_path: str,
    original_text: str,
    language: str = "en",
    tolerance: float = 80.0,
    job_file: Path = None,
    chunk_idx: int = None,
) -> bool:
    print(f"[{_ts()} XTTS_WHISPER] Verifying chunk: {Path(wav_path).name}")

    if whisper_mod.whisper_model is None:
        print(f"[{_ts()} XTTS_WHISPER] Whisper not loaded → skip verification")
        return True

    try:
        data, _ = sf.read(wav_path)
        if np.max(np.abs(data)) > XTTS_CLIPPING_THRESHOLD + 1e-10:
            print(f"[{_ts()} WHISPER] CLIPPED → REJECT")
            return False
    except Exception as e:
        print(f"[{_ts()} XTTS_WHISPER] Failed to read audio: {e}")
        return False

    audio = whisper.load_audio(wav_path)
    result = whisper_mod.whisper_model.transcribe(
        audio,
        language=language,
        fp16=False,
        word_timestamps=False
    )
    transcribed = result["text"].strip()

    orig_san = sanitize_for_whisper(original_text)
    trans_san = sanitize_for_whisper(transcribed)
    sim = SequenceMatcher(None, orig_san.split(), trans_san.split()).ratio()
    tolerance_norm = tolerance / 100.0
    passed = sim >= tolerance_norm

    # Write whisper_transcript and result to the CORRECT chunk
    if job_file and job_file.exists() and chunk_idx is not None:
        try:
            with open(job_file, "r+", encoding="utf-8") as f:
                j = json.load(f)
                chunk = j["chunks"][chunk_idx]  # ← this is the correct index
                chunk["whisper_transcript"] = transcribed
                chunk["verification_passed"] = passed
                chunk["whisper_similarity"] = round(sim, 4)
                chunk["processing_error"] = (
                    f"Whisper similarity {sim:.3f} < {tolerance_norm:.2f}"
                    if not passed else None
                )
                f.seek(0)
                json.dump(j, f, ensure_ascii=False, indent=2)
                f.truncate()
        except Exception as e:
            print(f"[{_ts()} XTTS_WHISPER] Failed to update job.json: {e}")

    print(f"[{_ts()} XTTS_WHISPER] Expected : \"{original_text}\"")
    print(f"[{_ts()} XTTS_WHISPER] Heard    : \"{transcribed}\"")
    print(f"[{_ts()} XTTS_WHISPER] Similarity {sim:.4f} ≥ {tolerance_norm:.2f} → {'PASS' if passed else 'FAIL'}")
    return passed

def post_process_xtts(wav_path: str, speed: float = 1.0, de_reverb: float = 0.7, de_ess: float = 0.0) -> str:
    """
    Full post-processing chain for a single XTTS chunk.

    Steps (in order):
    1. De-reverb (noisereduce using first 0.2s as profile)
    2. 80 Hz high-pass
    3. De-esser
    4. Tempo/speed adjustment
    5. Silence trimming with protection zones
    6. Loudness normalization (-23 LUFS)
    7. Final peak limiting

    The file is modified in-place and the path is returned.
    """
    try:
        print(f"\n[{_ts()} XTTS_POST] === START POST-PROCESS {wav_path} ===")
        print(f"[{_ts()} XTTS_POST] Params: speed={speed:.2f}, de_reverb={de_reverb:.2f}, de_ess={de_ess:.2f}")
        if not os.path.exists(wav_path):
            print(f"[{_ts()} XTTS_POST] File not found: {wav_path} → SKIP")
            return wav_path

        try:
            data, rate = sf.read(wav_path)
            print(f"[{_ts()} XTTS_POST] Loaded input: shape={data.shape}, rate={rate} Hz")
        except Exception as e:
            print(f"[{_ts()} XTTS_POST] FAILED load {wav_path}: {e}")
            return wav_path

        if len(data) > rate * 0.2:
            print(f"[{_ts()} XTTS_POST] Starting de-reverb (clip length > 0.2s)")
            noise_clip = data[:int(rate * 0.2)]
            print(f"[{_ts()} XTTS_POST] Noise clip shape={noise_clip.shape}")
            data = nr.reduce_noise(y=data, sr=rate, y_noise=noise_clip, prop_decrease=de_reverb)
            print(f"[{_ts()} XTTS_POST] De-reverb complete, new shape={data.shape}")
        else:
            print(f"[{_ts()} XTTS_POST] De-reverb skipped (clip too short)")

        print(f"[{_ts()} XTTS_POST] Starting high-pass filter (80 Hz)")
        sos = butter(4, 80, 'high', fs=rate, output='sos')
        data = sosfiltfilt(sos, data)
        print(f"[{_ts()} XTTS_POST] High-pass complete")

        data = _apply_de_esser(data, rate, de_ess)

        data = _adjust_tempo(data, rate, speed)

        print(f"[{_ts()} XTTS_POST] Saving intermediate audio")
        sf.write(wav_path, data, rate, subtype="PCM_16")
        print(f"[{_ts()} XTTS_POST] Intermediate saved")

        _trim_silence_xtts(wav_path)

        _normalize_loudness(wav_path)

        # FINAL UNIVERSAL PEAK SAFETY — respects config, protects forever
        data, rate = sf.read(wav_path)
        peak = np.max(np.abs(data))
        if peak > XTTS_CLIPPING_THRESHOLD:
            data = data * (XTTS_CLIPPING_THRESHOLD / peak)
            sf.write(wav_path, data, rate, subtype="PCM_16")
            print(f"[{_ts()} XTTS_POST] Peak limited {peak:.6f} → {XTTS_CLIPPING_THRESHOLD} (config threshold)")
        else:
            print(f"[{_ts()} XTTS_POST] Peak OK: {peak:.6f} ≤ {XTTS_CLIPPING_THRESHOLD}")

        print(f"\n[{_ts()} XTTS_POST] === START POST-PROCESS {wav_path} ===")
        # ... everything you already have ...
        print(f"[{_ts()} XTTS_POST] === POST-PROCESS COMPLETE ===\n")
        return wav_path

    except Exception as e:
            error_msg = f"Post-processing failed: {type(e).__name__}: {e}"
            print(f"[{_ts()} XTTS_POST] {error_msg}")
            raise RuntimeError(error_msg)

FILE: E:\tts_0\config.py

================================================================================

# config.py
from pathlib import Path
import os
import torch

APP_ROOT = Path(__file__).parent.resolve()

VOICE_DIR   = APP_ROOT / "voices" # The app's directory for your referance voices
OUTPUT_DIR  = APP_ROOT / "output_tts" # The app's work directory, can get cluttered up watch out.
DELETE_OUTPUT_ON_STARTUP = True # Empty the app's work directory on each start up, stays clean

# The following bin items you may already have installed somewhere, you can connect directly here.
FFMPEG_BIN = APP_ROOT / "bin" / "ffmpeg" / "bin" # All modules need this
RUBBERBAND_BIN = APP_ROOT / "bin" / "rubberband" # This is for adjusting pitch on TTS gens that have altered speeds
ESPEAK_DIR = APP_ROOT / "bin" / "espeak-ng" # Kokoro TTS needs this


#WHISPER_PATH = APP_ROOT / "models" / "base.en.pt" # fast, ~1.5 GB VRAM - use this one for the GPU poors
WHISPER_PATH = APP_ROOT / "models" / "medium.en.pt" # best quality, ~5 GB VRAM
# WHISPER_PATH = APP_ROOT / "models" / "large-v3.pt" # ~10 GB VRAM of overkill

# XTTS v2 TTS model path here
MODEL_PATH  = APP_ROOT / "models" / "XTTS-v2"

# Fish Speech TTS paths
FISH_REPO_DIR = Path(os.getenv("FISH_REPO_DIR", APP_ROOT / "fish-speech"))
FISH_MODEL_DIR = Path(os.getenv("FISH_MODEL_DIR", APP_ROOT / "models" / "fish-speech"))
FISH_TEXT2SEM_DIR  = FISH_MODEL_DIR
FISH_DAC_CKPT      = FISH_MODEL_DIR / "codec.pth"

#Kokoro TTS model paths, MUST have ESPEAK_DIR = APP_ROOT / "bin" / "espeak-ng" connected.
KOKORO_MODEL_DIR = APP_ROOT / "models" / "kokoro-82m"
DLL_PATH = ESPEAK_DIR / "libespeak-ng.dll"
DATA_DIR = ESPEAK_DIR / "espeak-ng-data"

# LocalSoundsAPI save directory
PROJECTS_OUTPUT = APP_ROOT / "projects_output"

# The local llama cpp needs to be hardcoded to LLM_DEVICE on start up 
# to which device to go to. Can be "cpu" for CPU.
LLM_DEVICE = "0" 
# LLM_DIRECTORY wherever you keep your .gguf models. 
# I have this set to my LM Studio drive because my .ggufs are there.
LLM_DIRECTORY = r"E:\LL STUDIO" 

# This is the default API endpoint for LM Studio, change as needed.
LMSTUDIO_API_BASE = "http://127.0.0.1:1234/v1"

# path resolver for loading models, use this for universal paths, 
# Linux & MAC may need to fiddle with this here and maybe use it in the app.
def resolve_device(device_input):
    if device_input is None:
        return "cuda:0" if torch.cuda.is_available() else "cpu"

    inp = str(device_input).strip().lower()
    if inp == "cpu":
        return "cpu"

    if inp.startswith("cuda:"):
        return inp
    try:
        idx = int(inp)
        if idx >= 0 and torch.cuda.is_available():
            if idx < torch.cuda.device_count():
                return f"cuda:{idx}"
            else:
                print(f"[resolve_device] GPU {idx} out of range → using 0")
                return "cuda:0"
        return "cpu"  
    except ValueError:
        pass

    if inp.isdigit() is False and inp != "cpu":
        fallback = f"cuda:{inp}" if inp.isdigit() is False and inp != "" else "cuda:0"
        print(f"[resolve_device] Weird input '{device_input}' → forcing {fallback}")
        return fallback
    return "cuda:0" if torch.cuda.is_available() else "cpu"
    
# These following settings are for setting each TTS chunks' padding and clipping detections and repair.
# I don't think you need to adjust these, but if you hear words are being clipped or there's too
# much silence between the chunks you adjust them here.
# XTTS
XTTS_PADDING_SECONDS    = 0.5
XTTS_CLIPPING_THRESHOLD = 0.95
XTTS_TARGET_LUFS        = -23.0
XTTS_TRIM_DB            = -35
XTTS_MIN_SILENCE        = 500
XTTS_FRONT_PROTECT      = 100
XTTS_END_PROTECT        = 800
XTTS_FRONT_PAD          = 0.0
XTTS_INTER_PAUSE        = 0.25


# Fish Speech
FISH_PADDING_SECONDS    = 0.5
FISH_CLIPPING_THRESHOLD = 0.95
FISH_TARGET_LUFS        = -23.0
FISH_TRIM_DB            = -40
FISH_MIN_SILENCE        = 400
FISH_FRONT_PROTECT      = 80
FISH_END_PROTECT        = 600
FISH_FRONT_PAD          = 0.0
FISH_INTER_PAUSE        = 0.2

# Kokoro
KOKORO_PADDING_SECONDS    = 0.5
KOKORO_CLIPPING_THRESHOLD = 0.95
KOKORO_TARGET_LUFS        = -23.0
KOKORO_TRIM_DB            = -40
KOKORO_MIN_SILENCE        = 300
KOKORO_FRONT_PROTECT      = 250
KOKORO_END_PROTECT        = 1100
KOKORO_FRONT_PAD          = 0.15
KOKORO_INTER_PAUSE        = 0.3

# Kokoro – Voice-specific TARGET_LUFS cheat sheet (paste under your Kokoro settings)
# → Pick the line that matches your voice and set KOKORO_TARGET_LUFS to that value
# → Also make sure the final "audio = audio * 0.89" line is disabled for Kokoro

# KOKORO_TARGET_LUFS = -26.0   # am_onyx, af_bella, ef_onyx, ef_bella, ef_adam, ef_emma → hottest voices, use this 95% of the time
# KOKORO_TARGET_LUFS = -25.0   # am_adam, af_emma → slightly cooler but still modern, safe and loud
# KOKORO_TARGET_LUFS = -23.0   # old em_* voices only (em_alice, em_john, etc.) → rarely used in 2025

# when a chunk fails for whatever reason in a TTS job, if a chunk fails
# you can set this to try it again. 3 times is enough, if that doesn't
# work there is something wrong with that chunk and must be corrected
# manually on the job.json file in the project directory you created.
XTTS_AUTO_TRIGGER_JOB_RECOVERY_ATTEMPTS = 3
FISH_AUTO_TRIGGER_JOB_RECOVERY_ATTEMPTS = 3
KOKORO_AUTO_TRIGGER_JOB_RECOVERY_ATTEMPTS = 3

# OpenRouter key here. Visit them if you need a key, it's not free FYI
# https://openrouter.ai/
OPENROUTER_API_KEY = "sk-or-v1-[your-key-numbers]"

FILE: E:\tts_0\main.py

================================================================================

# main.py

import logging
import shutil
import warnings
from flask import Flask, send_from_directory, request
from pathlib import Path
from tools import verify_portable_tools
from routes import register_blueprints
from config import VOICE_DIR, OUTPUT_DIR, DELETE_OUTPUT_ON_STARTUP
from models.xtts import load_speakers

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", message=".*weight_norm.*")

logging.getLogger('werkzeug').setLevel(logging.ERROR)

app = Flask(__name__, template_folder="templates")

for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)

logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    handlers=[logging.StreamHandler()],
    force=True
)

register_blueprints(app)

if DELETE_OUTPUT_ON_STARTUP:
    if OUTPUT_DIR.exists():
        shutil.rmtree(OUTPUT_DIR)
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    else:
        print("DELETE_OUTPUT_ON_STARTUP = True, but directory didn't exist yet.")

VOICE_DIR.mkdir(exist_ok=True)
OUTPUT_DIR.mkdir(exist_ok=True)

load_speakers()

@app.route("/file/<path:filename>")
def serve_file(filename):
    """
    Serve files ONLY from inside
    Outside paths → 404 (player fails — expected)
    """
    rel_str = request.args.get("rel", "").strip()
    rel_str = rel_str.replace("\\", "/")

    if rel_str:
        p = Path(rel_str).resolve()
        if p.name != filename:
            return "Invalid path", 400
        if p.suffix.lower() not in {".wav", ".mp3", ".ogg", ".flac", ".m4a", ".mp4", ".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp", ".webm"}:
            return "Invalid file type", 400
    else:
        p = OUTPUT_DIR / filename

    if p.is_absolute():
        pass 
    else:
        try:
            p.relative_to(Path.cwd())
        except ValueError:
            return "File outside project", 404

    if p.is_file():
        return send_from_directory(str(p.parent), p.name)
    return "File not found", 404


if __name__ == "__main__":
    print("LocalSoundsAPI → http://127.0.0.1:5006")
    verify_portable_tools()

    app.run(
        host="0.0.0.0",
        port=5006,
        debug=False,
        threaded=True,  
        processes=1,   
        use_reloader=False 
    )

FILE: E:\tts_0\save_utils.py

================================================================================

# save_utils.py
import os
import uuid
from pathlib import Path
from config import OUTPUT_DIR

def handle_save(
    temp_path: str,
    user_path: str | None,
    prefix: str,
    *,
    always_save_fails: bool = False,
) -> tuple[str | None, str | None]:
    """
    Save a temporary file and return (final_path, saved_rel).

    Returns:
        (absolute_path, posix_ui_path) or (None, None)
    """
    if user_path is None and not always_save_fails:
        return None, None

    if user_path:
        user_p = Path(user_path).expanduser()
        if user_p.is_absolute():
            final_path = user_p.resolve()
        else:
            final_path = (Path.cwd() / user_p).resolve()
    else:
        final_path = OUTPUT_DIR / f"{prefix}_fail_{uuid.uuid4().hex}.wav"

    final_path.parent.mkdir(parents=True, exist_ok=True)

    os.replace(temp_path, str(final_path))

    try:
        rel_path = final_path.relative_to(Path.cwd())
        saved_rel = str(rel_path).replace("\\", "/")
    except ValueError:
        saved_rel = f"{final_path.drive}{final_path.as_posix()[1:]}"

    return str(final_path), saved_rel

FILE: E:\tts_0\text_utils.py

================================================================================

# text_utils.py
import re, unicodedata

def prepare_xtts_text(text: str) -> str:
    """Normalize quotes, Unicode, collapse spaces."""
    text = text.replace('“', '"').replace('”', '"').replace('‘', "'").replace('’', "'")
    text = unicodedata.normalize('NFKC', text)
    return re.sub(r'\s+', ' ', text).strip()


def split_text_xtts(text: str, max_chars: int = 250) -> list[str]:
    """
    Split text into chunks ≤ max_chars for XTTS.
    • Prefers splits on sentence endings (. ! ?), then clauses (, ; : - —), then words.
    • Merges tiny chunks (<30 chars) if possible.
    • Never cuts words.
    • Forces space after periods if missing.
    """
    min_len = 30

    print(f"\n[XTTS SPLIT] ORIGINAL TEXT ({len(text)} chars):")
    print(f"{text}")

    # --- 1. CLEAN TEXT ---
    text = prepare_xtts_text(text)
    text = re.sub(r'\.(?=[^\s])', '. ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    print(f"\n[XTTS SPLIT] CLEANED TEXT ({len(text)} chars):")
    print(f"{text}")
    # ---------------------

    if len(text) <= max_chars:
        print(f"\n[XTTS SPLIT] SINGLE CHUNK (≤ {max_chars} chars) → returning 1 chunk")
        return [text]

    # --- 2. SPLIT INTO SENTENCES ---
    sentences = re.split(r'(?<=[.!?])\s*', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    print(f"\n[XTTS SPLIT] Split into {len(sentences)} sentence(s):")
    for i, s in enumerate(sentences, 1):
        print(f"  [{i}] {len(s):3d} chars: {s}")
    # -------------------------------

    # --- 3. BUILD CHUNKS HIERARCHICALLY ---
    chunks = []
    current = ""
    for sentence in sentences:
        if len(sentence) > max_chars:
            clauses = re.split(r'(?<=[,;:\-—])\s*', sentence)
            clauses = [c.strip() for c in clauses if c.strip()]
            for clause in clauses:
                if len(clause) > max_chars:
                    words = clause.split()
                    sub = ""
                    for word in words:
                        test = (sub + " " + word).strip() if sub else word
                        if len(test) > max_chars:
                            if sub:
                                if current and len(current + " " + sub) <= max_chars:
                                    current = (current + " " + sub).strip()
                                else:
                                    if current:
                                        chunks.append(current)
                                        current = ""
                                    chunks.append(sub)
                            sub = word
                        else:
                            sub = test
                    if sub:
                        if current and len(current + " " + sub) <= max_chars:
                            current = (current + " " + sub).strip()
                        else:
                            if current:
                                chunks.append(current)
                                current = ""
                            current = sub
                else:
                    test = (current + " " + clause).strip() if current else clause
                    if len(test) > max_chars:
                        if current:
                            chunks.append(current)
                            current = ""
                        current = clause
                    else:
                        current = test
        else:
            test = (current + " " + sentence).strip() if current else sentence
            if len(test) > max_chars:
                if current:
                    chunks.append(current)
                    current = ""
                current = sentence
            else:
                current = test

    if current:
        chunks.append(current)

    print(f"\n[XTTS SPLIT] After building: {len(chunks)} chunk(s)")

    i = 0
    merges = 0
    while i < len(chunks) - 1:
        if len(chunks[i + 1]) < min_len:
            test = (chunks[i] + " " + chunks[i + 1]).strip()
            if len(test) <= max_chars:
                chunks[i] = test
                del chunks[i + 1]
                merges += 1
                continue
        i += 1
    if merges:
        print(f"[XTTS SPLIT] Merged {merges} tiny chunk(s)")

    print(f"\n[XTTS SPLIT] FINAL RESULT: {len(chunks)} chunk(s)")
    total_chars = 0
    for idx, chunk in enumerate(chunks, 1):
        total_chars += len(chunk)
        print(f"\n  === CHUNK {idx} === ({len(chunk)} chars)")
        print(f"{chunk}")
        print(f"  {'─' * 50}")
    print(f"\n[XTTS SPLIT] TOTAL CHARACTERS IN ALL CHUNKS: {total_chars}")
    return chunks

def split_text_fish(text: str, max_chars: int = 250) -> list[str]:
    """
    Split text into chunks ≤ max_chars for Fish Speech.
    • Prefers splits on sentence endings (. ! ?), then clauses (, ; : - —), then words.
    • Merges tiny chunks (<30 chars) if possible.
    • Never cuts words.
    • Forces space after periods if missing.
    """
    min_len = 30

    print(f"\n[FISH SPLIT] ORIGINAL TEXT ({len(text)} chars):")
    print(f"{text}")

    text = text.replace('“', '"').replace('”', '"').replace('‘', "'").replace('’', "'")
    text = unicodedata.normalize('NFKC', text)
    text = re.sub(r'\.(?=[^\s])', '. ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    print(f"\n[FISH SPLIT] CLEANED TEXT ({len(text)} chars):")
    print(f"{text}")

    if len(text) <= max_chars:
        print(f"\n[FISH SPLIT] SINGLE CHUNK (≤ {max_chars} chars) → returning 1 chunk")
        return [text]

    sentences = re.split(r'(?<=[.!?])\s*', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    print(f"\n[FISH SPLIT] Split into {len(sentences)} sentence(s):")
    for i, s in enumerate(sentences, 1):
        print(f"  [{i}] {len(s):3d} chars: {s}")

    chunks = []
    current = ""
    for sentence in sentences:
        if len(sentence) > max_chars:
            clauses = re.split(r'(?<=[,;:\-—])\s*', sentence)
            clauses = [c.strip() for c in clauses if c.strip()]
            for clause in clauses:
                if len(clause) > max_chars:
                    words = clause.split()
                    sub = ""
                    for word in words:
                        test = (sub + " " + word).strip() if sub else word
                        if len(test) > max_chars:
                            if sub:
                                if current and len(current + " " + sub) <= max_chars:
                                    current = (current + " " + sub).strip()
                                else:
                                    if current:
                                        chunks.append(current)
                                        current = ""
                                    chunks.append(sub)
                            sub = word
                        else:
                            sub = test
                    if sub:
                        if current and len(current + " " + sub) <= max_chars:
                            current = (current + " " + sub).strip()
                        else:
                            if current:
                                chunks.append(current)
                                current = ""
                            current = sub
                else:
                    test = (current + " " + clause).strip() if current else clause
                    if len(test) > max_chars:
                        if current:
                            chunks.append(current)
                            current = ""
                        current = clause
                    else:
                        current = test
        else:
            test = (current + " " + sentence).strip() if current else sentence
            if len(test) > max_chars:
                if current:
                    chunks.append(current)
                    current = ""
                current = sentence
            else:
                current = test

    if current:
        chunks.append(current)

    print(f"\n[FISH SPLIT] After building: {len(chunks)} chunk(s)")

    i = 0
    merges = 0
    while i < len(chunks) - 1:
        if len(chunks[i + 1]) < min_len:
            test = (chunks[i] + " " + chunks[i + 1]).strip()
            if len(test) <= max_chars:
                chunks[i] = test
                del chunks[i + 1]
                merges += 1
                continue
        i += 1
    if merges:
        print(f"[FISH SPLIT] Merged {merges} tiny chunk(s)")

    print(f"\n[FISH SPLIT] FINAL RESULT: {len(chunks)} chunk(s)")
    total_chars = 0
    for idx, chunk in enumerate(chunks, 1):
        total_chars += len(chunk)
        print(f"\n  === CHUNK {idx} === ({len(chunk)} chars)")
        print(f"{chunk}")
        print(f"  {'─' * 50}")
    print(f"\n[FISH SPLIT] TOTAL CHARACTERS IN ALL CHUNKS: {total_chars}")
    return chunks


def split_text_kokoro(text: str, max_chars: int = 500) -> list[str]:
    """
    Kokoro ≥500 chars safe → use the same aggressive hierarchical splitter
    as XTTS/Fish for maximum efficiency.
    """
    min_len = 40   

    text = text.replace('“', '"').replace('”', '"').replace('‘', "'").replace('’', "'")
    text = unicodedata.normalize('NFKC', text)
    text = re.sub(r'\.(?=[^\s])', '. ', text)  
    text = re.sub(r'\s+', ' ', text).strip()

    if len(text) <= max_chars:
        return [text]

    sentences = re.split(r'(?<=[.!?])\s*', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    chunks = []
    current = ""

    for sentence in sentences:
        if len(sentence) > max_chars:
            clauses = re.split(r'(?<=[,;:\-—])\s*', sentence)
            clauses = [c.strip() for c in clauses if c.strip()]

            for clause in clauses:
                if len(clause) > max_chars:
                    words = clause.split()
                    sub = ""
                    for word in words:
                        test = (sub + " " + word).strip() if sub else word
                        if len(test) > max_chars:
                            if sub:
                                if current and len(current + " " + sub) <= max_chars:
                                    current = (current + " " + sub).strip()
                                else:
                                    if current:
                                        chunks.append(current)
                                        current = ""
                                    chunks.append(sub)
                            sub = word
                        else:
                            sub = test
                    if sub:
                        if current and len(current + " " + sub) <= max_chars:
                            current = (current + " " + sub).strip()
                        else:
                            if current:
                                chunks.append(current)
                                current = ""
                            current = sub
                else:
                    test = (current + " " + clause).strip() if current else clause
                    if len(test) > max_chars:
                        if current:
                            chunks.append(current)
                            current = ""
                        current = clause
                    else:
                        current = test
        else:
            # normal sentence
            test = (current + " " + sentence).strip() if current else sentence
            if len(test) > max_chars:
                if current:
                    chunks.append(current)
                    current = ""
                current = sentence
            else:
                current = test

    if current:
        chunks.append(current)

    i = 0
    while i < len(chunks) - 1:
        if len(chunks[i + 1]) < min_len:
            test = (chunks[i] + " " + chunks[i + 1]).strip()
            if len(test) <= max_chars:
                chunks[i] = test
                del chunks[i + 1]
                continue            
        i += 1

    return chunks



def sanitize_for_whisper(text: str) -> str:
    """Lower-case, keep only a-z 0-9 and spaces."""
    text = text.lower()
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    return re.sub(r'\s+', ' ', text).strip()

FILE: E:\tts_0\tools.py

================================================================================

# tools.py
import os
import subprocess
from pydub import AudioSegment
from config import FFMPEG_BIN, RUBBERBAND_BIN

# ---- Portable tool paths ----
AudioSegment.ffmpeg = str(FFMPEG_BIN / "ffmpeg.exe")
AudioSegment.ffprobe = str(FFMPEG_BIN / "ffprobe.exe")

if (RUBBERBAND_BIN / "rubberband.exe").exists():
    os.environ["PATH"] = str(RUBBERBAND_BIN) + os.pathsep + os.environ.get("PATH", "")
else:
    print(f"[WARNING] rubberband.exe NOT FOUND at: {RUBBERBAND_BIN}")

# ---- Verification ----
def verify_portable_tools() -> bool:
    issues = []

    if not (FFMPEG_BIN / "ffmpeg.exe").is_file():
        issues.append(f"FFmpeg missing: {FFMPEG_BIN / 'ffmpeg.exe'}")
    if not (FFMPEG_BIN / "ffprobe.exe").is_file():
        issues.append(f"FFprobe missing: {FFMPEG_BIN / 'ffprobe.exe'}")

    rb_exe = RUBBERBAND_BIN / "rubberband.exe"
    if not rb_exe.is_file():
        issues.append(f"Rubber Band missing: {rb_exe}")
    else:
        try:
            result = subprocess.run([str(rb_exe), "--version"], capture_output=True, text=True, timeout=5)
            if result.returncode != 0:
                issues.append("Rubber Band found but not working")
        except Exception as e:
            issues.append(f"Rubber Band test failed: {e}")

    if issues:
        print("\n" + "="*60)
        print("PORTABLE TOOLS - ISSUES DETECTED:")
        for i in issues:
            print(f"  [ERROR] {i}")
        print("="*60 + "\n")
        return False
    else:
        return True

FILE: E:\tts_0\API_client.py

================================================================================

[ERROR: File not found]

FILE: E:\tts_0\audio_post.py

================================================================================

# audio_post.py
import torchaudio.transforms as T
import os, re
import time
import numpy as np
import soundfile as sf
import torch
import pyloudnorm as pyln
from pydub import AudioSegment
from pydub.silence import detect_silence
from scipy.signal import butter, sosfiltfilt
from models.clap import load_clap

def _ts():
    return time.strftime("%H:%M:%S")

def stable_post_process(wav_path, audio_mode: str = "sfx_ambient"):
    """Apply tailored post-processing to a Stable Audio raw WAV file.

    Three available modes:
        "sfx_impact"  – tight, punchy sound effects
        "sfx_ambient" – loopable ambient textures (default)
        "music"       – loud, polished music/jingle

    Processing steps:
        • High-pass filtering
        • Intelligent leading/trailing silence trimming (mode-aware protection)
        • Loudness normalization to mode-specific LUFS target
        • True-peak limiting to -0.17 dBTP
        • Very short/subtle fade-in/out (mode-dependent)

    The file is overwritten in-place with PCM_16 WAV.

    Args:
        wav_path: Path to the input/output WAV file
        audio_mode: One of "sfx_impact", "sfx_ambient", "music"

    Returns:
        str: Path to the processed file (same as input)
    """
    if not os.path.exists(wav_path):
        print(f"[{_ts()} STABLE_POST] ERROR: File not found: {wav_path}")
        return wav_path

    print(f"\n[{_ts()} STABLE_POST] === START | MODE: {audio_mode.upper()} ===")
    print(f"[{_ts()} STABLE_POST] Input file: {wav_path}")

    configs = {
        "sfx_impact": {
            "target_lufs": -18.0,
            "trim_db": -45,
            "min_silence_ms": 50,
            "protect_front_ms": 0,
            "protect_end_ms": 150,
            "highpass_hz": 100
        },
        "sfx_ambient": {
            "target_lufs": -21.0,
            "trim_db": -35,
            "min_silence_ms": 200,
            "protect_front_ms": 0,
            "protect_end_ms": 800,
            "highpass_hz": 35
        },
        "music": {
            "target_lufs": -14.0,
            "trim_db": -40,
            "min_silence_ms": 100,
            "protect_front_ms": 0,
            "protect_end_ms": 500,
            "highpass_hz": 20
        }
    }
    cfg = configs.get(audio_mode, configs["sfx_ambient"])
    print(f"[{_ts()} STABLE_POST] Using config: {cfg}")

    # === LOAD AUDIO ===
    data, rate = sf.read(wav_path)
    duration_sec = len(data) / rate
    print(f"[{_ts()} STABLE_POST] Loaded: {len(data):,} samples @ {rate}Hz → {duration_sec:.2f}s")

    # Ensure 2D array
    if data.ndim == 1:
        data = data[:, np.newaxis]
    channels = data.shape[1]

    min_samples = int(rate * 0.01)  # 10ms
    if cfg["highpass_hz"] > 0:
        if len(data) > min_samples:
            print(f"[{_ts()} STABLE_POST] Applying high-pass filter @ {cfg['highpass_hz']}Hz")
            try:
                sos = butter(2, cfg["highpass_hz"], 'high', fs=rate, output='sos')
                data = sosfiltfilt(sos, data, axis=0)
                print(f"[{_ts()} STABLE_POST] High-pass applied successfully")
            except Exception as e:
                print(f"[{_ts()} STABLE_POST] High-pass FAILED: {e}")
        else:
            print(f"[{_ts()} STABLE_POST] High-pass SKIPPED: audio too short ({len(data)} < {min_samples} samples)")
    else:
        print(f"[{_ts()} STABLE_POST] High-pass disabled in config")

    # === 2. TRIM SILENCE ===
    print(f"[{_ts()} STABLE_POST] Detecting silence (thresh={cfg['trim_db']}dB, min={cfg['min_silence_ms']}ms)")

    # Convert to int16 for pydub
    peak = np.max(np.abs(data))
    if peak > 0:
        data_norm = data / peak
    else:
        data_norm = data
    data_int = np.clip(data_norm * 32767, -32768, 32767).astype(np.int16)

    audio = AudioSegment(
        data_int.tobytes(),
        frame_rate=rate,
        sample_width=2,
        channels=channels
    )
    silences = detect_silence(
        audio,
        min_silence_len=cfg["min_silence_ms"],
        silence_thresh=cfg["trim_db"]
    )

    start_trim = 0
    end_trim = 0

    if silences:
        if silences[0][0] == 0:
            front_ms = silences[0][1]
            start_trim = max(0, front_ms - cfg["protect_front_ms"])
            print(f"[{_ts()} STABLE_POST] Leading silence: {front_ms}ms → trim {start_trim}ms")

        if silences[-1][1] == len(audio):
            tail_ms = len(audio) - silences[-1][0]
            end_trim = max(0, tail_ms - cfg["protect_end_ms"])
            print(f"[{_ts()} STABLE_POST] Trailing silence: {tail_ms}ms → trim {end_trim}ms")
    else:
        print(f"[{_ts()} STABLE_POST] No silence detected")

    if start_trim or end_trim:
        trimmed = audio[start_trim:len(audio) - end_trim]
        data_int = np.array(trimmed.get_array_of_samples(), dtype=np.int16)
        data = (data_int / 32768.0).astype(np.float32)
        if channels > 1:
            data = data.reshape(-1, channels)
        new_dur = len(data) / rate
        print(f"[{_ts()} STABLE_POST] Trimmed → {len(data):,} samples → {new_dur:.2f}s")
    else:
        print(f"[{_ts()} STABLE_POST] No trimming needed")

    # === 3. LOUDNESS NORMALIZATION ===
    print(f"[{_ts()} STABLE_POST] Measuring loudness...")
    meter = pyln.Meter(rate)
    loudness = meter.integrated_loudness(data)
    print(f"[{_ts()} STABLE_POST] Measured: {loudness:.2f} LUFS → targeting {cfg['target_lufs']} LUFS")
    data = pyln.normalize.loudness(data, loudness, cfg["target_lufs"])
    print(f"[{_ts()} STABLE_POST] Normalized to {cfg['target_lufs']} LUFS")

    # === 4. TRUE-PEAK LIMITING ===
    peak = np.max(np.abs(data))
    if peak > 0.98:
        data *= 0.98 / peak
        print(f"[{_ts()} STABLE_POST] Peak limited: {peak:.4f} → 0.98 (-0.17 dBTP)")
    else:
        print(f"[{_ts()} STABLE_POST] Peak OK: {peak:.4f} ≤ 0.98")

    # === OPTIONAL SUBTLE FADES (mode-aware) ===
    fade_in_ms  = {"music": 8,  "sfx_ambient": 5,  "sfx_impact": 0}[audio_mode]
    fade_out_ms = {"music": 50, "sfx_ambient": 300, "sfx_impact": 0}[audio_mode]

    if fade_in_ms > 0 or fade_out_ms > 0:
        samples = data.shape[0]
        if fade_in_ms > 0:
            fade_in_samples = int(rate * (fade_in_ms / 1000))
            if fade_in_samples < samples:
                ramp = np.linspace(0, 1, fade_in_samples)
                data[:fade_in_samples] *= ramp.reshape(-1, 1)  # works for mono & stereo
                print(f"[{_ts()} STABLE_POST] Applied {fade_in_ms}ms fade-in")
        
        if fade_out_ms > 0:
            fade_out_samples = int(rate * (fade_out_ms / 1000))
            if fade_out_samples < samples:
                ramp = np.linspace(1, 0, fade_out_samples)
                data[-fade_out_samples:] *= ramp.reshape(-1, 1)
                print(f"[{_ts()} STABLE_POST] Applied {fade_out_ms}ms fade-out")


    # === 5. WRITE FINAL FILE ===
    sf.write(wav_path, data, rate, subtype="PCM_16")
    final_dur = len(data) / rate
    print(f"[{_ts()} STABLE_POST] FINAL OUTPUT: {final_dur:.2f}s @ {cfg['target_lufs']} LUFS")
    print(f"[{_ts()} STABLE_POST] === DONE ===\n")
    return wav_path

def ace_post_process(wav_path: str) -> str:
    """Post-process ACE-Step raw output (always music mode).

    Steps performed in-place on the WAV file:
        • Force stereo output
        • High-pass filter at 20 Hz
        • Intelligent silence trimming (protects up to 500 ms tail)
        • Loudness normalization to -14 LUFS
        • True-peak limiting to -0.17 dBTP

    Args:
        wav_path: Path to the input/output WAV file

    Returns:
        str: Same path (file is overwritten)
    """
    
    if not os.path.exists(wav_path):
        print(f"[{_ts()} ACE_POST] ERROR: File not found: {wav_path}")
        return wav_path

    print(f"\n[{_ts()} ACE_POST] === START | MODE: MUSIC ===")
    print(f"[{_ts()} ACE_POST] Input: {wav_path}")

    cfg = {
        "target_lufs": -14.0,
        "trim_db": -40,
        "min_silence_ms": 100,
        "protect_front_ms": 0,
        "protect_end_ms": 500,
        "highpass_hz": 20
    }
    print(f"[{_ts()} ACE_POST] Using config: {cfg}")

    data, rate = sf.read(wav_path)
    duration_sec = len(data) / rate
    print(f"[{_ts()} ACE_POST] Loaded: {len(data):,} samples @ {rate}Hz → {duration_sec:.2f}s")

    # STEREO UPMIX
    if data.ndim == 1:
        data = np.stack([data, data], axis=1)
        print(f"[{_ts()} ACE_POST] Mono → Stereo upmix")
    elif data.shape[1] == 1:
        data = np.repeat(data, 2, axis=1)
    channels = data.shape[1]
    print(f"[{_ts()} ACE_POST] Channels: {channels} (stereo)")

    # HIGH-PASS
    if cfg["highpass_hz"] > 0:
        min_samples = int(rate * 0.01)
        if len(data) > min_samples:
            print(f"[{_ts()} ACE_POST] High-pass @ {cfg['highpass_hz']}Hz")
            sos = butter(2, cfg["highpass_hz"], 'high', fs=rate, output='sos')
            data = sosfiltfilt(sos, data, axis=0)

    # TRIM
    print(f"[{_ts()} ACE_POST] Detecting silence (thresh={cfg['trim_db']}dB, min={cfg['min_silence_ms']}ms)")
    peak = np.max(np.abs(data))
    data_norm = data / peak if peak > 0 else data
    data_int = np.clip(data_norm * 32767, -32768, 32767).astype(np.int16)
    audio = AudioSegment(
        data_int.tobytes(),
        frame_rate=rate,
        sample_width=2,
        channels=channels
    )
    silences = detect_silence(audio, min_silence_len=cfg["min_silence_ms"], silence_thresh=cfg["trim_db"])
    start_trim = end_trim = 0
    if silences:
        if silences[0][0] == 0:
            front_ms = silences[0][1]
            start_trim = max(0, front_ms - cfg["protect_front_ms"])
            print(f"[{_ts()} ACE_POST] Leading trim: {start_trim}ms")
        if silences[-1][1] == len(audio):
            tail_ms = len(audio) - silences[-1][0]
            end_trim = max(0, tail_ms - cfg["protect_end_ms"])
            print(f"[{_ts()} ACE_POST] Trailing trim: {end_trim}ms")
    if start_trim or end_trim:
        trimmed = audio[start_trim:len(audio) - end_trim]
        data_int = np.array(trimmed.get_array_of_samples(), dtype=np.int16)
        data = (data_int / 32768.0).astype(np.float32)
        if channels > 1:
            data = data.reshape(-1, channels)
        print(f"[{_ts()} ACE_POST] Trimmed → {len(data):,} samples")

    # LOUDNESS
    print(f"[{_ts()} ACE_POST] Normalizing to {cfg['target_lufs']} LUFS")
    meter = pyln.Meter(rate)
    loudness = meter.integrated_loudness(data)
    data = pyln.normalize.loudness(data, loudness, cfg["target_lufs"])

    # PEAK LIMIT
    peak = np.max(np.abs(data))
    if peak > 0.98:
        data *= 0.98 / peak
        print(f"[{_ts()} ACE_POST] Peak limited: {peak:.4f} → 0.98")

    # WRITE
    sf.write(wav_path, data, rate, subtype="PCM_16")
    final_dur = len(data) / rate
    print(f"[{_ts()} ACE_POST] FINAL: {final_dur:.2f}s @ {cfg['target_lufs']} LUFS (stereo)")
    print(f"[{_ts()} ACE_POST] === DONE ===\n")
    return wav_path

def score_with_clap(audio_np: np.ndarray, prompt: str, rate: int = 44100) -> float:
    """
    Score audio vs text using the globally loaded CLAP model.
    Works for both ACE-Step and Stable Audio — automatically uses correct GPU.
    """
    try:
        # Use the already-loaded CLAP (from ace_step_loader or stable audio loader)
        clap_model, clap_processor = load_clap()  # ← no argument = uses correct GPU
        device = clap_model.device

        # Resample to 48kHz if needed
        if rate != 48000:
            resampler = T.Resample(orig_freq=rate, new_freq=48000).to(device)
            audio_tensor = torch.from_numpy(audio_np.T).float().to(device)
            audio_resampled = resampler(audio_tensor).cpu().numpy()
        else:
            audio_resampled = audio_np.T

        # Process text and audio in one go — returns dict already on CPU
        inputs = clap_processor(
            text=[prompt],
            audios=audio_resampled,
            sampling_rate=48000,
            return_tensors="pt",
            padding=True
        )

        # Move only the input tensors to GPU — this is enough
        inputs = {k: v.to(device) for k, v in inputs.items() if torch.is_tensor(v)}

        with torch.no_grad():
            # Use the clean unified forward pass
            outputs = clap_model(**inputs)
            score = torch.cosine_similarity(
                outputs.text_embeds, outputs.audio_embeds
            ).mean().item()

        return float(score)

    except Exception as e:
        print(f"[{_ts()} CLAP] Scoring failed: {e}")
        import traceback
        traceback.print_exc()
        return 0.0

FILE: E:\tts_0\models\ace_generate.py

================================================================================

import sys
from pathlib import Path

# === IMPORT EXISTING PATHS FROM YOUR CONFIG (no changes to config!) ===
from config import APP_ROOT, OUTPUT_DIR

# === 1. Add ACE-Step repo to path (bundled & portable) ===
ACE_STEP_REPO = APP_ROOT / "ACE-Step"
if str(ACE_STEP_REPO) not in sys.path:
    sys.path.insert(0, str(ACE_STEP_REPO))

# === 2. Model checkpoint directory (bundled & portable) ===
MODEL_DIR = APP_ROOT / "models" / "ace_step"

# === 3. Load model once (with basic safety) ===
print(f"[ACE-STEP] Loading model from: {MODEL_DIR}")
if not MODEL_DIR.exists():
    raise FileNotFoundError(f"ACE-Step model missing! Expected: {MODEL_DIR}")


from acestep.pipeline_ace_step import ACEStepPipeline # Your IDE yelling at you about can't import, ignore it

print("[ACE-STEP] Loading model from:", MODEL_DIR)
pipe = ACEStepPipeline(
    checkpoint_dir=MODEL_DIR,
    dtype="bfloat16",
    device_id=0,
    torch_compile=False,
    cpu_offload=False,
    overlapped_decode=False
)

# === 5. Generate function (unchanged from your working code) ===
def generate(
    prompt: str,
    duration: float = 10.0,
    output: str = "output.wav",
    infer_step: int = 27,
    guidance_scale: float = 3.5,
    scheduler_type: str = "euler",
    cfg_type: str = "cfg",
    omega_scale: float = 1.0,
    manual_seeds: str = "42",
    guidance_interval: float = 0.0,
    guidance_interval_decay: float = 1.0,
    min_guidance_scale: float = 1.0,
    use_erg_tag: bool = False,
    use_erg_lyric: bool = False,
    use_erg_diffusion: bool = False,
    oss_steps: str = "",
    guidance_scale_text: float = 0.0,
    guidance_scale_lyric: float = 0.0,
    play: bool = False
):
    import os
    from pathlib import Path

    output_path = str(Path(output).resolve())
    print(f"[ACE-STEP] Generating: '{prompt}' → {output_path}")

    pipe(
        audio_duration=duration,
        prompt=prompt,
        lyrics="",
        infer_step=infer_step,
        guidance_scale=guidance_scale,
        scheduler_type=scheduler_type,
        cfg_type=cfg_type,
        omega_scale=omega_scale,
        manual_seeds=manual_seeds,
        guidance_interval=guidance_interval,
        guidance_interval_decay=guidance_interval_decay,
        min_guidance_scale=min_guidance_scale,
        use_erg_tag=use_erg_tag,
        use_erg_lyric=use_erg_lyric,
        use_erg_diffusion=use_erg_diffusion,
        oss_steps=oss_steps,
        guidance_scale_text=guidance_scale_text,
        guidance_scale_lyric=guidance_scale_lyric,
        save_path=output_path
    )

    if play and os.name == 'nt':
        os.startfile(output_path)

    return output_path

FILE: E:\tts_0\models\ace_step_loader.py

================================================================================

# models/ace_step_loader.py (updated)
import sys
from pathlib import Path
import torch
import gc
import random
import os
import shutil
from huggingface_hub import snapshot_download  # NEW: Import for manual download

# GLOBAL FIX FOR WINDOWS SYMLINKS – THIS KILLS WINERROR 1314 FOREVER
os.environ["HF_HUB_DISABLE_SYMLINKS"] = "1"
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

from config import APP_ROOT

ACE_STEP_REPO = APP_ROOT / "ACE-Step"
MODEL_DIR     = APP_ROOT / "models" / "ace_step"

if str(ACE_STEP_REPO) not in sys.path:
    sys.path.insert(0, str(ACE_STEP_REPO))

pipe = None
model_loaded = False
_current_gpu = None

# NEW: Import CLAP loader (assuming it's in the same models/ dir or adjust path)
from models.clap import load_clap, unload_clap

def load_ace(device: str = "0") -> bool:
    global pipe, model_loaded, _current_gpu

    if not torch.cuda.is_available():
        print("[ACE-LOAD] CUDA not available")
        return False

    try:
        gpu_idx = int(device)
    except:
        print("[ACE-LOAD] Invalid device")
        return False

    if gpu_idx >= torch.cuda.device_count():
        print("[ACE-LOAD] GPU index out of range")
        return False

    if model_loaded and _current_gpu != gpu_idx:
        unload_ace()

    # Updated corruption check and download
    transformer_file = MODEL_DIR / "ace_step_transformer" / "diffusion_pytorch_model.safetensors"
    if MODEL_DIR.exists() and not transformer_file.exists():
        print("[ACE] Potentially corrupted or nested folder detected → deleting once")
        shutil.rmtree(MODEL_DIR, ignore_errors=True)

    if not transformer_file.exists():
        print("[ACE-LOAD] Downloading ACE-Step model to flat structure...")
        snapshot_download(
            repo_id="ACE-Step/ACE-Step-v1-3.5B",
            local_dir=str(MODEL_DIR),
            local_dir_use_symlinks=False,
            max_workers=8,
            tqdm_class=None,
        )
        print("[ACE-LOAD] Download complete. Structure should now be flat.")

    # Check again after download
    if not transformer_file.exists():
        print("[ACE-LOAD] FAILED: Model files not found even after download.")
        return False

    try:
        from acestep.pipeline_ace_step import ACEStepPipeline
        print(f"[ACE-LOAD] Loading ACE-Step 3.5B → cuda:{gpu_idx}")
        pipe = ACEStepPipeline(
            checkpoint_dir=str(MODEL_DIR),
            dtype="bfloat16",
            device_id=gpu_idx,
            torch_compile=False,
            cpu_offload=False,
            overlapped_decode=False,
        )
        model_loaded = True
        _current_gpu = gpu_idx
        print(f"[ACE-LOAD] SUCCESS on cuda:{gpu_idx}")

        # Load CLAP on the same device after ACE succeeds
        clap_device = f"cuda:{gpu_idx}"
        load_clap(clap_device)
        return True

    except Exception as e:
        print(f"[ACE-LOAD] FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False


def unload_ace() -> None:
    global pipe, model_loaded, _current_gpu
    if pipe is not None:
        del pipe
        pipe = None
    gc.collect()
    torch.cuda.empty_cache()
    model_loaded = False
    _current_gpu = None
    print("[ACE-UNLOAD] Done")
    unload_clap()


def is_model_loaded() -> bool:
    return model_loaded

def generate(
    prompt: str,
    duration: float = 10.0,
    output: str = "output.wav",
    infer_step: int = 27,
    guidance_scale: float = 3.5,
    scheduler_type: str = "euler",
    cfg_type: str = "cfg",
    omega_scale: float = 1.0,
    manual_seeds: str = "42",
    guidance_interval: float = 1.0,
    guidance_interval_decay: float = 1.0,
    min_guidance_scale: float = 1.0,
    use_erg_tag: bool = False,
    use_erg_lyric: bool = False,
    use_erg_diffusion: bool = False,
    oss_steps: str = "",
    guidance_scale_text: float = 0.0,
    guidance_scale_lyric: float = 0.0,
    play: bool = False
):
    """Generate music with the loaded ACE-Step pipeline.

    Args:
        prompt: Full multi-line prompt (first line = style, rest = lyrics)
        duration: Target audio length in seconds
        output: Path where the final WAV will be written
        infer_step: Number of diffusion steps
        guidance_scale: Main classifier-free guidance scale
        scheduler_type: Scheduler name (euler, dpmpp_2m, etc.)
        cfg_type: CFG variant ("cfg", "self-cfg", ...)
        omega_scale: Omega CFG multiplier
        manual_seeds: Seed as string; "-1"/"random" → random seed
        guidance_interval / guidance_interval_decay: Dynamic guidance scheduling
        min_guidance_scale: Floor value for decaying guidance
        use_erg_tag / use_erg_lyric / use_erg_diffusion: ERG ablation switches
        oss_steps: One-step scheduler step schedule string
        guidance_scale_text / guidance_scale_lyric: Separate text/lyric guidance
        play: On Windows, open the file after generation

    Returns:
        str: Path to the generated WAV file
    """
    global pipe
    if not model_loaded:
        raise RuntimeError("ACE-Step model not loaded")

    lyrics = ""
    if str(manual_seeds).strip() in ("-1", "-0", "random"):
        manual_seeds = str(random.randint(0, 2**32 - 1))

    print(f"[ACE-GEN] Generating: '{prompt[:60]}...' → {output}")

    pipe(
        audio_duration=duration,
        prompt=prompt,
        lyrics=lyrics,
        infer_step=infer_step,
        guidance_scale=guidance_scale,
        scheduler_type=scheduler_type,
        cfg_type=cfg_type,
        omega_scale=omega_scale,
        manual_seeds=manual_seeds,
        guidance_interval=guidance_interval,
        guidance_interval_decay=guidance_interval_decay,
        min_guidance_scale=min_guidance_scale,
        use_erg_tag=use_erg_tag,
        use_erg_lyric=use_erg_lyric,
        use_erg_diffusion=use_erg_diffusion,
        oss_steps=oss_steps,
        guidance_scale_text=guidance_scale_text,
        guidance_scale_lyric=guidance_scale_lyric,
        save_path=output
    )
    return output

FILE: E:\tts_0\models\stable_audio.py

================================================================================

# models/stable_audio.py

import warnings
warnings.filterwarnings("ignore", message=".*Should have tb<=t1.*")
warnings.filterwarnings("ignore", message=".*Should have ta>=t0.*")
warnings.filterwarnings("ignore", message=".*Should have tb>=t0.*")
import os
import random
import torch
import soundfile as sf
import torchaudio.transforms as T
import numpy as np
from diffusers import StableAudioPipeline
from config import OUTPUT_DIR
from pathlib import Path
import gc

from .stable_audio_state import is_model_loaded, set_model_loaded, set_current_device, get_current_device

APP_ROOT = Path(__file__).parent.parent
CLAP_DIR = APP_ROOT / "models" / "clap-htsat-unfused"

pipe = None
clap_processor = None
clap_model = None
generation_active = False

def load_stable_audio(device: str = "0") -> tuple[bool, str]:
    global pipe, clap_processor, clap_model

    if not torch.cuda.is_available():
        return False, "CUDA required"

    try:
        gpu_idx = int(device)
    except ValueError:
        return False, "Device must be integer"

    if gpu_idx < 0 or gpu_idx >= torch.cuda.device_count():
        return False, f"GPU {gpu_idx} out of range"

    dev = torch.device(f"cuda:{gpu_idx}")

    if is_model_loaded() and get_current_device() != str(dev):
        unload_stable_audio()

    # Auto-download Stable Audio Open 1.0 if missing
    model_dir = APP_ROOT / "models" / "stable-audio-open-1.0"
    if not model_dir.exists() or not any(model_dir.iterdir()):
        print(f"[STABLE] Downloading stable-audio-open-1.0 → {model_dir}")
        from huggingface_hub import snapshot_download
        snapshot_download(
            repo_id="stabilityai/stable-audio-open-1.0",
            local_dir=str(model_dir),
            local_dir_use_symlinks=False,
            resume_download=True,
        )
        print("[STABLE] Stable Audio Open 1.0 downloaded")

    # Load Stable Audio pipeline
    try:
        print(f"[LOAD] Loading Stable Audio from {model_dir}...")
        pipe = StableAudioPipeline.from_pretrained(
            str(model_dir),
            torch_dtype=torch.float16,
        ).to(dev)
        print(f"[LOAD] Pipeline loaded on {dev}")
    except Exception as e:
        set_model_loaded(False)
        return False, f"Pipeline load failed: {e}"

    # Load CLAP using your own auto-downloader (from models/clap.py)
    try:
        print("[LOAD] Loading CLAP (auto-download if missing)...")
        from models.clap import load_clap
        clap_model, clap_processor = load_clap(str(dev))
        torch.cuda.empty_cache()  # Critical for 3090
        print(f"[LOAD] CLAP loaded on {dev}")
    except Exception as e:
        unload_stable_audio()
        return False, f"CLAP load failed: {e}"

    # Warm-up
    print("[LOAD] Running tiny dummy generation...")
    try:
        generate_audio(prompt="init", steps=10, length_sec=1.0, seed=0, num_waveforms_per_prompt=1)
        print("[LOAD] Model ready")
    except Exception as e:
        print(f"[LOAD] Dummy failed (non-critical): {e}")

    set_model_loaded(True)
    set_current_device(str(dev))
    return True, "Loaded"


def unload_stable_audio() -> None:
    """Unload the Stable Audio pipeline and CLAP model from GPU memory."""
    global pipe, clap_model, clap_processor

    if pipe is not None:
        del pipe
        pipe = None

    # ← THIS IS THE KEY LINE — use the shared global unloader
    if clap_model is not None or clap_processor is not None:
        from models.clap import unload_clap
        unload_clap()

    # Also clear the local references (optional but clean)
    clap_model = None
    clap_processor = None

    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()

    set_model_loaded(False)
    set_current_device(None)
    print("[UNLOAD] Stable Audio + CLAP fully unloaded.")



def cancel_generation() -> None:
    """Signal the currently running generation to stop as soon as possible.

    Works with Diffusers ≥ 0.30 by raising StopIteration inside the callback.
    """
    global generation_active
    generation_active = False

def generate_audio(
    prompt,
    steps=100,
    length_sec=10.0,
    seed=42,
    negative_prompt=None,
    guidance_scale=7.0,
    num_waveforms_per_prompt=1,
    eta=0.0,
):
    """Generate audio using Stable Audio Open 1.0.

    Args:
        prompt: Text prompt describing the desired sound.
        steps: Number of diffusion inference steps (10–200 typical).
        length_sec: Desired audio length in seconds (max ~47s for this model).
        seed: Random seed. Use -1 for a random seed.
        negative_prompt: Optional negative prompt.
        guidance_scale: Classifier-free guidance scale (higher = stricter prompt adherence).
        num_waveforms_per_prompt: Number of variant waveforms to generate per prompt (1–4).
        eta: DDIM eta parameter (0.0 = deterministic, 1.0 = full stochastic).

    Returns:
        Tuple containing:
        - List[dict]: Each dict has "audio_np" (samples × channels numpy array),
          "score" (CLAP similarity when >1 waveform), and "is_best" flag.
        - sample_rate (int): Always 44100 Hz for this model.
        - final_seed (int): The seed actually used.

    When multiple waveforms are requested the best one is selected using CLAP similarity
    scoring against the text prompt.
    """
    if pipe is None:
        raise RuntimeError("Stable Audio model not loaded. Call /stable_load32 first.")

    global generation_active
    generation_active = True

    if seed == -1:
        seed = random.randint(0, 2**32 - 1)
    generator = torch.Generator("cuda").manual_seed(seed)

    def callback(step: int, timestep: int, latents: torch.Tensor):
        if not generation_active:
            return True
        return False

    try:
        outputs = pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=steps,
            audio_end_in_s=length_sec,
            guidance_scale=guidance_scale,
            num_waveforms_per_prompt=num_waveforms_per_prompt,
            eta=eta,
            generator=generator,
            callback=callback,
            callback_steps=1,
        )
    except StopIteration:
        print("[STABLE] Generation cancelled")
        return [], 44100, seed
    except Exception as e:
        print(f"[STABLE] Generation failed: {e}")
        raise

    audios = outputs.audios 
    results = []

    if len(audios) > 1 and generation_active:
        scores = []
        resampler = T.Resample(44100, 48000, dtype=torch.float32).to(pipe.device)

        for audio in audios:
            if not generation_active:
                break
            try:
                if audio.ndim == 3:
                    audio = audio.squeeze(0) 
                if audio.ndim != 2:
                    raise ValueError(f"Unexpected audio shape: {audio.shape}")

                # Step 2: Convert stereo → mono (CLAP only accepts mono)
                if audio.shape[0] == 2:
                    audio_mono = audio.mean(dim=0, keepdim=True)  # (1, samples)
                elif audio.shape[0] == 1:
                    audio_mono = audio
                else:
                    raise ValueError(f"Unsupported channel count: {audio.shape[0]}")

                # Step 3: Resample on GPU
                audio_tensor = audio_mono.to(pipe.device, dtype=torch.float32)
                resampled = resampler(audio_tensor)              # (1, samples_resampled)
                resampled = resampled.unsqueeze(0)                # (1, 1, samples) → batch + channel

                # Step 4: Convert to exact format CLAP expects: 1D NumPy array (mono)
                audio_np = resampled.squeeze(0).cpu().numpy()     # → (1, samples)
                audio_np = audio_np.flatten()                     # → (samples,)

                # Step 5: CLAP processing (now guaranteed to work)
                text_inputs = clap_processor(text=[prompt], return_tensors="pt").to(pipe.device)
                audio_inputs = clap_processor(
                    audios=audio_np,
                    sampling_rate=48000,
                    return_tensors="pt",
                    padding=True
                ).to(pipe.device)

                with torch.no_grad():
                    text_emb = clap_model.get_text_features(**text_inputs)
                    audio_emb = clap_model.get_audio_features(**audio_inputs)
                    similarity = (text_emb @ audio_emb.T).diag().item()

                scores.append(float(similarity))

                # Clean up everything
                del audio_mono, audio_tensor, resampled, audio_np
                del text_inputs, audio_inputs, text_emb, audio_emb
                torch.cuda.empty_cache()

            except Exception as e:
                print(f"[CLAP] Scoring failed for variant: {e}")
                scores.append(0.0)
                continue

        if generation_active:
            sorted_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)
            for rank, i in enumerate(sorted_idx):
                audio_np = audios[i].cpu().float().numpy()
                if audio_np.ndim == 3:
                    audio_np = audio_np[0]  # (channels, samples)
                results.append({
                    "audio_np": audio_np.T,  # (samples, channels) for soundfile
                    "score": scores[i],
                    "is_best": rank == 0,
                })
    elif generation_active:
        audio_np = audios[0].cpu().float().numpy()
        if audio_np.ndim == 3:
            audio_np = audio_np[0]
        results.append({
            "audio_np": audio_np.T,
            "score": None,
            "is_best": True
        })

    # Cleanup
    del audios, outputs
    torch.cuda.empty_cache()
    generation_active = False

    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1e9
        reserved = torch.cuda.memory_reserved(0) / 1e9
        print(f"[VRAM] After gen: {allocated:.2f} GB alloc, {reserved:.2f} GB res")

    return results, 44100, seed

FILE: E:\tts_0\models\openrouter.py

================================================================================

# models/openrouter.py
import requests
import json
from config import OPENROUTER_API_KEY

BASE_URL = "https://openrouter.ai/api/v1"
HEADERS = {
    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
    "HTTP-Referer": "http://localhost:5000",
    "X-Title": "TTS Studio Local",
}

# Top models we always want available
POPULAR_MODELS = [
    "openrouter/auto",
    "anthropic/claude-3.5-sonnet",
    "anthropic/claude-3-opus",
    "google/gemini-pro-1.5",
    "meta-llama/llama-3.1-405b-instruct",
    "meta-llama/llama-3.1-70b-instruct",
    "mistralai/mixtral-8x22b-instruct",
    "qwen/qwen-2-72b-instruct",
    "openai/gpt-4o",
    "openai/gpt-4o-mini",
    "cohere/command-r-plus",
    "01-ai/yi-large",
]

_session = None
def _get_session():
    global _session
    if _session is None:
        _session = requests.Session()
        _session.headers.update(HEADERS)
    return _session

def get_models():
    """Fetch all models from OpenRouter + inject our popular ones"""
    try:
        resp = _get_session().get(f"{BASE_URL}/models", timeout=10)
        resp.raise_for_status()
        data = resp.json().get("data", [])
        remote_ids = {m["id"] for m in data}
        # Add any missing popular models
        for model_id in POPULAR_MODELS:
            if model_id not in remote_ids:
                data.append({"id": model_id})
        return sorted(data, key=lambda x: (x["id"] not in POPULAR_MODELS, x["id"]))
    except Exception as e:
        print(f"[OPENROUTER] Failed to fetch models: {e}")
        # Fallback to popular list only
        return [{"id": mid} for mid in POPULAR_MODELS]

def health_check():
    """Silent tiny request to verify key works"""
    try:
        payload = {
            "model": "openrouter/auto",
            "messages": [{"role": "user", "content": "hi"}],
            "max_tokens": 1
        }
        resp = _get_session().post(f"{BASE_URL}/chat/completions", json=payload, timeout=15)
        return resp.status_code == 200
    except:
        return False

def infer_openrouter(
    messages,
    model="openrouter/auto",
    temperature=0.8,
    max_tokens=8192,
    top_p=0.95,
    top_k=40,                    # ← ignored by OpenRouter — safe to send
    presence_penalty=0.0,
    frequency_penalty=0.0
):
    payload = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "top_p": top_p,
        "top_k": top_k,
        "presence_penalty": presence_penalty,
        "frequency_penalty": frequency_penalty,
        "stream": True
    }
    try:
        with _get_session().post(f"{BASE_URL}/chat/completions", json=payload, stream=True, timeout=300) as resp:
            resp.raise_for_status()
            for line in resp.iter_lines():
                if not line:
                    continue
                line = line.strip()
                if not line.startswith(b"data: "):
                    continue
                if line == b"data: [DONE]":
                    break
                try:
                    json_str = line[6:].decode("utf-8")
                    chunk = json.loads(json_str)
                    content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
                    if content:
                        yield content
                except:
                    continue
    except Exception as e:
        yield f"\n[OPENROUTER ERROR] {str(e)}"

FILE: E:\tts_0\models\clap.py

================================================================================

import os, gc
from pathlib import Path
import torch
from transformers import ClapModel, ClapProcessor
from huggingface_hub import snapshot_download

os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

MODEL_DIR = Path(__file__).parent.parent / "models" / "clap-htsat-unfused"

# This PR adds the real model.safetensors (600 MB) to laion’s repo
REPO_ID = "laion/clap-htsat-unfused"
REVISION = "refs/pr/3"      # contains model.safetensors + everything else

_clap_model = None
_processor = None

def load_clap(device="cuda:0"):  # Changed: Accept dynamic device
    global _clap_model, _processor
    if _clap_model is not None:
        return _clap_model, _processor

    MODEL_DIR.mkdir(parents=True, exist_ok=True)
    print(f"[CLAP] Checking {MODEL_DIR} ...")

    # Only trigger download if the big file is missing (fast check)
    if not (MODEL_DIR / "model.safetensors").exists():
        print("[CLAP] Downloading full model (with safetensors) – this can take 20–90 seconds ...")
        snapshot_download(
            repo_id=REPO_ID,
            revision=REVISION,
            local_dir=str(MODEL_DIR),
            local_dir_use_symlinks=False,
            max_workers=8,          # faster + resume support
            tqdm_class=None,        # we print our own messages
        )
        print("[CLAP] Download complete")

    _clap_model = ClapModel.from_pretrained(str(MODEL_DIR)).to(device).eval()
    _processor = ClapProcessor.from_pretrained(str(MODEL_DIR))
    print(f"[CLAP] Loaded on {device}")
    return _clap_model, _processor


def unload_clap() -> None:
    """Unload the CLAP model and processor from GPU memory.

    Deletes the objects, clears PyTorch cache. Safe to call even if nothing is loaded.
    """
    global _clap_model, _processor
    if _clap_model is not None:
        del _clap_model
        _clap_model = None
    if _processor is not None:
        del _processor
        _processor = None
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
    print("[UNLOAD] CLAP unloaded.")

FILE: E:\tts_0\models\stable_audio_state.py

================================================================================

# models/stable_audio_state.py
"""
Stable Audio state — single source of truth.
Used by:
- models/stable_audio.py
- routes/stable_audio.py
- models/__init__.py
"""
_model_loaded = False
_current_device = None

def is_model_loaded():
    return _model_loaded

def set_model_loaded(value: bool):
    global _model_loaded
    _model_loaded = value

def get_current_device():
    return _current_device

def set_current_device(device: str):
    global _current_device
    _current_device = device

FILE: E:\tts_0\models\kokoro.py

================================================================================

# models/kokoro.py
"""
Kokoro TTS model loader and voice manager.

Handles:
- Automatic download of the Kokoro-82M model from HuggingFace if missing
- Proper eSpeak-ng phonemizer setup (required for English phoneme generation)
- Global pipeline lifecycle (load/unload with GPU memory cleanup)
- Exposure of the fixed list of 20 high-quality English voices

The actual inference is performed via the `KPipeline` class from the official
`kokoro` package. This module only manages loading and voice enumeration.
"""
import os
import torch
import logging
from pathlib import Path
from huggingface_hub import snapshot_download
from kokoro import KPipeline
from config import KOKORO_MODEL_DIR, resolve_device, ESPEAK_DIR

DLL_PATH = ESPEAK_DIR / "libespeak-ng.dll"
DATA_DIR = ESPEAK_DIR / "espeak-ng-data"

from phonemizer.backend.espeak.wrapper import EspeakWrapper
EspeakWrapper.library_path = str(DLL_PATH)
EspeakWrapper.data_path = str(DATA_DIR)

pipeline = None
model_loaded = False
device_id = None

ENGLISH_VOICES = [
    "af_heart", "af_alloy", "af_aoede", "af_bella", "af_jessica", "af_kore",
    "af_nicole", "af_nova", "af_river", "af_sarah", "af_sky",
    "am_adam", "am_echo", "am_eric", "am_fenrir", "am_liam", "am_michael",
    "am_onyx", "am_puck", "am_santa"
]

def _debug(msg: str):
    print(f"[KOKORO DEBUG] {msg}")

def load_kokoro(device=None):
    """Load the Kokoro-82M model and create the inference pipeline.

    Automatically downloads the model on first use if not present.

    Args:
        device: Target device string (e.g. "cuda:0", "cpu"). Resolved via config if None.

    Returns:
        Tuple[bool, str]: (success, status message)
    """
    global pipeline, model_loaded, device_id
    dev = device if device is not None else resolve_device(None)

    if model_loaded and device_id != dev:
        _debug(f"Device change {device_id} to {dev}, unloading")
        unload_kokoro()

    if model_loaded:
        _debug("Already loaded")
        return True, "Kokoro already loaded"

    try:
        if not KOKORO_MODEL_DIR.exists() or not any(KOKORO_MODEL_DIR.iterdir()):
            _debug(f"Downloading model to {KOKORO_MODEL_DIR}")
            snapshot_download(
                repo_id="hexgrad/Kokoro-82M",
                local_dir=KOKORO_MODEL_DIR,
                local_dir_use_symlinks=False,
            )
            _debug("Download complete")

        _debug(f"Creating KPipeline on {dev}")
        pipeline = KPipeline(lang_code="a", device=dev)  # 'a' = English
        pipeline.model_dir = str(KOKORO_MODEL_DIR)
        _debug(f"Pipeline ready, model_dir = {pipeline.model_dir}")

        model_loaded = True
        device_id = dev
        logging.info(f"Kokoro loaded on {dev} with {len(ENGLISH_VOICES)} English voices")
        _debug(f"Load complete – type: {type(pipeline)}")
        return True, f"Loaded on {dev}"

    except Exception as e:
        err = f"Load failed: {type(e).__name__}: {e}"
        logging.error(err)
        _debug(err)
        return False, err

def unload_kokoro():
    global pipeline, model_loaded, device_id
    if pipeline is not None:
        _debug("Deleting pipeline")
        del pipeline
        pipeline = None
    model_loaded = False
    device_id = None
    torch.cuda.empty_cache()
    _debug("Unloaded")
    return True, "Unloaded"

def get_voices():
    return ENGLISH_VOICES

FILE: E:\tts_0\models\fish.py

================================================================================

# models/fish.py
"""
FishSpeech model loader and high-level inference wrapper.

This module manages:
- Safe loading/unloading of the FishSpeech inference codebase
- Global temporary directory tracking with crash-safe cleanup
- Reference audio length safety trimming (max 29 seconds)
- The `FishSpeechDemo` class — a complete, self-contained inference run that:
    - Splits text into chunks
    - Executes DAC encode → Text2Sem → DAC decode for each chunk
    - Applies dedicated Fish post-processing (audio_post_FISH)
    - Performs Whisper verification per chunk
    - Assembles final audio with correct front-pad, inter-chunk pause, and global padding
    - Guarantees cleanup of all temporary files

All heavy lifting is done via the original FishSpeech repository scripts
invoked as subprocesses in the correct environment.
"""

import os
import sys
import uuid
import time
import gc
import logging
import subprocess
import tempfile
import shutil
import atexit
from pathlib import Path
from huggingface_hub import snapshot_download

import numpy as np
import soundfile as sf
import torch
from scipy.signal import resample_poly


from config import (
    FISH_REPO_DIR,
    FISH_MODEL_DIR,
    FISH_TEXT2SEM_DIR,
    FISH_DAC_CKPT,
    resolve_device,
    OUTPUT_DIR,
    FISH_PADDING_SECONDS,
    FISH_FRONT_PAD,
    FISH_INTER_PAUSE,
)
from text_utils import split_text_fish


from audio_post_FISH import (
    post_process_fish,
    verify_with_whisper
)

_global_temp_dirs: list[Path] = []

def _register_temp_dir(tdir: Path):
    if tdir not in _global_temp_dirs:
        _global_temp_dirs.append(tdir)

atexit.register(lambda: [shutil.rmtree(p, ignore_errors=True) for p in _global_temp_dirs])

fish_loaded = False
fish_device_id = None

def _ts() -> str:
    return time.strftime("%H:%M:%S")

def load_fish(device=None) -> tuple[bool, str]:
    """Load FishSpeech inference environment (lightweight — just path checks).

    Args:
        device: Target device string (e.g. "cuda:0"). If None, resolved via config.

    Returns:
        Tuple of (success: bool, message: str).
    """
    global fish_loaded, fish_device_id
    dev = device if device is not None else resolve_device(None)

    if fish_loaded and fish_device_id != dev:
        print(f"[{_ts()} FISH] Device change {fish_device_id} → {dev}, unloading first")
        unload_fish()

    if fish_loaded:
        return True, "Fish already loaded"

    # Auto-clone repo if missing
    if not FISH_REPO_DIR.exists():
        print(f"[{_ts()} FISH] Cloning repo to {FISH_REPO_DIR}")
        subprocess.run(["git", "clone", "https://github.com/fishaudio/fish-speech.git", str(FISH_REPO_DIR)], check=True)

    # Auto-download model if missing/empty (gated repo: run `huggingface-cli login` first)
    if not FISH_MODEL_DIR.exists() or not any(FISH_MODEL_DIR.iterdir()):
        print(f"[{_ts()} FISH] Downloading OpenAudio S1-mini (Fish 1.4 successor) to {FISH_MODEL_DIR}")
        snapshot_download(
            repo_id="fishaudio/openaudio-s1-mini",
            local_dir=str(FISH_MODEL_DIR),
            local_dir_use_symlinks=False,
            token=os.getenv("HF_TOKEN"),
        )

    try:
        if not FISH_DAC_CKPT.exists():
            raise FileNotFoundError(f"DAC checkpoint missing: {FISH_DAC_CKPT}")

        logging.info(f"Fish Speech ready on {dev}")
        fish_loaded = True
        fish_device_id = dev
        return True, f"Fish loaded on {dev}"
    except Exception as e:
        err = f"Fish load failed: {e}"
        logging.error(err)
        return False, err

def unload_fish() -> tuple[bool, str]:
    global fish_loaded, fish_device_id
    fish_loaded = False
    fish_device_id = None
    torch.cuda.empty_cache()
    gc.collect()
    logging.info("Fish Speech unloaded")
    return True, "Fish unloaded"


class FishSpeechDemo:
    def __init__(
        self,
        ref_text: str = "",
        ref_audio: str = "",
        temperature: float = 0.7,
        top_p: float = 0.7,
        max_tokens: int = 0,
        speed: float = 1.0,
        de_reverb: float = 0.7,
        de_ess: float = 0.0,
        gpu_id: str = "0",
    ):
        print(f"[{_ts()} FISH] Initializing FishSpeechDemo — reference encoding ONCE")

        self.ref_text = (ref_text or "").strip()
        self.ref_audio_raw = Path(ref_audio)
        self.temperature = temperature
        self.top_p = top_p
        self.max_tokens = max_tokens or 0
        self.speed = speed
        self.de_reverb = de_reverb
        self.de_ess = de_ess
        self.gpu_id = gpu_id

        self.repo_dir = FISH_REPO_DIR
        self.temp_dir = Path(tempfile.mkdtemp(dir=str(OUTPUT_DIR)))
        _register_temp_dir(self.temp_dir)

        # Trim reference once
        wav, sr = sf.read(self.ref_audio_raw)
        if len(wav) / sr > 29.0:
            wav = wav[:int(29.0 * sr)]
            safe_path = self.temp_dir / f"ref_trimmed_{uuid.uuid4().hex}.wav"
            sf.write(safe_path, wav, sr)
            self.ref_audio = safe_path
        else:
            self.ref_audio = self.ref_audio_raw

        # ENCODE REFERENCE ONCE — THIS IS THE ONLY TIME DAC ENCODE RUNS
        print(f"[{_ts()} FISH] Encoding reference audio ONCE...")
        env = os.environ.copy()
        if gpu_id == "cpu":
            env["CUDA_VISIBLE_DEVICES"] = ""
        else:
            # Make all GPUs visible, but use only gpu_id
            gpu_count = torch.cuda.device_count()
            visible = ",".join(str(i) for i in range(gpu_count))
            env["CUDA_VISIBLE_DEVICES"] = visible

        subprocess.run(
            [
                sys.executable,
                "-m", "fish_speech.models.dac.inference",
                "-i", str(self.ref_audio),
                "--checkpoint-path", str(FISH_DAC_CKPT),
                "--device", resolve_device(gpu_id),
            ],
            check=True,
            env=env,
            cwd=self.repo_dir,
        )
        print(f"[{_ts()} FISH] Reference encoded — codes cached")

    def infer(self, text: str, output_wav: str) -> tuple[str, float]:
        chunk = text.strip()
        if not chunk:
            raise ValueError("Empty text chunk")

        cwd = self.repo_dir
        env = os.environ.copy()
        if self.gpu_id == "cpu":
            env["CUDA_VISIBLE_DEVICES"] = ""
        else:
            gpu_count = torch.cuda.device_count()
            visible = ",".join(str(i) for i in range(gpu_count))
            env["CUDA_VISIBLE_DEVICES"] = visible

        # Text2Sem
        t2s_cmd = [
            sys.executable, "-m", "fish_speech.models.text2semantic.inference",
            "--text", chunk,
            "--prompt-text", self.ref_text,
            "--prompt-tokens", str(self.repo_dir / "fake.npy"),
            "--checkpoint-path", str(FISH_TEXT2SEM_DIR),
            "--device", resolve_device(self.gpu_id),
            "--temperature", str(self.temperature),
            "--top-p", str(self.top_p),
        ]
        if self.max_tokens:
            t2s_cmd += ["--max-new-tokens", str(self.max_tokens)]
        subprocess.run(t2s_cmd, check=True, env=env, cwd=cwd)

        # DAC Decode (uses already-encoded codes)
        subprocess.run(
            [
                sys.executable, "-m", "fish_speech.models.dac.inference",
                "-i", "temp/codes_0.npy",
                "--checkpoint-path", str(FISH_DAC_CKPT),
                "--device", resolve_device(self.gpu_id),
            ],
            check=True,
            env=env,
            cwd=cwd,
        )

        # Load and resample
        wav_path = cwd / "fake.wav"
        wav, sr = sf.read(wav_path)
        wav_24k = resample_poly(wav, 24000, sr) if sr != 24000 else wav

        # Post-process
        temp_wav = self.temp_dir / f"temp_{uuid.uuid4().hex}.wav"
        sf.write(temp_wav, wav_24k, 24000, subtype="PCM_16")
        processed = post_process_fish(str(temp_wav), self.speed, self.de_reverb, self.de_ess)

        # Final output
        final_path = Path(output_wav)
        Path(processed).replace(final_path)
        duration = len(wav_24k) / 24000

        return str(final_path), duration

    def __del__(self):
        if hasattr(self, "temp_dir") and self.temp_dir.exists():
            shutil.rmtree(self.temp_dir, ignore_errors=True)

FILE: E:\tts_0\models\xtts.py

================================================================================

# models/xtts.py
"""
XTTS model loader and manager.

Handles lazy loading/unloading of the Coqui XTTS-v2 model with dynamic device selection
(CPU or any available CUDA GPU). Also manages built-in speaker embeddings and
provides safe access to the global model instance.

Features:
- Automatic GPU name detection
- Dynamic device resolution via config.resolve_device()
- Speaker manager loading from speakers_xtts.pth
- Thread-safe global state (tts_model, model_loaded flags)
- Proper cleanup with torch.cuda.empty_cache()
"""
import os
import torch
import logging
from TTS.api import TTS
from config import MODEL_PATH, resolve_device   # ← use resolve_device
from huggingface_hub import snapshot_download

# Global symlink fix
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

tts_model = None
model_loaded = False
speaker_manager = None
built_in_speakers = {}

def _gpu_name() -> str:
    try:
        return torch.cuda.get_device_name(0)
    except Exception:
        return "Unknown GPU"

GPU_NAME = _gpu_name()

def load_speakers() -> tuple[bool, str]:
    global speaker_manager, built_in_speakers
    if speaker_manager is not None:
        return True, "Already loaded"

    speaker_file = MODEL_PATH / "speakers_xtts.pth"
    if not speaker_file.exists():
        return False, f"Speaker file not found: {speaker_file}"

    try:
        from TTS.tts.layers.xtts.xtts_manager import SpeakerManager
        speaker_manager = SpeakerManager(speaker_file_path=str(speaker_file))
        built_in_speakers = speaker_manager.name_to_id
        return True, "Speakers loaded"
    except Exception as e:
        error_msg = f"Speaker load failed: {type(e).__name__}: {e}"
        logging.error(error_msg)
        return False, error_msg

def _ensure_model_exists():
    if MODEL_PATH.exists() and any(MODEL_PATH.iterdir()):
        return

    print(f"[XTTS] Model missing → downloading coqui/XTTS-v2 to {MODEL_PATH}")
    snapshot_download(
        repo_id="coqui/XTTS-v2",
        local_dir=MODEL_PATH,
        local_dir_use_symlinks=False,
        resume_download=True,
    )
    print("[XTTS] Download complete")

def load_xtts(device=None) -> tuple[bool, str]:
    global tts_model, model_loaded, built_in_speakers, speaker_manager

    dev = device if device is not None else resolve_device(None)
    print(f"[XTTS.load_xtts] Requested device resolved → {dev}")

    current_dev_str = None
    if tts_model is not None:
        try:
            current_tensor_dev = next(tts_model.parameters()).device
            if current_tensor_dev.type == "cuda":
                current_dev_str = f"cuda:{current_tensor_dev.index}"
            else:
                current_dev_str = "cpu"
        except:
            current_dev_str = "unknown"

    print(f"[XTTS.load_xtts] Currently loaded on → {current_dev_str or 'not loaded'}")

    should_reload = False

    if model_loaded:
        if current_dev_str != dev:
            print(f"[XTTS.load_xtts] DEVICE CHANGE DETECTED: {current_dev_str} → {dev} — forcing reload")
            should_reload = True
            unload_xtts()
        else:
            print(f"[XTTS.load_xtts] Model already loaded on correct device ({dev}) — skipping reload")
            return True, "XTTS already loaded on correct device"

    if not model_loaded or should_reload:
        print(f"[XTTS.load_xtts] Proceeding to load XTTS on {dev}...")
        _ensure_model_exists()
        try:
            logging.info(f"Loading XTTS on {dev} from {MODEL_PATH}")
            print(f"[XTTS.load_xtts] Creating TTS() instance...")
            tts_model = TTS(
                model_path=str(MODEL_PATH),
                config_path=str(MODEL_PATH / "config.json"),
                progress_bar=True
            ).to(dev)

            print(f"[XTTS.load_xtts] Model moved to {dev} — loading speakers...")
            success, msg = load_speakers()
            if not success:
                logging.warning(msg)
                print(f"[XTTS.load_xtts] Speaker load warning: {msg}")
            if speaker_manager:
                tts_model.speaker_manager = speaker_manager

            model_loaded = True
            built_in_speakers = tts_model.speaker_manager.name_to_id if tts_model.speaker_manager else {}
            logging.info("XTTS loaded successfully")
            print(f"[XTTS.load_xtts] XTTS successfully loaded on {dev}")
            return True, "XTTS loaded"
        except Exception as e:
            error_msg = f"XTTS load failed: {type(e).__name__}: {e}"
            logging.error(error_msg)
            print(f"[XTTS.load_xtts] LOAD FAILED → {error_msg}")
            return False, error_msg
    else:
        print(f"[XTTS.load_xtts] Model already loaded and up-to-date on {dev}")
        return True, "XTTS already loaded"

def unload_xtts() -> None:
    global tts_model, model_loaded, built_in_speakers
    if tts_model:
        del tts_model
        tts_model = None
    model_loaded = False
    built_in_speakers = {}
    torch.cuda.empty_cache()
    logging.info("XTTS unloaded")

def get_builtin_speakers() -> list[str]:
    if not built_in_speakers:
        load_speakers()
    return sorted(built_in_speakers.keys())

FILE: E:\tts_0\models\whisper.py

================================================================================

# models/whisper.py
import whisper
import torch
import gc
from config import WHISPER_PATH, resolve_device

# Valid choices: tiny.en, base.en, small.en, medium.en, large-v3, turbo
# You set it once in config.py → it works forever

WHISPER_MODEL_NAME = WHISPER_PATH.stem  # extracts "medium.en" or "base.en" etc.

whisper_model = None
_current_device = None

def load_whisper(device=None):
    global whisper_model, _current_device
    if whisper_model is not None:
        print(f"[WHISPER] Unloading from {_current_device}...")
        _force_unload()

    dev = device if device is not None else "cpu"
    gpu_id = int(dev.split(":")[-1]) if "cuda" in dev else None
    if gpu_id is not None:
        torch.cuda.reset_peak_memory_stats(gpu_id)

    try:
        # Auto-download the exact model name you chose in config.py
        if not WHISPER_PATH.exists():
            print(f"[WHISPER] Downloading '{WHISPER_MODEL_NAME}' → {WHISPER_PATH.parent}")
            whisper.load_model(WHISPER_MODEL_NAME, download_root=str(WHISPER_PATH.parent))

        print(f"[WHISPER] Loading {WHISPER_MODEL_NAME} → {dev}")
        whisper_model = whisper.load_model(str(WHISPER_PATH), device=dev)
        whisper_model = whisper_model.to(dtype=torch.float32)
        _current_device = dev

        if gpu_id is not None:
            peak = torch.cuda.max_memory_allocated(gpu_id) / 1024**3
            print(f"[WHISPER] Peak VRAM: {peak:.2f} GB")

        print(f"[WHISPER] Loaded {WHISPER_MODEL_NAME} on {dev}")
        return True

    except Exception as e:
        print(f"[WHISPER LOAD FAILED] {e}")
        return False

def _force_unload():
    global whisper_model, _current_device
    if whisper_model is not None:
        del whisper_model
        whisper_model = None
    if _current_device and "cuda" in _current_device:
        torch.cuda.empty_cache()
    _current_device = None
    gc.collect()

def unload_whisper():
    _force_unload()
    print("[WHISPER] Unloaded")

FILE: E:\tts_0\models\__init__.py

================================================================================

# models/__init__.py
from models.xtts import load_xtts, unload_xtts, get_builtin_speakers, tts_model, model_loaded
from models.whisper import load_whisper, unload_whisper, whisper_model
from models.fish import load_fish, unload_fish, fish_loaded, FishSpeechDemo

from models.stable_audio import load_stable_audio, unload_stable_audio, cancel_generation
from models.stable_audio_state import is_model_loaded as stable_model_loaded

from models.ace_step_loader import load_ace, unload_ace, model_loaded as ace_model_loaded, generate

from models.lmstudio import (
    infer_lmstudio, model_loaded as lmstudio_model_loaded,
    current_model_name as lmstudio_current_model
)

FILE: E:\tts_0\models\lmstudio.py

================================================================================

# models/lmstudio.py

import requests
import json
from threading import Lock
import pathlib
from config import LMSTUDIO_API_BASE
session = None
current_model_name = None
model_loaded = False
loading_in_progress = False
lock = Lock()

API_BASE = LMSTUDIO_API_BASE

def infer_lmstudio(messages, temperature=0.7, max_tokens=512, top_p=1.0, top_k=40,
                   presence_penalty=0.0, frequency_penalty=0.0):

    payload = {
        "model": "any",  # LM Studio ignores this when only one model loaded
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "top_p": top_p,
        "top_k": top_k,
        "presence_penalty": presence_penalty,
        "frequency_penalty": frequency_penalty,
        "stream": True
    }

    try:
        resp = requests.post(
            f"{API_BASE}/chat/completions",
            json=payload,
            stream=True,
            timeout=300
        )
        resp.raise_for_status()
        for line in resp.iter_lines():
            if not line or not line.startswith(b"data: "):
                continue
            if line == b"data: [DONE]":
                break
            try:
                content = json.loads(line[6:].decode()).get("choices", [{}])[0].get("delta", {}).get("content", "")
                if content:
                    yield content
            except:
                continue
    except Exception as e:
        yield f"\n[LM Studio error: {str(e)}] - check if a model is loaded on LM Studio for use."

def lmstudio_model_loaded():
    from models.llama import model_loaded as llama_loaded
    return llama_loaded  

def lmstudio_current_model():
    from models.llama import current_model_path
    if current_model_path:
        return pathlib.Path(current_model_path).stem
    return None

def lmstudio_loading_in_progress():
    from models.llama import loading_in_progress
    return loading_in_progress

FILE: E:\tts_0\models\llama.py

================================================================================

# models/llama.py
import gc
import os
import threading
from llama_cpp import Llama
import torch
import time
from config import LLM_DEVICE
LLM_GPU_ID = int(LLM_DEVICE.strip())

if not (0 <= LLM_GPU_ID < torch.cuda.device_count()):
    raise RuntimeError(
        f"LLM_DEVICE={LLM_GPU_ID} is invalid. "
        f"You have {torch.cuda.device_count()} GPUs (0–{torch.cuda.device_count()-1})"
    )

tensor_split = [100 if i == LLM_GPU_ID else 0 for i in range(torch.cuda.device_count())]

print(f"[LLAMA] LLM will be forced onto GPU {LLM_GPU_ID} "
      f"→ {torch.cuda.get_device_name(LLM_GPU_ID)}")
#print(f"[LLAMA] tensor_split = {tensor_split}")

# Global state
llm = None
current_model_path = None
model_loaded = False
loading_in_progress = False
model_lock = threading.Lock()

def load_llama(model_path: str, n_ctx: int = 8192, n_gpu_layers: int = 999) -> str:
    global llm, current_model_path, model_loaded, loading_in_progress

    print("\n" + "="*80)
    print("[LLAMA] load_llama() CALLED")
    print(f"[LLAMA] Model path: {model_path}")
    print(f"[LLAMA] n_ctx: {n_ctx} | n_gpu_layers: {n_gpu_layers}")
    print(f"[LLAMA] Thread: {threading.current_thread().name}")
    print("="*80)

    with model_lock:
        if loading_in_progress:
            print("[LLAMA] REJECTED: Already loading in another thread!")
            raise RuntimeError("Model loading already in progress")
        loading_in_progress = True
        print("[LLAMA] Lock acquired — starting load sequence")

    try:
        print("[LLAMA] Step 1 → Unloading any previous model...")
        unload_llama()

        if not os.path.isfile(model_path):
            print(f"[LLAMA] ERROR: File not found → {model_path}")
            raise FileNotFoundError(model_path)

        print(f"[LLAMA] File exists → {os.path.getsize(model_path) / 1e9:.2f} GB")
        print("[LLAMA] Step 2 → About to call Llama() constructor...")
        print("[LLAMA] If you see this and nothing after → it's stuck in CUDA/driver init")
        print(f"[LLAMA] n_gpu_layers = {n_gpu_layers} → 0 = CPU, >0 = GPU")

        start_time = time.time()
        llm = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_batch=512,
            n_gpu_layers=n_gpu_layers,
            main_gpu=LLM_GPU_ID,
            tensor_split=tensor_split,    # e.g. [0, 100] or [0, 0, 100]
            rpc_tensor=True,              # mandatory when using tensor_split            
            verbose=True,
            logits_all=True,
        )
        load_time = time.time() - start_time
        print(f"[LLAMA] CONSTRUCTOR RETURNED SUCCESSFULLY in {load_time:.1f}s!")

        current_model_path = model_path
        model_loaded = True
        mode = "CPU" if n_gpu_layers == 0 else "GPU"
        print(f"[LLAMA] MODEL FULLY LOADED on {mode} — READY FOR INFERENCE")
        print("="*80 + "\n")
        return f"Loaded on {mode} ({load_time:.1f}s)"

    except Exception as e:
        print("[LLAMA] EXCEPTION DURING LOAD:")
        print(f"[LLAMA] Type: {type(e).__name__}")
        print(f"[LLAMA] Message: {str(e)}")
        
        error_lower = str(e).lower()
        if any(kw in error_lower for kw in ["cuda", "driver", "out of memory", "failed to", "invalid", "unsupported"]):
            print("[LLAMA] → This is almost certainly a CUDA/VRAM/driver issue")
        raise

    finally:
        with model_lock:
            loading_in_progress = False
            print("[LLAMA] loading_in_progress = False (lock released)")

def unload_llama(force=False) -> None:
    global llm, current_model_path, model_loaded
    print("[LLAMA] >>> ENTERING unload_llama()")
    if llm is not None:
        try:
            print("[LLAMA] Deleting llm object...")
            del llm
            print("[LLAMA] del llm → success")
        except Exception as e:
            print(f"[LLAMA] del llm failed: {e}")
        llm = None

    current_model_path = None
    model_loaded = False
    torch.cuda.empty_cache()
    gc.collect()
    print("[LLAMA] VRAM cleared + GC done")

def infer_llama(
    messages: list,
    temperature: float = 0.8,
    max_tokens: int = 8192,
    top_p: float = 0.95,
    top_k: int = 40,
    presence_penalty: float = 0.0,
    frequency_penalty: float = 0.0
):
    with model_lock:
        if not model_loaded or llm is None:
            raise RuntimeError("No model loaded")
        if loading_in_progress:
            raise RuntimeError("Still loading model")

        print(f"[LLAMA] Starting inference → {len(messages)} messages")
        print(f"  temp={temperature} max_tokens={max_tokens} top_p={top_p} top_k={top_k} "
              f"presence={presence_penalty} freq={frequency_penalty}")

        completion = llm.create_chat_completion(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p,
            top_k=top_k,
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            stop=["<|end_of_text|>", "<|eot_id|>", "<|im_end|>", "</s>"],
            stream=True,
        )

        for chunk in completion:
            delta = chunk["choices"][0]["delta"].get("content", "")
            if delta:
                yield delta

FILE: E:\tts_0\routes\lmstudio.py

================================================================================

# routes/lmstudio.py

import requests
from flask import Blueprint, request, jsonify, Response
from models.lmstudio import (
    infer_lmstudio,
)
from config import LMSTUDIO_API_BASE
bp = Blueprint('lmstudio', __name__, url_prefix='/lmstudio')
API_BASE = LMSTUDIO_API_BASE

@bp.route('/infer', methods=['POST'])
def lmstudio_infer():
    print("\n" + "="*100)
    print(" LMSTUDIO INFERENCE HIT ".center(100, "="))
    print("="*100)

    data = request.get_json() or {}
    messages = data.get("messages", [])  # ← Already intact!

    temperature        = float(data.get("temperature", 0.8))
    max_tokens         = int(data.get("max_tokens", 8192))
    top_p              = float(data.get("top_p", 0.95))
    top_k              = int(data.get("top_k", 40))
    presence_penalty   = float(data.get("presence_penalty", 0.0))
    frequency_penalty  = float(data.get("frequency_penalty", 0.0))

    print(f"[PARAMS] temperature={temperature} | max_tokens={max_tokens} | top_p={top_p} | top_k={top_k}")
    print(f"[PARAMS] presence_penalty={presence_penalty} | frequency_penalty={frequency_penalty}")
    print(f"[MESSAGES] Total messages received: {len(messages)}")

    # Enhanced full message dump
    for i, msg in enumerate(messages):
        role = msg.get("role", "unknown").upper()
        preview = msg.get("content", "").replace("\n", " ")[:150] + ("..." if len(msg.get("content", "")) > 150 else "")
        print(f"  [{i:02d}] {role:<6} → {preview}")

    print("-" * 100)

    def generate():
        try:
            for chunk in infer_lmstudio(
                messages,  # ← Direct pass — no changes needed
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                top_k=top_k,
                presence_penalty=presence_penalty,
                frequency_penalty=frequency_penalty
            ):
                yield chunk
        except Exception as e:
            error = f"[LMSTUDIO ERROR] {str(e)}"
            print(error)
            yield f"\n{error}"

    return Response(generate(), mimetype='text/plain')


@bp.route('/status', methods=['GET'])
def lmstudio_status():
    try:
        resp = requests.get(f"{API_BASE}/models", timeout=3)
        if resp.status_code == 200:
            models = resp.json().get("data", [])
            if models:
                current = models[0]["id"]
                return jsonify({
                    "loaded": True,
                    "model": current,
                    "loading": False
                })
        
        # No model returned → truly unloaded
        return jsonify({
            "loaded": False,
            "model": "—",
            "loading": False
        })
    except Exception as e:
        print(f"[LMSTUDIO] Status: SERVER OFFLINE → {e}")
        return jsonify({
            "loaded": False,
            "model": "—",
            "loading": False
        })

@bp.route('/models', methods=['GET'])
def lmstudio_models():
    try:
        resp = requests.get(f"{API_BASE}/models", timeout=8)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        return jsonify({"error": "Failed to fetch models"}), 500

FILE: E:\tts_0\routes\model.py

================================================================================

# routes/model.py
from flask import jsonify, request
from . import bp
from config import resolve_device

from models.xtts import load_xtts, unload_xtts
from models.fish import load_fish, unload_fish, fish_loaded, fish_device_id
from models.kokoro import load_kokoro, unload_kokoro, model_loaded as kokoro_loaded
import models.kokoro as kokoro_mod

whisper_model = None
load_whisper = None
unload_whisper = None

try:
    from models.whisper import (
        load_whisper as _lw,
        unload_whisper as _uw,
        whisper_model as _wm,
    )
    load_whisper = _lw
    unload_whisper = _uw
    # NOTE: 
    
except Exception as e:   
    print(f"[WARN] Whisper import failed: {e}")

@bp.route("/whisper_load", methods=["POST"])
def whisper_load():
    if load_whisper is None:
        return jsonify({"error": "Whisper not available"}), 500

    raw_device = request.json.get("device") or "cpu"
    device = resolve_device(raw_device)  
    success = load_whisper(device)    

    if success:
        return jsonify({"message": "Whisper loaded"})
    return jsonify({"error": "Failed"}), 500


@bp.route("/whisper_unload", methods=["POST"])
def whisper_unload():
    if unload_whisper is None:
        return jsonify({"error": "Whisper not available"}), 500

    print("Unloading Whisper model...")
    unload_whisper()
    print("Whisper unloaded")
    return jsonify({"message": "Unloaded"})

@bp.route("/whisper_status", methods=["GET"])
def whisper_status():
    """Return {"loaded": true/false} – used by the UI badge."""
    try:
        from models.whisper import whisper_model
        return jsonify({"loaded": whisper_model is not None})
    except Exception:        
        return jsonify({"loaded": False})

@bp.route("/load", methods=["POST"])
def load():
    raw_device = request.json.get("device") 
    device = resolve_device(raw_device)  
    success, msg = load_xtts(device)  
    if success:
        return jsonify({"message": msg or "Loaded"})
    return jsonify({"error": msg or "Failed"}), 500

@bp.route("/unload", methods=["POST"])
def unload():
    print("Unloading XTTS model...")
    unload_xtts()
    return jsonify({"message": "Unloaded"})

@bp.route("/fish_load", methods=["POST"])
def fish_load():
    device = request.json.get("device") or "cpu"
    dev = resolve_device(device)

    # UNLOAD IF DEVICE CHANGED
    if fish_loaded and fish_device_id != dev:
        print(f"[FISH] UI requested device {dev}, current {fish_device_id} → unloading")
        unload_fish()

    success, msg = load_fish(device)
    if success:
        return jsonify({"message": msg or "Loaded"})
    return jsonify({"error": msg or "Failed"}), 500

@bp.route("/fish_unload", methods=["POST"])
def fish_unload():
    print("Unloading Fish model...")
    unload_fish()
    return jsonify({"message": "Unloaded"})

@bp.route("/kokoro_load", methods=["POST"])
def kokoro_load():
    device = request.json.get("device") or "cpu"
    resolved = resolve_device(device)
    success, msg = kokoro_mod.load_kokoro(resolved)
    return jsonify({"message": msg}), 200 if success else 500

@bp.route("/kokoro_unload", methods=["POST"])
def kokoro_unload():
    kokoro_mod.unload_kokoro()
    return jsonify({"message": "Kokoro unloaded"})

FILE: E:\tts_0\routes\openrouter.py

================================================================================

# routes/openrouter.py
from flask import Blueprint, request, jsonify, Response
from models.openrouter import get_models, health_check, infer_openrouter
bp = Blueprint('openrouter', __name__, url_prefix='/openrouter')

@bp.route('/models', methods=['GET'])
def openrouter_models():
    """Return full model list from OpenRouter"""
    try:
        models = get_models()
        return jsonify(models)
    except Exception as e:
        print(f"[OPENROUTER] Failed to fetch models: {e}")
        return jsonify({"error": "Failed to fetch models"}), 500

@bp.route('/status', methods=['GET'])
def openrouter_status():
    """Simple health check — returns whether API key is valid"""
    connected = health_check()
    return jsonify({
        "connected": connected,
        "model": "OpenRouter" if connected else None
    })

@bp.route('/infer', methods=['POST'])
def openrouter_infer():
    """Streaming inference endpoint for OpenRouter"""
    data = request.get_json() or {}
    messages = data.get("messages", [])  # ← Already intact!
    model = data.get("model", "openrouter/auto")

    temperature        = float(data.get("temperature", 0.8))
    max_tokens         = int(data.get("max_tokens", 8192))
    top_p              = float(data.get("top_p", 0.95))
    top_k              = int(data.get("top_k", 40))          # ignored by OpenRouter – safe to send
    presence_penalty   = float(data.get("presence_penalty", 0.0))
    frequency_penalty  = float(data.get("frequency_penalty", 0.0))

    print(f"[OPENROUTER] Inference → model: {model} | messages: {len(messages)}")
    print(f"  temp={temperature} max_tokens={max_tokens} top_p={top_p} top_k={top_k} "
          f"presence={presence_penalty} freq={frequency_penalty}")

    # Enhanced message dump
    for i, msg in enumerate(messages):
        role = msg.get("role", "unknown").upper()
        preview = msg.get("content", "").replace("\n", " ")[:150] + ("..." if len(msg.get("content", "")) > 150 else "")
        print(f"  [{i:02d}] {role:<6} → {preview}")

    def generate():
        try:
            for token in infer_openrouter(
                messages=messages,  # ← Direct pass — no changes needed
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                top_k=top_k,
                presence_penalty=presence_penalty,
                frequency_penalty=frequency_penalty
            ):
                yield token
        except Exception as e:
            error_msg = f"[OPENROUTER ERROR] {str(e)}"
            print(error_msg)
            yield f"\n{error_msg}"

    return Response(generate(), mimetype='text/plain')

FILE: E:\tts_0\routes\production.py

================================================================================

# routes/production.py

from flask import Blueprint, request, jsonify, render_template
from werkzeug.utils import secure_filename
from config import OUTPUT_DIR, FFMPEG_BIN
from pathlib import Path
import json
import subprocess
import torch
import models.whisper as whisper_mod
import logging

log = logging.getLogger(__name__)
bp = Blueprint('production', __name__, url_prefix='/production')
@bp.route("/")
def production_page():
    return "This page has been moved to the main interface", 200

@bp.route("/upload_media", methods=["POST"])
def upload_media():
    if "file" not in request.files:
        log.warning("Upload attempted without files")
        return jsonify({"error": "No file"}), 400

    files = request.files.getlist("file")
    if not files or any(f.filename == "" for f in files):
        log.warning("Upload attempted with empty file list")
        return jsonify({"error": "No valid files"}), 400

    project_dir = request.form.get("project_dir", "").strip()
    out_dir = Path(project_dir).resolve() if project_dir else OUTPUT_DIR
    out_dir.mkdir(parents=True, exist_ok=True)

    uploaded = []
    allowed_exts = {
        ".wav", ".mp3", ".flac", ".ogg", ".m4a", ".webm",
        ".mp4", ".mov", ".avi", ".mkv",
        ".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp"
    }

    for f in files:
        ext = Path(f.filename).suffix.lower()
        if ext in allowed_exts:
            fn = secure_filename(f.filename)
            path = out_dir / fn
            f.save(str(path))
            uploaded.append(fn)
            log.info(f"Uploaded: {fn}")

    if not uploaded:
        log.warning(f"No valid media files uploaded to {out_dir}")
        return jsonify({"error": "No valid media files"}), 400

    log.info(f"Uploaded {len(uploaded)} file(s) → {out_dir}")
    return jsonify({"success": True, "filenames": uploaded})


@bp.route("/list_audio")
def list_audio():
    dir_path = request.args.get("dir", "").strip()
    recursive = request.args.get("recursive") == "1"
    search_dir = Path(dir_path).resolve() if dir_path else OUTPUT_DIR

    log.info(f"Listing audio in {search_dir} (recursive={recursive})")

    if not search_dir.exists():
        return jsonify([])

    exts = {'.wav', '.mp3', '.flac', '.ogg', '.m4a', '.webm', '.mp4', '.mov', '.avi', '.mkv'}
    pattern = '**/*' if recursive else '*'
    files = [str(p.relative_to(search_dir).as_posix()) for p in search_dir.glob(pattern)
             if p.is_file() and p.suffix.lower() in exts]

    log.info(f"Found {len(files)} audio/video file(s)")
    return jsonify(sorted(files))


@bp.route("/list_images")
def list_images():
    dir_path = request.args.get("dir", "").strip()
    recursive = request.args.get("recursive") == "1"
    search_dir = Path(dir_path).resolve() if dir_path else OUTPUT_DIR

    log.info(f"Listing images/video in {search_dir} (recursive={recursive})")

    if not search_dir.exists():
        return jsonify([])

    exts = {'.jpg','.jpeg','.png','.gif','.bmp','.webp','.mp4','.webm','.mov','.avi','.mkv'}
    pattern = '**/*' if recursive else '*'
    files = [str(p.relative_to(search_dir).as_posix()) for p in search_dir.glob(pattern)
             if p.is_file() and p.suffix.lower() in exts]

    log.info(f"Found {len(files)} image/video file(s)")
    return jsonify(sorted(files))

@bp.route("/transcribe", methods=["POST"])
def transcribe():
    data = request.get_json() or {}
    filename = data.get("filename")
    project_dir = data.get("project_dir", "").strip()

    if not filename:
        log.warning("Transcribe requested without filename")
        return jsonify({"error": "No filename"}), 400

    search_dir = Path(project_dir).resolve() if project_dir else OUTPUT_DIR
    audio_path = search_dir / filename

    if not audio_path.exists():
        log.warning(f"Transcribe: File not found → {audio_path}")
        return jsonify({"error": "File not found"}), 404

    stem = audio_path.stem
    srt_path = search_dir / f"{stem}.srt"
    json_path = search_dir / f"{stem}_timing.json"

    # ——— Already transcribed? Return cached result ———
    if srt_path.exists() or json_path.exists():
        log.info(f"Transcribe SKIPPED (already exists): {stem}")

        if json_path.exists():
            try:
                with open(json_path, "r", encoding="utf-8") as f:
                    existing = json.load(f)
                cached_text = existing.get("text", "")
                log.info(f"Returning cached transcription ({len(cached_text.split())} words)")
                return jsonify({"text": cached_text})
            except Exception as e:
                log.error(f"Failed to read cached _timing.json for {stem}: {e}")

        return jsonify({"text": ""})  # SRT exists but no JSON → force re-transcribe if user wants

    # ——— Start fresh transcription ———
    log.info(f"Starting transcription → {audio_path.name}")

    # Load Whisper if needed
    if whisper_mod.whisper_model is None:
        log.info("Whisper model not loaded → loading now (this may take a moment)")
        if not whisper_mod.load_whisper():
            log.error("Failed to load Whisper model")
            return jsonify({"error": "Failed to load Whisper model"}), 500

    try:
        result = whisper_mod.whisper_model.transcribe(
            str(audio_path),
            word_timestamps=True,
            fp16=False
        )
    except Exception as e:
        log.error(f"Whisper transcription crashed: {e}")
        return jsonify({"error": "Transcription failed"}), 500

    # ——— Process words & build full text ———
    words = []
    full_text = []

    for seg in result.get("segments", []):
        for w in seg.get("words", []):
            clean = w["word"].strip()
            words.append({"word": clean, "start": w["start"], "end": w["end"]})
            full_text.append(clean)

    text = " ".join(full_text)

    # ——— Save timing JSON ———
    timing_path = search_dir / f"{stem}_timing.json"
    with open(timing_path, "w", encoding="utf-8") as f:
        json.dump({"words": words, "text": text}, f, ensure_ascii=False, indent=2)
    log.info(f"Saved timing JSON → {timing_path.name}")

    # ——— Generate SRT (3-word chunks) ———
    def _fmt(seconds):
        if not isinstance(seconds, (int, float)) or seconds is None:
            return "00:00:00,000"
        hrs = int(seconds // 3600)
        mins = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        ms = int(round((seconds - int(seconds)) * 1000))
        return f"{hrs:02d}:{mins:02d}:{secs:02d},{ms:03d}"

    if words:
        text_words = text.split()
        chunks = [text_words[i:i+3] for i in range(0, len(text_words), 3)]
        with open(srt_path, "w", encoding="utf-8") as f:
            for idx, chunk in enumerate(chunks):
                start_idx = sum(len(prev) for prev in chunks[:idx])
                end_idx = min(start_idx + len(chunk) - 1, len(words) - 1)
                start_time = words[start_idx]["start"]
                end_time = words[end_idx]["end"]
                line = " ".join(chunk)
                f.write(f"{idx + 1}\n{_fmt(start_time)} --> {_fmt(end_time)}\n{line}\n\n")
        log.info(f"Saved SRT subtitles → {srt_path.name} ({len(chunks)} lines)")

    # ——— Done! ———
    word_count = len(text.split())
    log.info(f"Transcription COMPLETE → {stem} | {len(words)} timed words | ~{word_count} words total")

    return jsonify({"text": text})

@bp.route("/make_video", methods=["POST"])
def make_video():
    data = request.get_json() or {}
    selected_audio = data.get("audio_file")
    project_dir = data.get("project_dir", "").strip()
    resolution = data.get("resolution", "1080p").strip()  # exact string from dropdown
    bg_mode = data.get("bg_mode", "color")
    chroma_color = data.get("chroma_color", "red")

    print(f"[VIDEO] START → resolution='{resolution}' | bg_mode='{bg_mode}' | color='{chroma_color}'")

    if not selected_audio:
        return jsonify({"error": "No audio file selected"}), 400

    out_dir = Path(project_dir).resolve() if project_dir else OUTPUT_DIR
    audio_path = (out_dir / selected_audio).resolve()
    if not audio_path.is_file():
        return jsonify({"error": "Audio file not found"}), 404

    # ───── RESOLUTION MAPS (lowercase keys only) ─────
    HORIZONTAL = {
        "720p":  "1280x720",
        "1080p": "1920x1080",
        "1440p": "2560x1440",
        "4k":    "3840x2160",
    }

    VERTICAL = {
        "1080p_vertical": "1080x1920",
        "4k_vertical":    "2160x3840",
    }

    # Normalize once
    key = resolution.lower()

    if key in VERTICAL:
        w, h = map(int, VERTICAL[key].split('x'))
        print(f"[VIDEO] VERTICAL MODE → {resolution} → canvas {w}×{h}")
    elif key in HORIZONTAL:
        w, h = map(int, HORIZONTAL[key].split('x'))
        print(f"[VIDEO] HORIZONTAL MODE → {resolution} → canvas {w}×{h}")
    else:
        print(f"[VIDEO] UNKNOWN RESOLUTION '{resolution}' → falling back to 1080p")
        w, h = 1920, 1080

    # ───── DURATION ─────
    try:
        duration = float(subprocess.check_output([
            str(FFMPEG_BIN / "ffprobe.exe"), "-v", "error",
            "-show_entries", "format=duration",
            "-of", "default=noprint_wrappers=1:nokey=1",
            str(audio_path)
        ], stderr=subprocess.DEVNULL).decode().strip())
        print(f"[VIDEO] Audio duration → {duration:.2f}s")
    except Exception as e:
        print(f"[VIDEO] Duration probe failed → {e}")
        return jsonify({"error": "Cannot read audio duration"}), 500

    # ───── SUBTITLES ─────
    stem = audio_path.stem
    srt_path = audio_path.parent / f"{stem}.srt"
    if not srt_path.exists():
        return jsonify({"error": "No subtitles found. Please transcribe first."}), 400

    # ───── MARGIN (per resolution, exact match) ─────
    MARGINS = {
        "720p":            26,
        "1080p":           32,
        "1440p":           34,
        "4k":              30,
        "1080p_vertical":  40,
        "4k_vertical":     40,
    }
    margin_v = MARGINS.get(key, 32)
    print(f"[VIDEO] Subtitle margin → {margin_v}px (bottom-center)")

    # ───── OUTPUT FILE ─────
    suffix = "_alpha.webm" if bg_mode == "transparent" else ".mp4"
    video_name = f"video_{stem}_{resolution}{suffix}"
    video_path = out_dir / video_name
    print(f"[VIDEO] Output → {video_name}")

    # ───── ESCAPE SRT PATH ─────
    srt_ffmpeg = str(srt_path).replace("\\", "/").replace(":", "\\:")

    # ───── BUILD COMMAND ─────
    cmd = [str(FFMPEG_BIN / "ffmpeg.exe"), "-y"]

    if bg_mode == "transparent":
        print("[VIDEO] Mode → Transparent background")
        cmd += [
            "-f", "lavfi", "-i", f"color=c=black:s={w}x{h}:d={duration}",
            "-i", str(audio_path),
            "-vf", f"subtitles=filename='{srt_ffmpeg}':force_style='Fontsize=28,PrimaryColour=&HFFFFFF&,BorderStyle=1,Outline=1,Shadow=0,Alignment=2,MarginV={margin_v}',format=yuva420p",
            "-c:v", "vp9", "-b:v", "0", "-crf", "30", "-row-mt", "1",
            "-c:a", "libopus", "-b:a", "128k",
            str(video_path)
        ]

    elif bg_mode == "color":
        print(f"[VIDEO] Mode → Solid color ({chroma_color})")
        color = {"green": "green", "blue": "blue", "red": "#DC143C"}.get(chroma_color.lower(), "green")
        cmd += [
            "-f", "lavfi", "-i", f"color=c={color}:s={w}x{h}:d={duration}",
            "-i", str(audio_path),
            "-vf", f"subtitles=filename='{srt_ffmpeg}':force_style='Fontsize=28,PrimaryColour=&HFFFFFF&,BorderStyle=1,Outline=2,Shadow=1,Alignment=2,MarginV={margin_v},Bold=-1'",
            "-map", "0:v", "-map", "1:a",
            "-c:v", "h264_nvenc" if torch.cuda.is_available() else "libx264",
            "-preset", "slow", "-crf", "18", "-pix_fmt", "yuv420p",
            "-c:a", "aac", "-b:a", "320k",
            str(video_path)
        ]

    else:  # images
        print("[VIDEO] Mode → Image/video background")
        media_files = [p for p in out_dir.iterdir()
                      if p.suffix.lower() in {'.jpg','.jpeg','.png','.gif','.bmp','.webp','.mp4','.webm','.mov','.avi','.mkv'}
                      and p.name != selected_audio and not p.name.lower().startswith("video_")]
        media_files.sort(key=lambda x: x.name)

        if not media_files:
            return jsonify({"error": "No background media found"}), 400

        for mf in media_files:
            cmd += ["-loop", "1", "-t", "5", "-i", str(mf)]

        inputs = "".join(f"[{i}:v]" for i in range(len(media_files)))
        concat = f"{inputs}concat=n={len(media_files)}:v=1:a=0[outv]"
        scale = f"scale=w={w}:h={h}:force_original_aspect_ratio=decrease:flags=lanczos,pad={w}:{h}:(ow-iw)/2:(oh-ih)/2:color=black"

        cmd += [
            "-i", str(audio_path),
            "-filter_complex",
            f"{concat};[outv]{scale}[scaled];"
            f"[scaled]subtitles=filename='{srt_ffmpeg}':force_style='Fontsize=28,PrimaryColour=&HFFFFFF&,BorderStyle=1,Outline=1,Shadow=0,Alignment=2,MarginV={margin_v}'[vo]",
            "-map", "[vo]", "-map", f"{len(media_files)}:a",
            "-t", str(duration),
            "-c:v", "h264_nvenc" if torch.cuda.is_available() else "libx264",
            "-preset", "slow", "-crf", "18", "-pix_fmt", "yuv420p",
            "-c:a", "aac", "-b:a", "320k",
            str(video_path)
        ]

    # ───── RUN ─────
    print(f"[VIDEO] Executing FFmpeg ({'vertical' if key in VERTICAL else 'horizontal'})")
    try:
        subprocess.run(cmd, check=True, capture_output=True, text=True, timeout=900)
        print(f"[VIDEO] SUCCESS → {video_name}")
        return jsonify({"video_file": video_name})
    except Exception as e:
        log.error(f"[VIDEO] FFMPEG CRASH: {e}")
        return jsonify({"error": "FFmpeg failed"}), 500


@bp.route("/transcribe_status", methods=["POST"])
def transcribe_status():
    data = request.get_json() or {}
    filename = data.get("filename")
    project_dir = data.get("project_dir", "").strip()

    if not filename:
        print("[WHISPER] transcribe_status → no filename")
        return jsonify({"cached": False})

    search_dir = Path(project_dir).resolve() if project_dir else OUTPUT_DIR
    audio_path = search_dir / filename
    stem = audio_path.stem
    json_path = search_dir / f"{stem}_timing.json"
    srt_path = search_dir / f"{stem}.srt"

    cached = json_path.exists() or srt_path.exists()

    if cached:
        print(f"[WHISPER] Cache HIT → {filename} (transcription skipped)")
    else:
        print(f"[WHISPER] Cache MISS → {filename} (will need Whisper)")

    return jsonify({"cached": cached})

FILE: E:\tts_0\routes\settings_manager.py

================================================================================

# routes\settings_manager.py
from flask import Blueprint, jsonify, request
import json
from pathlib import Path

bp = Blueprint('settings', __name__, url_prefix='/settings')

SETTINGS_DIR = Path("settings")
SETTINGS_DIR.mkdir(exist_ok=True)
#print(f"[SETTINGS] Directory: {SETTINGS_DIR.resolve()}")

@bp.route("/list", methods=["GET"])
def list_presets():
    presets = sorted([f.stem for f in SETTINGS_DIR.glob("*.json")])
    print(f"[SETTINGS] Presets: {presets or 'none'}")
    return jsonify({"presets": presets})

@bp.route("/save", methods=["POST"])
def save_preset():
    data = request.get_json(silent=True) or {}
    name = (data.get("name") or "").strip()
    settings = data.get("settings") or {}

    if not name:
        print("[SETTINGS] SAVE FAILED: No name")
        return jsonify({"error": "Preset name required"}), 400

    path = SETTINGS_DIR / f"{name}.json"
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(settings, f, indent=2, ensure_ascii=False)
        print(f"[SETTINGS] SAVED: '{name}' → {path}")
        return jsonify({"message": "Saved"})
    except Exception as e:
        print(f"[SETTINGS] SAVE ERROR: {e}")
        return jsonify({"error": str(e)}), 500

@bp.route("/load", methods=["POST"])
def load_preset():
    name = request.get_json(silent=True, force=True).get("name", "").strip()
    if not name:
        return jsonify({"error": "Name required"}), 400

    path = SETTINGS_DIR / f"{name}.json"
    if not path.exists():
        print(f"[SETTINGS] LOAD FAILED: '{name}' not found")
        return jsonify({"error": "Preset not found"}), 404

    try:
        with open(path, "r", encoding="utf-8") as f:
            settings = json.load(f)
        print(f"[SETTINGS] LOADED: '{name}' ({len(settings)} fields)")
        return jsonify({"settings": settings})
    except Exception as e:
        print(f"[SETTINGS] LOAD ERROR: {e}")
        return jsonify({"error": str(e)}), 500

@bp.route("/delete", methods=["POST"])
def delete_preset():
    name = request.get_json(silent=True).get("name", "").strip()
    if not name:
        return jsonify({"error": "Name required"}), 400

    path = SETTINGS_DIR / f"{name}.json"
    if path.exists():
        path.unlink()
        print(f"[SETTINGS] DELETED: '{name}'")
        return jsonify({"message": "Deleted"})
    print(f"[SETTINGS] DELETE FAILED: '{name}' not found")
    return jsonify({"error": "Not found"}), 404

FILE: E:\tts_0\routes\stable_audio.py

================================================================================

# routes/stable_audio.py
import os
import base64
import uuid
import time
import json
import subprocess
import soundfile as sf
from flask import request, jsonify, make_response
from . import bp
from config import OUTPUT_DIR, FFMPEG_BIN
from models.stable_audio import generate_audio, load_stable_audio, unload_stable_audio, cancel_generation
from models.stable_audio_state import is_model_loaded
from save_utils import handle_save
from audio_post import stable_post_process
from pathlib import Path
import traceback

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

def _ffmpeg_args(fmt: str):
    fmt = fmt.lower()
    if fmt == "mp3": return ["-c:a", "libmp3lame", "-q:a", "0"]
    if fmt == "ogg": return ["-c:a", "libvorbis", "-q:a", "6"]
    if fmt == "flac": return ["-c:a", "flac", "-compression_level", "12"]
    if fmt == "m4a": return ["-c:a", "aac", "-b:a", "320k"]
    return []

@bp.route("/stable_load", methods=["POST"])
def stable_load():
    """Load the Stable Audio model onto a specific GPU.

    Request JSON:
        { "device": "0" }  (optional, defaults to GPU 0)

    Responses:
        200 → { "message": "Loaded" }
        500 → { "error": "<reason>" }
    """
    try:
        device = request.json.get("device", "0")
        success, msg = load_stable_audio(device)
        return make_response(jsonify({"message": msg} if success else {"error": msg}), 200 if success else 500)
    except Exception as e:
        return make_response(jsonify({"error": str(e)}), 500)

@bp.route("/stable_unload", methods=["POST"])
def stable_unload():
    """Unload the Stable Audio + CLAP models from VRAM.

    Response:
        200 → { "message": "Unloaded" }
    """
    try:
        unload_stable_audio()
        return make_response(jsonify({"message": "Unloaded"}), 200)
    except Exception as e:
        return make_response(jsonify({"error": str(e)}), 500)

@bp.route("/stable_status", methods=["GET"])
def stable_status():
    """Check whether the Stable Audio model is currently loaded.

    Response:
        200 → { "loaded": true/false }
    """
    return make_response(jsonify({"loaded": is_model_loaded()}), 200)

@bp.route("/stable_cancel", methods=["POST"])
def stable_cancel():
    """Cancel any in-progress Stable Audio generation.

    Response:
        200 → { "message": "Cancelled" }
    """
def stable_cancel():
    cancel_generation()
    return make_response(jsonify({"message": "Cancelled"}), 200)

@bp.route("/stable_infer", methods=["POST"])
def stable_infer():
    """Generate audio from a text prompt using Stable Audio 1.0.

    Request JSON fields (all optional except prompt):
        prompt (str)                  – Required text prompt
        negative_prompt (str)         – Negative prompt
        steps (int)                   – Inference steps (10–200)
        length (float)                – Duration in seconds (1–47)
        guidance_scale (float)        – CFG scale, default 7.0
        eta (float)                   – DDIM eta, default 0.0
        num_waveforms_per_prompt (int)– Number of variants (1–4), default 3
        seed (int)                    – Seed, -1 for random
        output_format (str)           – "wav" (default), "mp3", "ogg", "flac", "m4a"
        audio_mode (str)              – "sfx_impact" | "sfx_ambient" | "music"
        save_path (str)               – If provided, generated files are saved here

    Responses:
        • If save_path provided → { "saved_files": [...], "num_generated": N }
        • Otherwise               → { "audios": [{ "audio_base64": "...", "score": ..., "is_best": bool }, ...] }
    """
    try:
        d = request.json
        print("\n" + "="*60)
        print("STABLE AUDIO INFERENCE STARTED")
        print("="*60)
        print(f"Raw payload: {d}")

        prompt = d.get("prompt", "").strip()
        if not prompt:
            return make_response(jsonify({"error": "Missing prompt"}), 400)

        negative_prompt = d.get("negative_prompt") or ""
        steps = max(10, min(200, int(d.get("steps", 100))))
        length = max(1.0, min(47.0, float(d.get("length", 30.0))))
        guidance_scale = float(d.get("guidance_scale", 7.0))
        eta = float(d.get("eta", 0.0))
        num_waveforms = max(1, min(4, int(d.get("num_waveforms_per_prompt", 3))))
        output_format = d.get("output_format", "wav").lower()
        audio_mode = d.get("audio_mode", "sfx_ambient")

        # Validate audio_mode
        valid_modes = {"sfx_impact", "sfx_ambient", "music"}
        if audio_mode not in valid_modes:
            audio_mode = "sfx_ambient"

        raw_seed = d.get("seed")
        seed = -1 if raw_seed in (-1, "-1", "null", None) else int(raw_seed or 42)

        # === SAVE PATH FIX: Normalize slashes ===
        raw_save_path = (d.get("save_path") or "").strip()
        save_path = str(Path(raw_save_path).as_posix()) if raw_save_path else ""
        should_save = bool(save_path)

        print(f"PROMPT: {prompt}")
        print(f"STEPS: {steps} | LENGTH: {length}s | FORMAT: {output_format.upper()}")
        print(f"WAVEFORMS: {num_waveforms} | SAVE: {should_save} | PATH: '{save_path}'")

        results, sample_rate, final_seed = generate_audio(
            prompt=prompt,
            negative_prompt=negative_prompt,
            steps=steps,
            length_sec=length,
            seed=seed,
            guidance_scale=guidance_scale,
            num_waveforms_per_prompt=num_waveforms,
            eta=eta
        )

        print(f"Generated {len(results)} waveform(s) | Seed: {final_seed}")
        processed_paths = []
        audios = []

        for i, res in enumerate(results):
            score = res.get("score")
            is_best = res.get("is_best", False)
            score_str = f"{score:.4f}" if score is not None else "N/A"
            best_marker = " (BEST)" if is_best else ""
            print(f" Variant {i+1}: score={score_str}{best_marker}")

            audio_np = res["audio_np"]  # (samples, channels)
            temp_wav = OUTPUT_DIR / f"stable_temp_{uuid.uuid4().hex}.wav"
            sf.write(str(temp_wav), audio_np, sample_rate, subtype="PCM_16")

            processed = stable_post_process(str(temp_wav), audio_mode=audio_mode)

            # Encode if needed
            final_path = processed
            if output_format != "wav":
                conv = Path(processed).with_suffix(f".{output_format}")
                cmd = [
                    str(FFMPEG_BIN / "ffmpeg.exe"),
                    "-i", processed,
                    *_ffmpeg_args(output_format),
                    str(conv),
                    "-y"
                ]
                result = subprocess.run(cmd, capture_output=True, text=True)
                if result.returncode != 0:
                    print(f"FFMPEG failed: {result.stderr}")
                    conv = processed  # fallback
                else:
                    os.remove(processed)
                final_path = str(conv)

            with open(final_path, "rb") as f:
                b64 = base64.b64encode(f.read()).decode()

            audios.append({
                "audio_base64": b64,
                "score": score,
                "is_best": is_best
            })
            processed_paths.append(final_path)

        if should_save:
            p = Path(save_path)
            cleaned_stem = p.stem.split(".")[0]
            final_path = p.parent / f"{cleaned_stem}.{output_format}"
            saved_files = []

            for i, (path, a) in enumerate(zip(processed_paths, audios)):
                variant_num = i + 1
                best_suffix = " (BEST)" if a["is_best"] else ""
                variant_name = f"{final_path.stem}_v{variant_num}{best_suffix}{final_path.suffix}"
                variant_path = final_path.parent / variant_name
                saved_path, saved_rel = handle_save(path, str(variant_path), "stable")
                saved_files.append({
                    "filename": Path(saved_path).name,
                    "rel_path": saved_rel,
                    "score": a["score"],
                    "is_best": a["is_best"]
                })

            # Debug JSON
            stem = final_path.stem.split("_v")[0]
            timing_data = {
                "prompt": prompt,
                "negative_prompt": negative_prompt,
                "scores": [a["score"] for a in audios if a["score"] is not None],
                "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "model": "Stable Audio 1.0",
                "sample_rate": sample_rate,
                "duration_sec": length,
                "steps": steps,
                "guidance_scale": guidance_scale,
                "eta": eta,
                "seed": final_seed,
                "num_waveforms": num_waveforms,
                "output_format": output_format
            }
            json_path = p.parent / f"{stem}_timing.json"
            json_path.write_text(json.dumps(timing_data, indent=2, ensure_ascii=False))

            # Clean temps
            for p in processed_paths:
                if Path(p).exists():
                    os.remove(p)

            return make_response(jsonify({
                "saved_files": saved_files,
                "num_generated": len(audios)
            }), 200)
        else:
            for p in processed_paths:
                if Path(p).exists():
                    os.remove(p)
            return make_response(jsonify({"audios": audios}), 200)

    except StopIteration:
        return make_response(jsonify({"error": "Cancelled"}), 200)
    except Exception as e:
        error_msg = f"[ERROR]: {str(e)}\n{traceback.format_exc()}"
        print(error_msg)
        return make_response(jsonify({"error": "Server error"}), 500)

FILE: E:\tts_0\routes\static.py

================================================================================

# routes/static.py
from flask import render_template, send_from_directory, jsonify
from config import OUTPUT_DIR, VOICE_DIR
from models.xtts import GPU_NAME, resolve_device
from . import bp

@bp.route("/")
def home():
    return render_template("index.html", gpu_id=resolve_device, gpu_name=GPU_NAME)

@bp.route("/status")
def status():
    from models.xtts import model_loaded
    return jsonify({"loaded": model_loaded})

@bp.route("/fish_status")
def fish_status():
    from models.fish import fish_loaded
    return jsonify({"loaded": fish_loaded})


@bp.route("/audio/<path:filename>")
def serve_audio(filename):
    for folder in (OUTPUT_DIR, VOICE_DIR):
        p = folder / filename
        if p.exists():
            return send_from_directory(str(folder), filename)
    return "File not found", 404

FILE: E:\tts_0\routes\voice.py

================================================================================

# routes/voice.py
import os
from flask import request, jsonify
from werkzeug.utils import secure_filename
from config import VOICE_DIR
from models.xtts import get_builtin_speakers
from . import bp

ALLOWED_EXT = {".wav", ".flac", ".mp3"}

def _allowed(name):
    return os.path.splitext(name)[1].lower() in ALLOWED_EXT

def _list_voices():
    return sorted([f for f in os.listdir(VOICE_DIR) if _allowed(f)])

@bp.route("/voices")
def voices():
    return jsonify(_list_voices())

@bp.route("/speakers")
def speakers():
    return jsonify(get_builtin_speakers())

@bp.route("/upload", methods=["POST"])
def upload():
    if "file" not in request.files:
        return jsonify({"success": False, "error": "No file"}), 400
    f = request.files["file"]
    if not f or not _allowed(f.filename):
        return jsonify({"success": False, "error": "Invalid file"}), 400
    fn = secure_filename(f.filename)
    save_path = VOICE_DIR / fn
    f.save(str(save_path))
    return jsonify({"success": True, "filename": fn})

@bp.route("/refresh_voices", methods=["POST"])
def refresh_voices():
    return jsonify(_list_voices())

FILE: E:\tts_0\routes\voice_transcribe.py

================================================================================

# routes\voice_transcribe.py
from flask import Blueprint, request, jsonify, current_app
import json
import logging
from pathlib import Path
from config import VOICE_DIR, resolve_device
import models.whisper as whisper_mod
import threading
import gc

bp = Blueprint("voice_transcribe", __name__)
log = logging.getLogger(__name__)

transcribe_cancel_event = threading.Event()

def _resolve_voice(filename: str):
    if not filename:
        return None, None
    path = (VOICE_DIR / filename).resolve()
    if not path.is_file():
        return None, None
    return path, path.stem

# NEW: Mirror Production's /whisper_status — simple global check
@bp.route("/voice_whisper_status", methods=["GET"])
def voice_whisper_status():
    return jsonify({"loaded": whisper_mod.whisper_model is not None})

@bp.route("/voice_transcribe_cache", methods=["POST"])
def voice_transcribe_cache():
    data = request.get_json() or {}
    filename = data.get("filename", "").strip()
    if not filename:
        return jsonify({"error": "No filename"}), 400

    audio_path, stem = _resolve_voice(filename)
    if not audio_path:
        return jsonify({"error": "File not found"}), 404

    cache_file = VOICE_DIR / f"{stem}_timing.json"
    if not cache_file.exists():
        return jsonify({"cached": False})

    try:
        with open(cache_file, "r", encoding="utf-8") as f:
            cached = json.load(f)
        return jsonify({"cached": True, "text": cached.get("text", "")})
    except Exception as e:
        log.error(f"Cache read error {cache_file}: {e}")
        return jsonify({"cached": False})

@bp.route("/voice_transcribe", methods=["POST"])
def voice_transcribe():
    global transcribe_cancel_event
    transcribe_cancel_event.clear()

    data = request.get_json() or {}
    filename = data.get("filename", "").strip()
    device_raw = data.get("device", "cpu")
    device = resolve_device(device_raw)  # "0" → "cuda:0"

    if not filename:
        return jsonify({"error": "No filename"}), 400

    audio_path, stem = _resolve_voice(filename)
    if not audio_path:
        return jsonify({"error": "File not found"}), 404

    # SMART RELOAD — ONLY IF NEEDED
    current_dev = getattr(whisper_mod, "_current_device", None)

    if whisper_mod.whisper_model is None:
        log.info(f"[VOICE] Whisper not loaded → loading on {device}")
        whisper_mod.load_whisper(device)
    elif current_dev != device:
        log.info(f"[VOICE] Device changed {current_dev} → {device} → reloading")
        whisper_mod.unload_whisper()
        whisper_mod.load_whisper(device)
    else:
        log.info(f"[VOICE] Whisper already on {device} → reusing (no reload)")

    # Now transcribe
    try:
        result = whisper_mod.whisper_model.transcribe(
            str(audio_path),
            word_timestamps=True,
            fp16=False,
            condition_on_previous_text=False,
        )
    except Exception as e:
        if transcribe_cancel_event.is_set():
            return jsonify({"cancelled": True}), 200
        log.error(f"Whisper failed: {e}")
        return jsonify({"error": "Transcription failed"}), 500

    words = []
    full_text = []
    for seg in result.get("segments", []):
        if transcribe_cancel_event.is_set():
            return jsonify({"cancelled": True}), 200
        for w in seg.get("words", []):
            clean = w["word"].strip()
            words.append({"word": clean, "start": w["start"], "end": w["end"]})
            full_text.append(clean)
    text = " ".join(full_text)

    if transcribe_cancel_event.is_set():
        return jsonify({"cancelled": True}), 200

    # Save cache
    cache_file = VOICE_DIR / f"{stem}_timing.json"
    try:
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump({"words": words, "text": text}, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.warning(f"Failed to save cache: {e}")

    return jsonify({"text": text})


@bp.route("/voice_transcribe_cancel", methods=["POST"])
def voice_transcribe_cancel():
    global transcribe_cancel_event
    transcribe_cancel_event.set()
    log.info("Transcription cancelled by user")
    return jsonify({"success": True})

FILE: E:\tts_0\routes\__init__.py

================================================================================

# routes/__init__.py
from flask import Blueprint

# Main blueprint for the whole app
bp = Blueprint('api', __name__)

from . import static, voice, model, infer_xtts, infer_fish, admin
from . import stable_audio
from . import ace_step
from .settings_manager import bp as settings_bp
from .voice_transcribe import bp as voice_transcribe_bp
from . import infer_kokoro
from . import chatbot
from . import lmstudio
from . import openrouter
from .production import bp as production_bp

# This will be called from main.py
def register_blueprints(app):
    app.register_blueprint(voice_transcribe_bp)
    app.register_blueprint(bp) 
    app.register_blueprint(settings_bp)
    app.register_blueprint(chatbot.bp)
    app.register_blueprint(lmstudio.bp)
    app.register_blueprint(openrouter.bp)
    app.register_blueprint(production_bp)

FILE: E:\tts_0\routes\ace_step.py

================================================================================

# routes/ace_step.py
import os
import random
import uuid
import base64
import json
import time
import subprocess
import soundfile as sf
import torch
from flask import request, jsonify, make_response
from pathlib import Path
from . import bp
from config import OUTPUT_DIR, FFMPEG_BIN
from models.ace_step_loader import load_ace, unload_ace, is_model_loaded, generate as ace_generate
from save_utils import handle_save
from audio_post import ace_post_process, score_with_clap

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

def _ffmpeg_args(fmt: str):
    fmt = fmt.lower()
    if fmt == "mp3":  return ["-c:a", "libmp3lame", "-q:a", "0"]
    if fmt == "ogg":  return ["-c:a", "libvorbis", "-q:a", "6"]
    if fmt == "flac": return ["-c:a", "flac", "-compression_level", "12"]
    if fmt == "m4a":  return ["-c:a", "aac", "-b:a", "320k"]
    return []  # WAV = no extra args

@bp.route("/ace_load", methods=["POST"])
def ace_load():
    """Load the ACE-Step model onto the specified GPU.

    Request JSON:
        { "device": "0" }   # optional, defaults to GPU 0

    Response:
        200 → { "success": true, "loaded": true, "message": "Loaded" }
        500 → { "success": false, ..., "message": "Failed" }
    """
    device = request.json.get("device", "0")
    success = load_ace(device)
    return jsonify({
        "success": success,
        "loaded": is_model_loaded(),
        "message": "Loaded" if success else "Failed"
    }), 200 if success else 500


@bp.route("/ace_unload", methods=["POST"])
def ace_unload():
    """Unload the ACE-Step model and free VRAM.

    Response:
        200 → { "loaded": false, "message": "Unloaded" }
    """
    unload_ace()
    return jsonify({"loaded": False, "message": "Unloaded"})

@bp.route("/ace_status", methods=["GET"])
def ace_status():
    from models.ace_step_loader import is_model_loaded
    return jsonify({"loaded": is_model_loaded()})

@bp.route("/ace_infer", methods=["POST"])
def ace_infer():
    """Generate music using ACE-Step.

    Accepts the full ACE-Step parameter set (all fields optional except prompt).

    Key request fields:
        prompt (str)                     – Required multi-line prompt (STYLE line + lyrics)
        duration (float)                 – 1.0–60.0 seconds
        steps (int)                      – Inference steps (10–200)
        guidance (float)                 – Main CFG scale
        min_guidance (float)             – Minimum guidance during decay
        guidance_interval / decay        – Dynamic guidance scheduling
        guidance_text / guidance_lyric   – Separate text/lyric guidance weights
        scheduler (str)                  – euler, dpmpp_2m, etc.
        cfg_type (str)                   – cfg / self-cfg / etc.
        omega (float)                    – Omega CFG scaling
        erg_tag / erg_lyric / erg_diffusion – ERG ablation flags
        oss_steps (str)                  – One-step scheduler steps string
        num_waveforms_per_prompt (int)   – 1–4 variants (sorted by CLAP score)
        seed (int/str)                   – Fixed seed or "-1" for random
        output_format (str)              – wav (default), mp3, ogg, flac, m4a
        save_path (str)                  – If present → files saved on disk

    Returns (when save_path given):
        { "saved_files": [ { "filename", "rel_path", "clap_score", "is_best", "seed" }, ... ], "num_generated": N }

    Returns (play in browser):
        { "audios": [ { "audio_base64", "score", "is_best", "seed" }, ... ] }
    """
    
    try:
        d = request.json
        if not d:
            return jsonify({"error": "Empty payload"}), 400

        print("\n" + "="*70)
        print("ACE-STEP MUSIC GENERATOR")
        print("="*70)

        # PROMPT
        prompt = d.get("prompt", "").strip()
        if not prompt:
            return jsonify({"error": "Missing prompt"}), 400

        # SAVE PATH
        raw_save_path = d.get("save_path")
        save_path = "" if raw_save_path is None else str(raw_save_path).strip()
        should_save = bool(save_path)

        # ALL ACE PARAMS (STRICT ORDER)
        duration = max(1.0, min(60.0, float(d.get("duration", 10.0))))
        steps = max(10, min(200, int(d.get("steps", 60))))
        guidance = max(1.0, min(10.0, float(d.get("guidance", 3.5))))
        scheduler = d.get("scheduler", "euler")
        cfg_type = d.get("cfg_type", "cfg")
        omega = float(d.get("omega", 1.0))
        min_guidance = float(d.get("min_guidance", 1.0))
        guidance_interval = float(d.get("guidance_interval", 0.0))
        guidance_decay = float(d.get("guidance_decay", 1.0))
        guidance_text = float(d.get("guidance_text", 0.0))
        guidance_lyric = float(d.get("guidance_lyric", 0.0))
        erg_tag = bool(d.get("erg_tag", False))
        erg_lyric = bool(d.get("erg_lyric", False))
        erg_diffusion = bool(d.get("erg_diffusion", False))
        oss_steps = d.get("oss_steps", "")
        num_waveforms = max(1, min(4, int(d.get("num_waveforms_per_prompt", 3))))
        output_format = d.get("output_format", "wav").lower()

        # SEED
        raw_seed = d.get("seed", "-1")
        use_random_seed = raw_seed in ("-1", "", None)

        # LOGGING
        print(f"[PROMPT] Raw prompt from UI:")
        print(f"         └> {prompt!r}")
        print(f"[PROMPT] Lines: {len(prompt.splitlines())} | Chars: {len(prompt)}")
        for i, line in enumerate(prompt.splitlines(), 1):
            clean = line.strip()
            prefix = "STYLE" if i == 1 else "LYRIC" if clean else "EMPTY"
            print(f"         [{prefix:5}] {line!r}")

        print("\n[PARAMETERS]")
        print(f"   Duration           : {duration:.1f}s")
        print(f"   Steps              : {steps}")
        print(f"   Guidance Scale     : {guidance:.2f}")
        print(f"   Min Guidance       : {min_guidance:.2f}")
        print(f"   Guidance Interval  : {guidance_interval:.2f}")
        print(f"   Guidance Decay     : {guidance_decay:.2f}")
        print(f"   Text Guidance      : {guidance_text:.2f}")
        print(f"   Lyric Guidance     : {guidance_lyric:.2f}")
        print(f"   Omega Scale        : {omega:.2f}")
        print(f"   Scheduler          : {scheduler}")
        print(f"   CFG Type           : {cfg_type}")
        print(f"   ERG Tag            : {erg_tag}")
        print(f"   ERG Lyric          : {erg_lyric}")
        print(f"   ERG Diffusion      : {erg_diffusion}")
        print(f"   OSS Steps          : {oss_steps or 'None'}")
        print(f"   Seed               : {raw_seed} → {'RANDOM' if use_random_seed else 'FIXED'}")
        print(f"   Variants           : {num_waveforms}")
        print(f"   Output Format      : {output_format.upper()}")
        print(f"   Save Path          : {save_path or 'Play in browser'}")


        # AUTO-LOAD
        if not is_model_loaded():
            device_raw = d.get("device", "0")
            print(f"[MUSIC] Loading model on GPU {device_raw}...")
            if not load_ace(device_raw):
                return jsonify({"error": "Failed to load model"}), 500

        # GENERATE
        temp_wavs = []
        results = []

        for i in range(num_waveforms):
            tmp = OUTPUT_DIR / f"ace_tmp_{uuid.uuid4().hex}.wav"
            temp_wavs.append(tmp)

            variant_seed = str(random.randint(0, 2**32 - 1)) if use_random_seed else str(int(raw_seed))
            print(f"[VARIANT {i+1}/{num_waveforms}] Seed: {variant_seed}")

            ace_generate(
                prompt=prompt,
                duration=duration,
                output=str(tmp),
                infer_step=steps,
                guidance_scale=guidance,
                scheduler_type=scheduler,
                cfg_type=cfg_type,
                omega_scale=omega,
                manual_seeds=variant_seed,
                guidance_interval=guidance_interval,
                guidance_interval_decay=guidance_decay,
                min_guidance_scale=min_guidance,
                use_erg_tag=erg_tag,
                use_erg_lyric=erg_lyric,
                use_erg_diffusion=erg_diffusion,
                oss_steps=oss_steps,
                guidance_scale_text=guidance_text,
                guidance_scale_lyric=guidance_lyric,
            )

            processed = ace_post_process(str(tmp))
            data, rate = sf.read(processed)
            score = score_with_clap(data, prompt, rate)

            results.append({
                "audio_np": data,
                "score": score,
                "path": processed,
                "seed": variant_seed
            })

            print(f"[VARIANT {i+1}] CLAP: {score:.4f} | {Path(tmp).name}")
            torch.cuda.empty_cache()

        results.sort(key=lambda x: x["score"], reverse=True)

        # SAVE WITH FORMAT CONVERSION
        if should_save:
            p = Path(save_path)
            cleaned_stem = p.stem.split(".")[0]
            final_path = p.parent / f"{cleaned_stem}.{output_format}"
            saved_files = []

            for idx, res in enumerate(results):
                is_best = idx == 0
                suffix = " (BEST)" if is_best else f"_v{idx+1}"
                variant_name = f"{final_path.stem}{suffix}{final_path.suffix}"
                variant_path = final_path.parent / variant_name

                final_wav = res["path"]
                if output_format != "wav":
                    conv_path = Path(res["path"]).with_suffix(f".{output_format}")
                    cmd = [
                        str(FFMPEG_BIN / "ffmpeg.exe"),
                        "-i", final_wav,
                        *_ffmpeg_args(output_format),
                        str(conv_path),
                        "-y"
                    ]
                    result = subprocess.run(cmd, capture_output=True, text=True)
                    if result.returncode != 0:
                        print(f"FFMPEG failed: {result.stderr}")
                        conv_path = final_wav
                    else:
                        os.remove(final_wav)
                    final_wav = str(conv_path)

                saved_path, saved_rel = handle_save(final_wav, str(variant_path), "ace")
                saved_files.append({
                    "filename": Path(saved_path).name,
                    "rel_path": saved_rel,
                    "clap_score": res["score"],
                    "is_best": is_best,
                    "seed": res["seed"]
                })
                if not is_best and Path(final_wav).exists():
                    os.remove(final_wav)

            for p in temp_wavs:
                if p.exists():
                    os.remove(p)

            return jsonify({"saved_files": saved_files, "num_generated": len(results)})

        # PLAY IN BROWSER
        audios = []
        for idx, res in enumerate(results):
            is_best = idx == 0
            with open(res["path"], "rb") as f:
                b64 = base64.b64encode(f.read()).decode()
            audios.append({
                "audio_base64": b64,
                "score": res["score"],
                "is_best": is_best,
                "seed": res["seed"]
            })
            os.remove(res["path"])

        for p in temp_wavs:
            if p.exists():
                os.remove(p)

        return jsonify({"audios": audios})

    except Exception as e:
        import traceback
        print(traceback.format_exc())
        return jsonify({"error": "Server error"}), 500

FILE: E:\tts_0\routes\admin.py

================================================================================

# routes/admin.py
import os, datetime, psutil, subprocess, pynvml
from flask import request, jsonify
from . import bp
from config import OUTPUT_DIR
from models.xtts import unload_xtts
from models.fish import unload_fish
from models.whisper import unload_whisper 

def _kill_tree(pid):
    try:
        parent = psutil.Process(pid)
        for child in parent.children(recursive=True):
            try: child.kill()
            except psutil.NoSuchProcess: pass
        parent.kill()
    except psutil.NoSuchProcess: pass

def _terminate_lingering():
    for proc in psutil.process_iter(['pid', 'name']):
        name = proc.info['name'].lower()
        if name in {"ffmpeg.exe", "ffprobe.exe", "rubberband.exe"}:
            print(f"[CLEANUP] Killing stray {name} (PID {proc.pid})")
            _kill_tree(proc.pid)

@bp.route("/shutdown", methods=["POST"])
def shutdown():
    print("[SHUTDOWN] Unloading models...")
    unload_xtts()
    unload_whisper() 
    unload_fish()
    _terminate_lingering()

    func = request.environ.get("werkzeug.server.shutdown")
    if func:
        func()
    else:
        os._exit(0)
    return jsonify({"message": "Server shutting down..."}), 200

FILE: E:\tts_0\routes\api_docs.py

================================================================================

[ERROR: File not found]

FILE: E:\tts_0\routes\chatbot.py

================================================================================

# routes/chatbot.py

from flask import Blueprint, request, jsonify, Response
import json
from pathlib import Path
from config import LLM_DIRECTORY
from datetime import datetime
import time

from models.llama import load_llama, unload_llama, infer_llama

bp = Blueprint('chatbot', __name__, url_prefix='/chatbot')

BRAIN_DIR = Path("brain")
BRAIN_DIR.mkdir(exist_ok=True)

HISTORY_DIR = BRAIN_DIR / "context_history"
HISTORY_DIR.mkdir(exist_ok=True)
CURRENT_HISTORY = HISTORY_DIR / "current.json"
ARCHIVE_DIR = HISTORY_DIR / "archives"
ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)

# Default files
(BRAIN_DIR / "system_prompt.json").write_text(
    json.dumps({"content": "You are a helpful assistant."}, indent=2),
    encoding="utf-8"
) if not (BRAIN_DIR / "system_prompt.json").exists() else None

if not CURRENT_HISTORY.exists():
    CURRENT_HISTORY.write_text(json.dumps([], indent=2), encoding="utf-8")

@bp.route('/scan_models')
def scan_models():
    if not Path(LLM_DIRECTORY).exists():
        return jsonify([])
    return jsonify(sorted(str(p) for p in Path(LLM_DIRECTORY).rglob("*.gguf")))

@bp.route('/brain/system_prompt', methods=['GET', 'POST'])
def system_prompt_manager():
    default_path = BRAIN_DIR / "system_prompt.json"

    if request.method == 'GET':
        file = request.args.get("file")
        if file:
            path = BRAIN_DIR / file
            if path.exists() and path.suffix == ".json":
                data = json.loads(path.read_text(encoding="utf-8"))
                return jsonify({"content": data.get("content", ""), "filename": file})
            return jsonify({"error": "not found"}), 404
        
        if default_path.exists():
            data = json.loads(default_path.read_text(encoding="utf-8"))
            return jsonify({"content": data.get("content", ""), "filename": None})
        return jsonify({"content": "You are a helpful assistant.", "filename": None})

    if request.method == 'POST':
        data = request.get_json() or {}
        content = data.get("content", "").strip()
        filename = data.get("filename")

        default_path.write_text(
            json.dumps({"content": content}, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )

        if filename:
            safe = "".join(c if c.isalnum() or c in " _-." else "_" for c in filename)
            safe = safe.strip()[:100]
            if not safe.lower().endswith(".json"):
                safe += ".json"
            if safe != "system_prompt.json":
                (BRAIN_DIR / safe).write_text(
                    json.dumps({"content": content}, ensure_ascii=False, indent=2),
                    encoding="utf-8"
                )

        return jsonify({"success": True})


@bp.route('/brain/system_prompt', methods=['DELETE'])
def delete_system_prompt():
    file = request.args.get("delete")
    if not file or file == "system_prompt.json":
        return jsonify({"error": "cannot delete active prompt"}), 400
    
    path = BRAIN_DIR / file
    if path.exists():
        path.unlink()
        return jsonify({"success": True})
    
    return jsonify({"error": "not found"}), 404


@bp.route('/brain/list_system_prompts')
def list_system_prompts():
    ignore = {"system_prompt.json", "current.json"}
    files = [
        f.name for f in BRAIN_DIR.glob("*.json")
        if f.name not in ignore and "archive" not in f.name.lower()
    ]
    files.sort(key=lambda f: (BRAIN_DIR / f).stat().st_mtime, reverse=True)
    return jsonify(files)


@bp.route('/brain/history', methods=['GET', 'POST'])
def brain_history():
    if request.method == 'POST':
        data = request.get_json() or []
        CURRENT_HISTORY.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
        return jsonify({"saved": True})
    return jsonify(json.loads(CURRENT_HISTORY.read_text(encoding="utf-8")))

@bp.route('/brain/save_archive', methods=['POST'])
def save_archive():
    data = request.get_json()
    filename = data.get("filename", "archive")
    history = data.get("history", [])
    
    safe = "".join(c if c.isalnum() or c in " _-." else "_" for c in filename)
    safe = safe.strip()[:100]
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")
    path = ARCHIVE_DIR / f"{timestamp}_{safe}.json"
    
    path.write_text(json.dumps(history, ensure_ascii=False, indent=2), encoding="utf-8")
    return jsonify({"saved": True})

@bp.route('/brain/list_archives')
def list_archives():
    files = sorted(ARCHIVE_DIR.glob("*.json"), key=lambda x: x.stat().st_mtime, reverse=True)
    return jsonify([f.name for f in files])

@bp.route('/brain/load_archive')
def load_archive():
    file = request.args.get("file")
    if not file:
        return jsonify({"error": "no file"}), 400
    path = ARCHIVE_DIR / file
    if not path.exists():
        return jsonify({"error": "not found"}), 404
    history = json.loads(path.read_text(encoding="utf-8"))
    return jsonify({"history": history})

@bp.route('/load', methods=['POST'])
def load():
    data = request.json or {}
    path = data.get("model_path", "").strip()
    if not path:
        return jsonify({"error": "model_path required"}), 400

    n_ctx = int(data.get("n_ctx", 8192))
    n_gpu_layers = -1 if data.get("n_gpu_layers") == 99 else int(data.get("n_gpu_layers", 0))

    try:
        message = load_llama(path, n_ctx=n_ctx, n_gpu_layers=n_gpu_layers)
        return jsonify({"success": True, "message": message})
    except Exception as e:
        unload_llama(force=True)
        error_msg = str(e)
        return jsonify({"error": error_msg}), 500

@bp.route('/unload', methods=['POST'])
def unload():
    print("[CHATBOT] Unload button pressed — forcing full reset")
    unload_llama(force=True)
    return jsonify({"success": True, "message": "Killed & reset — try again"})

@bp.route('/infer', methods=['POST'])
def infer():
    from models.llama import llm, model_loaded
    if not model_loaded or llm is None:
        return jsonify({"error": "No model loaded"}), 400

    data = request.get_json() or {}
    messages = data.get("messages", [])  # ← System prompt stays intact!

    temperature        = float(data.get("temperature", 0.8))
    max_tokens         = int(data.get("max_tokens", 8192))
    top_p              = float(data.get("top_p", 0.95))
    top_k              = int(data.get("top_k", 40))
    presence_penalty   = float(data.get("presence_penalty", 0.0))
    frequency_penalty  = float(data.get("frequency_penalty", 0.0))

    print("\n" + "="*88)
    print(" LOCAL LLAMA.CPP INFERENCE STARTED ")
    print("="*88)
    print(f"Temperature        : {temperature}")
    print(f"Max tokens         : {max_tokens}")
    print(f"Top P              : {top_p}")
    print(f"Top K              : {top_k}")
    print(f"Presence penalty   : {presence_penalty}")
    print(f"Frequency penalty  : {frequency_penalty}")
    print(f"Messages sent      : {len(messages)}")
    for i, msg in enumerate(messages):
        role = msg["role"].upper()
        preview = msg["content"].replace("\n", " ")[:150] + ("..." if len(msg["content"]) > 150 else "")
        print(f"  [{i:02d}] {role:<6} → {preview}")
    print("-" * 88)

    def generate():
        for token in infer_llama(
            messages,  # ← Direct pass — no tampering!
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p,
            top_k=top_k,
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty
        ):
            yield token

    return Response(generate(), mimetype='text/plain')

@bp.route('/status')
def status():
    from models.llama import model_loaded, current_model_path, loading_in_progress
    return jsonify({
        "loaded": model_loaded,
        "path": current_model_path or "—",
        "loading": loading_in_progress
    })

FILE: E:\tts_0\routes\infer_xtts.py

================================================================================

# routes/infer_xtts.py
import os
import uuid
from datetime import datetime
import base64
import time
import re
import json
import queue
import subprocess
from pathlib import Path
import numpy as np
import soundfile as sf
import torch
from flask import request, jsonify
from . import bp

from config import (
    OUTPUT_DIR, VOICE_DIR, PROJECTS_OUTPUT, XTTS_AUTO_TRIGGER_JOB_RECOVERY_ATTEMPTS,
    FFMPEG_BIN, XTTS_INTER_PAUSE, XTTS_PADDING_SECONDS,
    XTTS_FRONT_PAD, resolve_device
)
import models.xtts as xtts_mod
import models.whisper as whisper_mod
from text_utils import split_text_xtts
from save_utils import handle_save
from audio_post_XTTS import (
    post_process_xtts, verify_with_whisper,
    _trim_silence_xtts
)

def _ts():
    return time.strftime("%H:%M:%S")

cancel_queue = queue.Queue()

def is_cancelled() -> bool:
    """
    Check if the current generation has been cancelled via /xtts_cancel.
    Used inside long-running loops to allow graceful early exit.
    """
    try:
        cancel_queue.get_nowait()
        print(f"[{_ts()} XTTS_INFER] Cancellation detected")
        return True
    except queue.Empty:
        return False

@bp.route("/xtts_cancel", methods=["POST"])
def xtts_cancel():
    """
    Endpoint to cancel an in-progress XTTS generation.
    Puts a cancellation token into the shared queue.
    """
    print(f"[{_ts()} XTTS_INFER] Cancel request received")
    cancel_queue.put(True)
    return jsonify({"message": "XTTS generation cancelled"})

def _ffmpeg_args(fmt: str):
    """
    Return appropriate ffmpeg encoding arguments for common output formats.
    Used when converting final WAV to mp3/ogg/flac/m4a.
    """
    args = {
        "mp3": ["-c:a", "libmp3lame", "-q:a", "0"],
        "ogg": ["-c:a", "libvorbis", "-q:a", "6"],
        "flac": ["-c:a", "flac", "-compression_level", "12"],
        "m4a": ["-c:a", "aac", "-b:a", "320k"]
    }.get(fmt, [])
    return args

@bp.route("/infer", methods=["POST"])
def infer():
    print(f"\n{'='*100}")
    print(f"[{_ts()} XTTS_INFER] NEW XTTS INFERENCE REQUEST")
    print(f"{'='*100}")
    d = request.json

    # === PARAMETER DUMP ===
    print(f"[{_ts()} XTTS_INFER] → Voice           : {d.get('voice', 'MISSING')}")
    print(f"[{_ts()} XTTS_INFER] → Mode            : {d.get('mode', 'cloned')}")
    print(f"[{_ts()} XTTS_INFER] → Language        : {d.get('language', 'en')}")
    print(f"[{_ts()} XTTS_INFER] → Temperature     : {d.get('temperature', 0.65):.2f}")
    print(f"[{_ts()} XTTS_INFER] → Speed           : {d.get('speed', 1.0):.2f}")
    print(f"[{_ts()} XTTS_INFER] → Repetition Pen  : {d.get('repetition_penalty', 2.0):.3f}")
    print(f"[{_ts()} XTTS_INFER] → De-ess          : {float(d.get('de_ess', 0))/100:.2f}")
    print(f"[{_ts()} XTTS_INFER] → De-reverb       : {d.get('de_reverb', 0.7):.2f}")
    print(f"[{_ts()} XTTS_INFER] → Tolerance       : {float(d.get('tolerance', 80))}%")
    print(f"[{_ts()} XTTS_INFER] → Verify Whisper  : {d.get('verify_whisper', True)}")
    print(f"[{_ts()} XTTS_INFER] → Output Format   : {d.get('output_format', 'wav')}")
    print(f"[{_ts()} XTTS_INFER] → Save Path       : {d.get('save_path') or '← temp'}")
    print(f"[{_ts()} XTTS_INFER] → Text Length     : {len(d.get('text',''))} chars")
    print(f"{'-'*100}")

    raw_text = d.get("text", "").strip()

    # ——————————————————— RECOVERY MODE ———————————————————
    
    
    if raw_text.strip().lower() == "##recover##":
        target = (d.get("save_path") or "").strip()
        if not target:
            return jsonify({"message": "Set save_path to the folder you want to recover"}), 400

        job_dir = PROJECTS_OUTPUT / target if "/" not in target and "\\" not in target else Path(target).expanduser().resolve()
        job_file = job_dir / "job.json"

        if not job_file.exists():
            return jsonify({"message": f"No job found in folder: '{target}'"}), 400

        try:
            job_data = json.load(open(job_file, "r", encoding="utf-8"))
        except Exception as e:
            return jsonify({"message": f"job.json corrupted in '{target}': {e}"}), 400

        if job_data.get("chunks_completed", 0) >= job_data.get("total_chunks", 0):
            return jsonify({"message": f"Job in '{target}' is already finished"}), 400

        # ←←← NOW WE DEFINE THE VARIABLES FIRST
        d = job_data["parameters"].copy()
        text = job_data["input_text"]
        chunks = [c["text"] for c in job_data["chunks"]]
        start_from_chunk = job_data["chunks_completed"]
        save_path_raw = target
        output_format = job_data.get("output_format", "wav").lower()

        # ←←← NOW IT'S SAFE TO PRINT
        print(f"\n{'='*100}")
        print(f"[{_ts()} XTTS_INFER] RECOVERY MODE — Resuming job: {target}")
        print(f"[{_ts()} XTTS_INFER] Progress: {start_from_chunk}/{job_data['total_chunks']} chunks")
        print(f"{'='*100}")
        print(f"[{_ts()} XTTS_INFER] → Voice           : {d.get('voice', 'MISSING')}")
        print(f"[{_ts()} XTTS_INFER] → Language        : {d.get('language', 'en')}")
        print(f"[{_ts()} XTTS_INFER] → Temperature     : {d.get('temperature', 0.65):.2f}")
        print(f"[{_ts()} XTTS_INFER] → Speed           : {d.get('speed', 1.0):.2f}")
        print(f"[{_ts()} XTTS_INFER] → Repetition Pen  : {d.get('repetition_penalty', 2.0):.3f}")
        print(f"[{_ts()} XTTS_INFER] → De-ess          : {float(d.get('de_ess', 0))/100:.2f}")
        print(f"[{_ts()} XTTS_INFER] → De-reverb       : {d.get('de_reverb', 0.7):.2f}")
        print(f"[{_ts()} XTTS_INFER] → Tolerance       : {float(d.get('tolerance', 80))}%")
        print(f"[{_ts()} XTTS_INFER] → Verify Whisper  : {d.get('verify_whisper', False)}")
        print(f"[{_ts()} XTTS_INFER] → Output Format   : {output_format.upper()}")
        print(f"{'-'*100}\n")




    # ——————————————————— NEW JOB ———————————————————
    else:
        text = raw_text
        if not text:
            return jsonify({"error": "Missing text"}), 400

        chunks = split_text_xtts(text.strip(), max_chars=250)
        start_from_chunk = 0
        save_path_raw = d.get("save_path") or None
        output_format = d.get("output_format", "wav").lower()

    # ——————————————————— JOB DIRECTORY & JSON ———————————————————
    save_path_input = (save_path_raw or "").strip()
    if not save_path_input:
        stem = f"xtts_{int(time.time())}"
        job_dir = OUTPUT_DIR / f"temp_{stem}"
        final_stem = stem
    elif "/" in save_path_input or "\\" in save_path_input:
        full_path = Path(save_path_input).expanduser().resolve()
        job_dir = full_path.parent if full_path.suffix else full_path
        final_stem = full_path.stem if full_path.suffix else full_path.name
    else:
        job_dir = PROJECTS_OUTPUT / save_path_input
        final_stem = save_path_input

    job_dir.mkdir(parents=True, exist_ok=True)
    stem = final_stem
    job_file = job_dir / "job.json"

    if not job_file.exists():
        job_payload = {
            "job_id": str(uuid.uuid4()),
            "model": "xtts",
            "timestamp": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
            "status": "running",
            "input_text": text,
            "total_chunks": len(chunks),
            "chunks_completed": start_from_chunk,
            "total_duration_sec": None,
            "sample_rate": 22050,
            "output_format": output_format,
            "final_file": None,
            "expected_files": [f"chunk_{i:03d}.wav" for i in range(len(chunks))] + [f"{stem}_final.{output_format}"],
            "missing_files": [f"chunk_{i:03d}.wav" for i in range(start_from_chunk, len(chunks))] + [f"{stem}_final.{output_format}"],
            "chunks": [
                {
                    "index": i,
                    "text": c,
                    "char_length": len(c),
                    "duration_sec": None,
                    "file": f"chunk_{i:03d}.wav",
                    "verification_passed": None,
                    "whisper_transcript": None,
                    "processing_error": None
                }
                for i, c in enumerate(chunks)
            ],
            "parameters": d.copy(), 
            "failure_reason": None  
        }
        with open(job_file, "w", encoding="utf-8") as f:
            json.dump(job_payload, f, ensure_ascii=False, indent=2)

    else:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            if "failure_reason" not in j:
                j["failure_reason"] = None

            j["status"] = "running"
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()



    de_ess = float(d.get("de_ess", 0)) / 100.0
    tolerance = float(d.get("tolerance", 80))

    mode = d.get("mode", "cloned")
    voice = d.get("voice", "")
    speaker_param = (
        {"speaker_wav": str(VOICE_DIR / voice)} if mode == "cloned" else {"speaker": voice}
    )

    base_params = {
        "language": d.get("language", "en"),
        "temperature": d.get("temperature", 0.65),
        "speed": d.get("speed", 1.0),
        "repetition_penalty": float(d.get("repetition_penalty") or 2.000000001),
        "split_sentences": False,
        **speaker_param
    }

    # ——————————————————— GENERATION — SINGLE ATTEMPT, FAIL ONCE ———————————————————
    audio_parts = []
    sr = None

    max_retries = int(d.get("auto_retry", XTTS_AUTO_TRIGGER_JOB_RECOVERY_ATTEMPTS))

    try:
        with torch.no_grad():
            for i in range(start_from_chunk, len(chunks)):
                if is_cancelled():
                    return jsonify({"error": "Cancelled"}), 499

                chunk = chunks[i]
                retry_count = 0

                while True:
                    try:
                        
                        # ——————————————————— MODEL LOADING ———————————————————
                        if not xtts_mod.model_loaded:
                            dev_input = d.get("xttsDeviceSelect") or d.get("device")
                            resolved_dev = resolve_device(dev_input)
                            print(f"[MODEL] XTTS not loaded → loading on {resolved_dev}")
                            xtts_mod.load_xtts(resolved_dev)

                        verify_whisper = d.get("verify_whisper", False)
                        if verify_whisper:
                            target_dev = d.get("whisperDeviceSelect") or "cpu"
                            resolved_dev = resolve_device(target_dev)
                            if whisper_mod.whisper_model is None or whisper_mod._current_device != resolved_dev:
                                print(f"[MODEL] verify_whisper=True → loading Whisper on {resolved_dev}")
                                whisper_mod.load_whisper(resolved_dev)
                        else:
                            if whisper_mod.whisper_model is not None:
                                print(f"[MODEL] verify_whisper=False → unloading Whisper")
                                whisper_mod.unload_whisper()                        
                        
                        
                        wav = xtts_mod.tts_model.tts(text=chunk, **base_params)
                        data = np.array(wav, dtype=np.float32)
                        if sr is None:
                            sr = xtts_mod.tts_model.synthesizer.output_sample_rate

                        data = np.concatenate([np.zeros(int(sr * XTTS_FRONT_PAD), dtype=np.float32), data])

                        tmp = OUTPUT_DIR / f"raw_{i}_{uuid.uuid4().hex}.wav"
                        sf.write(tmp, data, sr, subtype="PCM_16")

                        skip_post_process = d.get("skip_post_process", False)
                        if not skip_post_process:
                            processed = post_process_xtts(str(tmp), d.get("speed", 1.0), d.get("de_reverb", 0.7), de_ess)
                        else:
                            processed = str(tmp)
                            _trim_silence_xtts(processed)

                        if verify_whisper:
                            if not verify_with_whisper(processed, chunk, d.get("language", "en"), tolerance, job_file, i):
                                handle_save(processed, None, "xtts", always_save_fails=True)
                                raise ValueError("Whisper verification failed")

                        data, _ = sf.read(processed)
                        duration_sec = len(data) / sr
                        audio_parts.append(data)

                        chunk_wav = job_dir / f"chunk_{i:03d}.wav"
                        os.replace(processed, str(chunk_wav))
                        print(f"[{_ts()} CHUNK] {i:03d} → {duration_sec:.2f}s (success)")

                        _update_chunk_success(job_file, i, duration_sec)

                        # Record retry count (you expected this)
                        try:
                            with open(job_file, "r+", encoding="utf-8") as f:
                                j = json.load(f)
                                j["chunk_retry_counts"][str(i)] = retry_count
                                f.seek(0)
                                json.dump(j, f, ensure_ascii=False, indent=2)
                                f.truncate()
                        except:
                            pass

                        break  # success → next chunk

                    except Exception as e:
                        retry_count += 1
                        error_msg = str(e) or "Unknown error"

                        if retry_count > max_retries:
                            print(f"[{_ts()} CHUNK FAILED] {i:03d} → failed after {max_retries} retries → giving up")
                            _record_chunk_error(job_file, i, f"Permanently failed after {max_retries} retries: {error_msg}")
                            _mark_job_failed(job_file, f"Chunk {i} failed after {max_retries} retries")
                            return jsonify({
                                "error": "generation_failed",
                                "reason": f"Chunk {i} failed after {max_retries} retries",
                                "failed_at_chunk": i,
                                "job_folder": str(job_dir.name),
                                "recover_command": f"##recover## (save_path: {job_dir.name})"
                            }), 200

                        print(f"[{_ts()} CHUNK RETRY] {i:03d} → attempt {retry_count}/{max_retries} failed ({error_msg}) → retrying...")
                        time.sleep(1)  # tiny pause so GPU can breathe

    except Exception as e:
        # This should never fire now
        error_str = str(e) or "Unknown error"
        print(f"[{_ts()} FAILED] Unexpected error: {error_str}")
        _mark_job_failed(job_file, error_str)
        return jsonify({
            "error": "generation_failed",
            "reason": error_str,
            "job_folder": str(job_dir.name)
        }), 200

    # ——————————————————— FINAL ASSEMBLY ———————————————————
    missing_chunks = [
        f"chunk_{i:03d}.wav" for i in range(len(chunks))
        if not (job_dir / f"chunk_{i:03d}.wav").exists()
    ]

    if missing_chunks:
        print(f"[{_ts()} ASSEMBLY] SKIPPED — {len(missing_chunks)} chunk(s) missing. Use ##recover##")
        try:
            with open(job_file, "r+", encoding="utf-8") as f:
                j = json.load(f)
                j["missing_files"] = missing_chunks + [f"{stem}_final.{output_format}"]
                f.seek(0)
                json.dump(j, f, ensure_ascii=False, indent=2)
                f.truncate()
        except: pass

        return jsonify({
            "status": "incomplete",
            "message": f"Waiting for {len(missing_chunks)} missing chunk(s)",
            "missing_count": len(missing_chunks),
            "job_folder": str(job_dir.name),
            "recover_command": f"##recover## (save_path: {job_dir.name})"
        }), 200

    # ——— ALL CHUNKS PRESENT → BUILD FINAL AUDIO ———
    print(f"[{_ts()} ASSEMBLY] All {len(chunks)} chunks ready → assembling final file...")

    parts = [sf.read(job_dir / f"chunk_{i:03d}.wav")[0] for i in range(len(chunks))]

    inter = np.zeros(int(sr * XTTS_INTER_PAUSE), dtype=np.float32)
    pad = np.zeros(int(sr * XTTS_PADDING_SECONDS), dtype=np.float32)
    final_wav = np.concatenate([pad, parts[0], *[np.concatenate([inter, p]) for p in parts[1:]], pad])

    final_temp = OUTPUT_DIR / f"final_{uuid.uuid4().hex}.wav"
    sf.write(final_temp, final_wav, sr, subtype="PCM_16")

    final_path = final_temp.with_suffix(f".{output_format}") if output_format != "wav" else final_temp
    if output_format != "wav":
        subprocess.run([
            str(FFMPEG_BIN / "ffmpeg.exe"), "-i", str(final_temp),
            "-y", str(final_path)
        ] + _ffmpeg_args(output_format), check=True)
        final_temp.unlink()

    final_save_path = job_dir / f"{stem}_final.{output_format}"
    final_path.replace(final_save_path)

    try:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            j["status"] = "completed"
            j["total_duration_sec"] = round(len(final_wav) / sr, 3)
            j["final_file"] = final_save_path.name
            j["missing_files"] = []
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()
        print(f"[{_ts()} COMPLETE] {final_save_path.name} — {len(final_wav)/sr:.1f}s")
    except: pass

    rel_path = str(final_save_path.relative_to(Path.cwd())).replace("\\", "/")
    resp = {
        "filename": final_save_path.name,
        "saved_to": str(final_save_path),
        "saved_rel": rel_path,
        "sample_rate": sr,
        "duration_sec": round(len(final_wav) / sr, 3),
        "format": output_format
    }
    if not save_path_raw:
        resp = {"audio_base64": base64.b64encode(final_save_path.read_bytes()).decode("utf-8")}
        final_save_path.unlink(missing_ok=True)

    print(f"[{_ts()} DONE] XTTS job finished")
    return jsonify(resp)


def _record_chunk_error(job_file: Path, chunk_idx: int, message: str):
    try:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            j["chunks"][chunk_idx]["processing_error"] = message
            j["chunks"][chunk_idx]["verification_passed"] = False
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()
    except:
        pass

def _update_chunk_success(job_file: Path, chunk_idx: int, duration: float):
    try:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            j["chunks_completed"] = chunk_idx + 1
            j["chunks"][chunk_idx]["duration_sec"] = round(duration, 3)
            j["chunks"][chunk_idx]["verification_passed"] = True
            j["chunks"][chunk_idx]["processing_error"] = None
            chunk_file_name = f"chunk_{chunk_idx:03d}.wav"
            if chunk_file_name in j["missing_files"]:
                j["missing_files"].remove(chunk_file_name)
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()
    except:
        pass

def _mark_job_failed(job_file: Path, reason: str):
    try:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            j["status"] = "failed"
            j["failure_reason"] = reason
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()
    except:
        pass

@bp.route("/xtts_builtin_speakers")
def xtts_builtin_speakers():
    from models.xtts import get_builtin_speakers
    return jsonify(sorted(get_builtin_speakers()))

@bp.route("/list_voice_files")
def list_voice_files():
    from config import VOICE_DIR
    from pathlib import Path
    files = [f.name for f in Path(VOICE_DIR).glob("*.wav") if f.is_file()]
    return jsonify(sorted(files))

FILE: E:\tts_0\routes\infer_fish.py

================================================================================

# routes/infer_fish.py
"""
Flask route for FishSpeech inference with full-featured generation pipeline.

Handles:
- Long-text chunking
- Multi-attempt generation with automatic retry on Whisper verification failure
- Live job.json progress tracking for frontend polling
- Robust recovery system via "##recover##" magic text
- Cancellation support via /fish_cancel endpoint
- Final assembly with configurable inter-chunk pause and global padding
- Optional conversion to mp3/ogg/flac/m4a via ffmpeg
- Base64 return for temporary jobs, persistent files for saved projects

All temporary files and job state are managed safely across crashes/recoveries.
"""
import torch
import os
import uuid
from datetime import datetime
import base64
import time
import re
import json
import queue
import subprocess
from pathlib import Path
import numpy as np
import soundfile as sf
from flask import request, jsonify
from . import bp
from save_utils import handle_save
from config import (
    OUTPUT_DIR, VOICE_DIR, PROJECTS_OUTPUT, FISH_AUTO_TRIGGER_JOB_RECOVERY_ATTEMPTS,
    FFMPEG_BIN, FISH_INTER_PAUSE, FISH_PADDING_SECONDS, resolve_device
)
import models.fish as fish_mod
import models.whisper as whisper_mod
from text_utils import split_text_fish
from audio_post_FISH import verify_with_whisper, _trim_silence_fish, post_process_fish

def _ts():
    return time.strftime("%H:%M:%S")

cancel_queue = queue.Queue()

def is_cancelled() -> bool:
    try:
        cancel_queue.get_nowait()
        print(f"[{_ts()} FISH_INFER] CANCELLATION DETECTED")
        return True
    except queue.Empty:
        return False

@bp.route("/fish_cancel", methods=["POST"])
def fish_cancel():
    print(f"[{_ts()} FISH_INFER] CANCEL REQUEST RECEIVED")
    cancel_queue.put(True)
    return jsonify({"message": "Fish generation cancelled"})

def _ffmpeg_args(fmt: str):
    """Return ffmpeg codec/quality arguments for the requested output format.

    Args:
        fmt: Target format string (mp3, ogg, flac, m4a, etc.).

    Returns:
        List of ffmpeg arguments for that format.
    """
    args = {
        "mp3": ["-c:a", "libmp3lame", "-q:a", "0"],
        "ogg": ["-c:a", "libvorbis", "-q:a", "6"],
        "flac": ["-c:a", "flac", "-compression_level", "12"],
        "m4a": ["-c:a", "aac", "-b:a", "320k"]
    }.get(fmt, [])
    print(f"[{_ts()} FISH_INFER] FFMPEG args for {fmt}: {args}")
    return args

@bp.route("/fish_infer", methods=["POST"])
def fish_infer():
    print(f"\n{'='*100}")
    print(f"[{_ts()} FISH_INFER] NEW FISH INFERENCE REQUEST")
    print(f"{'='*100}")
    d = request.json
    print(f"[{_ts()} FISH_INFER] → Voice           : {d.get('voice', 'MISSING')}")
    print(f"[{_ts()} FISH_INFER] → Mode            : {d.get('mode', 'cloned')}")
    print(f"[{_ts()} FISH_INFER] → Language        : {d.get('language', 'en')}")
    print(f"[{_ts()} FISH_INFER] → Temperature     : {d.get('temperature', 0.65):.2f}")
    print(f"[{_ts()} FISH_INFER] → Speed           : {d.get('speed', 1.0):.2f}")
    print(f"[{_ts()} FISH_INFER] → Repetition Pen  : {d.get('repetition_penalty', 2.0):.3f}")
    print(f"[{_ts()} FISH_INFER] → De-ess          : {float(d.get('de_ess', 0))/100:.2f}")
    print(f"[{_ts()} FISH_INFER] → De-reverb       : {d.get('de_reverb', 0.7):.2f}")
    print(f"[{_ts()} FISH_INFER] → Tolerance       : {float(d.get('tolerance', 80))}%")
    print(f"[{_ts()} FISH_INFER] → Verify Whisper  : {d.get('verify_whisper', True)}")
    print(f"[{_ts()} FISH_INFER] → Output Format   : {d.get('output_format', 'wav')}")
    print(f"[{_ts()} FISH_INFER] → Save Path       : {d.get('save_path') or '← temp'}")
    print(f"[{_ts()} FISH_INFER] → Text Length     : {len(d.get('text',''))} chars")
    print(f"{'-'*100}")
    raw_text = d.get("text", "").strip()

    # ——————— RECOVERY ———————
    if raw_text.strip().lower() == "##recover##":
        target = (d.get("save_path") or "").strip()
        if not target:
            return jsonify({"message": "Set save_path to the folder you want to recover"}), 400

        job_dir = PROJECTS_OUTPUT / target if "/" not in target and "\\" not in target else Path(target).expanduser().resolve()
        job_file = job_dir / "job.json"

        if not job_file.exists():
            return jsonify({"message": f"No job found in folder: '{target}'"}), 400

        try:
            job_data = json.load(open(job_file, "r", encoding="utf-8"))
        except Exception as e:
            return jsonify({"message": f"job.json corrupted in '{target}': {e}"}), 400

        if job_data.get("chunks_completed", 0) >= job_data.get("total_chunks", 0):
            return jsonify({"message": f"Job in '{target}' is already finished"}), 400

        # ←←← NOW WE DEFINE THE VARIABLES FIRST
        d = job_data["parameters"].copy()
        text = job_data["input_text"]
        chunks = [c["text"] for c in job_data["chunks"]]
        start_from_chunk = job_data["chunks_completed"]
        save_path_raw = target
        output_format = job_data.get("output_format", "wav").lower()

        # ←←← NOW IT'S SAFE TO PRINT
        print(f"\n{'='*100}")
        print(f"[{_ts()} FISH_INFER] RECOVERY MODE — Resuming job: {target}")
        print(f"[{_ts()} FISH_INFER] Progress: {start_from_chunk}/{job_data['total_chunks']} chunks")
        print(f"{'='*100}")
        print(f"[{_ts()} FISH_INFER] → Voice           : {d.get('voice', 'MISSING')}")
        print(f"[{_ts()} FISH_INFER] → Language        : {d.get('language', 'en')}")
        print(f"[{_ts()} FISH_INFER] → Temperature     : {d.get('temperature', 0.65):.2f}")
        print(f"[{_ts()} FISH_INFER] → Speed           : {d.get('speed', 1.0):.2f}")
        print(f"[{_ts()} FISH_INFER] → Repetition Pen  : {d.get('repetition_penalty', 2.0):.3f}")
        print(f"[{_ts()} FISH_INFER] → De-ess          : {float(d.get('de_ess', 0))/100:.2f}")
        print(f"[{_ts()} FISH_INFER] → De-reverb       : {d.get('de_reverb', 0.7):.2f}")
        print(f"[{_ts()} FISH_INFER] → Tolerance       : {float(d.get('tolerance', 80))}%")
        print(f"[{_ts()} FISH_INFER] → Verify Whisper  : {d.get('verify_whisper', False)}")
        print(f"[{_ts()} FISH_INFER] → Output Format   : {output_format.upper()}")
        print(f"{'-'*100}\n")      

    # ——————— NEW JOB ———————
    else:
        text = raw_text
        if not text:
            return jsonify({"error": "Missing text"}), 400
        chunks = split_text_fish(text.strip(), max_chars=300)
        start_from_chunk = 0
        save_path_raw = d.get("save_path") or None
        output_format = d.get("output_format", "wav").lower()

    # ——————— JOB DIR & JSON ———————
    save_path_input = (save_path_raw or "").strip()

    if not save_path_input:
        stem = f"fish_{int(time.time())}"
        job_dir = OUTPUT_DIR / f"temp_{stem}"
        final_stem = stem
    elif "/" in save_path_input or "\\" in save_path_input:
        # Has path separator → treat as full path
        full_path = Path(save_path_input).expanduser().resolve()
        job_dir = full_path.parent if full_path.suffix else full_path
        final_stem = full_path.stem if full_path.suffix else full_path.name
    else:
        # No slash → project name under PROJECTS_OUTPUT
        job_dir = PROJECTS_OUTPUT / save_path_input
        final_stem = save_path_input

    job_dir.mkdir(parents=True, exist_ok=True)
    stem = final_stem
    job_file = job_dir / "job.json"

    if not job_file.exists():
        job_payload = {
            "job_id": str(uuid.uuid4()),
            "model": "fish",
            "timestamp": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
            "status": "running",
            "input_text": text,
            "total_chunks": len(chunks),
            "chunks_completed": start_from_chunk,
            "total_duration_sec": None,
            "sample_rate": 24000,
            "output_format": output_format,
            "final_file": None,
            "expected_files": [f"chunk_{i:03d}.wav" for i in range(len(chunks))] + [f"{stem}_final.{output_format}"],
            "missing_files": [f"chunk_{i:03d}.wav" for i in range(start_from_chunk, len(chunks))] + [f"{stem}_final.{output_format}"],
            "chunks": [
                {
                    "index": i,
                    "text": c,
                    "char_length": len(c),
                    "duration_sec": None,
                    "file": f"chunk_{i:03d}.wav",
                    "verification_passed": None,
                    "whisper_transcript": None,
                    "processing_error": None
                }
                for i, c in enumerate(chunks)
            ],
            "parameters": d.copy(), 
            "failure_reason": None  
        }
        with open(job_file, "w", encoding="utf-8") as f:
            json.dump(job_payload, f, ensure_ascii=False, indent=2)

    else:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            if "failure_reason" not in j:
                j["failure_reason"] = None

            j["status"] = "running"
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()



    # ——————— REFERENCE ENCODING (once) ———————
    ref_path = VOICE_DIR / d.get("voice")
    demo = fish_mod.FishSpeechDemo(
        ref_text=d.get("ref_text", ""),
        ref_audio=str(ref_path),
        temperature=float(d.get("fishTemp", 0.7)),
        top_p=float(d.get("fishTopP", 0.7)),
        max_tokens=int(d.get("fishMaxTokens", 0)),
        speed=float(d.get("speed", 1.0)),
        de_reverb=float(d.get("de_reverb", 0.7)),
        de_ess=float(d.get("de_ess", 0))/100.0,
        gpu_id=getattr(fish_mod, "fish_device_id", "0"),
    )
    # ——————— GENERATION WITH FULL SAFETY & CONSISTENCY ———————
    max_retries = int(d.get("auto_retry", FISH_AUTO_TRIGGER_JOB_RECOVERY_ATTEMPTS))      # ← now defaults to 3 like the others
    tolerance = float(d.get("tolerance", 80))      # ← pulled out for clarity
    sr = 24000
    audio_parts = []

    try:
        with torch.no_grad():
            # Load models once
            if not fish_mod.fish_loaded:
                dev = resolve_device(d.get("fishDeviceSelect") or "cuda:0")
                print(f"[MODEL] FISH not loaded → loading on {dev}")
                fish_mod.load_fish(dev)

            verify_whisper = d.get("verify_whisper", False)
            skip_post_process = d.get("skip_post_process", False)

            if verify_whisper:
                dev = resolve_device(d.get("whisperDeviceSelect") or "cpu")
                if whisper_mod.whisper_model is None or whisper_mod._current_device != dev:
                    print(f"[MODEL] verify_whisper=True → loading Whisper on {dev}")
                    whisper_mod.load_whisper(dev)
            else:
                if whisper_mod.whisper_model is not None:
                    print(f"[MODEL] verify_whisper=False → unloading Whisper to free VRAM")
                    whisper_mod.unload_whisper()
                    
            for i in range(start_from_chunk, len(chunks)):
                if is_cancelled():
                    return jsonify({"error": "Cancelled"}), 499

                chunk_text = chunks[i]
                retry_count = 0

                while True:
                    try:
                        out_wav = job_dir / f"temp_{i}_{uuid.uuid4().hex}.wav"
                        path, dur = demo.infer(text=chunk_text, output_wav=str(out_wav))

                        processed = path
                        if not skip_post_process:
                            processed = post_process_fish(path)
                        else:
                            _trim_silence_fish(processed)

                        if verify_whisper:
                            if not verify_with_whisper(
                                processed,
                                chunk_text,
                                d.get("language", "en"),
                                tolerance,   
                                job_file,
                                i
                            ):
                                handle_save(processed, None, "fish", always_save_fails=True)
                                _record_chunk_error(job_file, i, "Whisper verification failed")
                                raise ValueError("Whisper verification failed")

                        data, _ = sf.read(processed)
                        audio_parts.append(data)

                        final_chunk = job_dir / f"chunk_{i:03d}.wav"
                        Path(processed).replace(final_chunk)
                        print(f"[{_ts()} FISH] {i:03d} → {dur:.2f}s (success)")

                        _update_chunk_success(job_file, i, dur)

                        # Track retry count
                        try:
                            with open(job_file, "r+", encoding="utf-8") as f:
                                j = json.load(f)
                                j.setdefault("chunk_retry_counts", {})[str(i)] = retry_count
                                f.seek(0)
                                json.dump(j, f, ensure_ascii=False, indent=2)
                                f.truncate()
                        except:
                            pass

                        break  # ← success → next chunk

                    except Exception as e:
                        retry_count += 1
                        error_msg = str(e) or "Unknown error"

                        if retry_count > max_retries:
                            print(f"[{_ts()} FISH CHUNK FAILED] {i:03d} → failed after {max_retries} retries → giving up")
                            _record_chunk_error(job_file, i, f"Permanently failed: {error_msg}")
                            _mark_job_failed(job_file, f"Fish chunk {i} failed after {max_retries} retries")
                            return jsonify({
                                "error": "generation_failed",
                                "reason": f"Fish chunk {i} failed after {max_retries} retries",
                                "failed_at_chunk": i,
                                "job_folder": str(job_dir.name),
                                "recover_command": f"##recover## (save_path: {job_dir.name})"
                            }), 200

                        print(f"[{_ts()} FISH RETRY] {i:03d} → {retry_count}/{max_retries} failed ({error_msg}) → retrying...")
                        time.sleep(1)

    except Exception as e:
        error_str = str(e) or "Unknown error"
        print(f"[{_ts()} FISH FAILED] Unexpected error: {error_str}")
        _mark_job_failed(job_file, error_str)
        return jsonify({
            "error": "generation_failed",
            "reason": error_str,
            "job_folder": str(job_dir.name)
        }), 200

    # ——————— FINAL ASSEMBLY ———————
    missing_chunks = [
        f"chunk_{i:03d}.wav" for i in range(len(chunks))
        if not (job_dir / f"chunk_{i:03d}.wav").exists()
    ]

    if missing_chunks:
        print(f"[{_ts()} FISH] ASSEMBLY SKIPPED — {len(missing_chunks)} chunk(s) missing")
        try:
            with open(job_file, "r+", encoding="utf-8") as f:
                j = json.load(f)
                j["missing_files"] = missing_chunks + [f"{final_stem}_final.{output_format}"]
                f.seek(0)
                json.dump(j, f, ensure_ascii=False, indent=2)
                f.truncate()
        except: pass

        return jsonify({
            "status": "incomplete",
            "message": f"Waiting for {len(missing_chunks)} missing chunk(s)",
            "missing_count": len(missing_chunks),
            "job_folder": str(job_dir.name),
            "recover_command": f"##recover## (save_path: {job_dir.name})"
        }), 200

    # ——— ALL CHUNKS PRESENT → BUILD FINAL AUDIO ———
    print(f"[{_ts()} FISH] All {len(chunks)} chunks ready → assembling...")

    parts = [sf.read(job_dir / f"chunk_{i:03d}.wav")[0] for i in range(len(chunks))]

    inter = np.zeros(int(sr * FISH_INTER_PAUSE), dtype=np.float32)
    pad = np.zeros(int(sr * FISH_PADDING_SECONDS), dtype=np.float32)
    final_wav = np.concatenate([pad, parts[0], *[np.concatenate([inter, p]) for p in parts[1:]], pad])

    tmp = OUTPUT_DIR / f"final_{uuid.uuid4().hex}.wav"
    sf.write(tmp, final_wav, sr, subtype="PCM_16")

    final_path = tmp.with_suffix(f".{output_format}") if output_format != "wav" else tmp
    if output_format != "wav":
        subprocess.run([
            str(FFMPEG_BIN / "ffmpeg.exe"), "-i", str(tmp),
            "-y", str(final_path)
        ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        tmp.unlink()

    final_save = job_dir / f"{final_stem}_final.{output_format}"
    final_path.replace(final_save)

    try:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            j["status"] = "completed"
            j["total_duration_sec"] = round(len(final_wav) / sr, 3)
            j["final_file"] = final_save.name
            j["missing_files"] = []
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()
    except: pass

    resp = {
        "filename": final_save.name,
        "saved_to": str(final_save),
        "saved_rel": str(final_save.relative_to(Path.cwd())).replace("\\", "/"),
        "sample_rate": sr,
        "duration_sec": round(len(final_wav)/sr, 3),
        "format": output_format
    }
    if not save_path_raw:
        resp = {"audio_base64": base64.b64encode(final_save.read_bytes()).decode()}
        final_save.unlink(missing_ok=True)

    print(f"[{_ts()} FISH] DONE → {final_save.name}")
    return jsonify(resp)

def _record_chunk_error(job_file: Path, idx: int, msg: str):
    try:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            j["chunks"][idx]["processing_error"] = msg
            j["chunks"][idx]["verification_passed"] = False
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()
    except: pass

def _update_chunk_success(job_file: Path, idx: int, duration: float):
    try:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            j["chunks_completed"] = idx + 1
            j["chunks"][idx]["duration_sec"] = round(duration, 3)
            j["chunks"][idx]["verification_passed"] = True
            j["chunks"][idx]["processing_error"] = None
            name = f"chunk_{idx:03d}.wav"
            if name in j["missing_files"]:
                j["missing_files"].remove(name)
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()
    except: pass

def _mark_job_failed(job_file: Path, reason: str):
    try:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            j["status"] = "failed"
            j["failure_reason"] = reason
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()
    except: pass

FILE: E:\tts_0\routes\infer_kokoro.py

================================================================================

# routes/infer_kokoro.py
"""
Flask inference route for Kokoro TTS with full production-grade features.

Implements:
- Long-text chunking with Kokoro-specific limits
- Multi-attempt generation (3 retries) with Whisper verification
- Live job.json tracking for frontend progress polling
- Robust "##recover##" recovery system
- Cancellation via /kokoro_cancel (also supports legacy /kokoro_stop)
- Per-chunk and final post-processing via audio_post_KOKORO
- Optional conversion to mp3/ogg/flac/m4a
- Base64 return for temp jobs, persistent storage for projects

Fully backward compatible with existing frontend endpoints.
"""
import os, base64
import uuid
from datetime import datetime
import time
import torch
import re
import json
import queue
import subprocess
from pathlib import Path
import numpy as np
import soundfile as sf
from flask import request, jsonify
from . import bp
from save_utils import handle_save
from config import (
    OUTPUT_DIR, VOICE_DIR, PROJECTS_OUTPUT, KOKORO_AUTO_TRIGGER_JOB_RECOVERY_ATTEMPTS,
    FFMPEG_BIN, KOKORO_INTER_PAUSE, KOKORO_FRONT_PAD,
    KOKORO_PADDING_SECONDS, resolve_device
)
import models.kokoro as kokoro_mod
import models.whisper as whisper_mod
from text_utils import split_text_kokoro
from save_utils import handle_save
from audio_post_KOKORO import (
    post_process_kokoro,
    verify_with_whisper, 
    _trim_silence_kokoro
)

def _ts():
    return time.strftime("%H:%M:%S")

cancel_queue = queue.Queue()

def is_cancelled() -> bool:
    try:
        cancel_queue.get_nowait()
        print(f"[{_ts()} KOKORO_INFER] CANCELLATION DETECTED")
        return True
    except queue.Empty:
        return False

@bp.route("/kokoro_status", methods=["GET"])
def kokoro_status():
    loaded = kokoro_mod.model_loaded
    device = kokoro_mod.device_id if loaded else None
    return jsonify({
        "loaded": loaded,
        "device": device,
        "model": "kokoro"
    })


@bp.route("/kokoro_voices", methods=["GET"])
def kokoro_voices():
    ENGLISH_VOICES = [
        "af_heart", "af_alloy", "af_aoede", "af_bella", "af_jessica", "af_kore",
        "af_nicole", "af_nova", "af_river", "af_sarah", "af_sky",
        "am_adam", "am_echo", "am_eric", "am_fenrir", "am_liam", "am_michael",
        "am_onyx", "am_puck", "am_santa"
    ]
    print(f"[{_ts()} KOKORO] Voices request → {len(ENGLISH_VOICES)} English voices")
    return jsonify({"voices": ENGLISH_VOICES})


@bp.route("/kokoro_stop", methods=["POST"]) 
@bp.route("/kokoro_cancel", methods=["POST"])
def kokoro_cancel():
    print(f"[{_ts()} KOKORO_INFER] CANCEL REQUEST RECEIVED")
    cancel_queue.put(True)
    return jsonify({"message": "Kokoro generation cancelled"})


def _ffmpeg_args(fmt: str):
    args = {
        "mp3": ["-c:a", "libmp3lame", "-q:a", "0"],
        "ogg": ["-c:a", "libvorbis", "-q:a", "6"],
        "flac": ["-c:a", "flac", "-compression_level", "12"],
        "m4a": ["-c:a", "aac", "-b:a", "320k"]
    }.get(fmt, [])
    return args

@bp.route("/kokoro_infer", methods=["POST"])
def kokoro_infer():
    print(f"\n{'=' * 100}")
    print(f"[{_ts()} KOKORO_INFER] NEW KOKORO INFERENCE REQUEST")
    print(f"{'=' * 100}")

    d = request.json

    print(f"[{_ts()} KOKORO_INFER] → Voice           : {d.get('voice', 'af_heart')}")
    print(f"[{_ts()} KOKORO_INFER] → Speed           : {float(d.get('speed', 1.0)):.2f}")
    print(f"[{_ts()} KOKORO_INFER] → De-reverb       : {d.get('de_reverb', 70)/100:.2f}")
    print(f"[{_ts()} KOKORO_INFER] → De-ess          : {float(d.get('de_ess', 0))/100:.2f}")
    print(f"[{_ts()} KOKORO_INFER] → Tolerance       : {float(d.get('tolerance', 80))}%")
    print(f"[{_ts()} KOKORO_INFER] → Verify Whisper  : {d.get('verify_whisper', True)}")
    print(f"[{_ts()} KOKORO_INFER] → Output Format   : {d.get('output_format', 'wav')}")
    print(f"[{_ts()} KOKORO_INFER] → Save Path       : {d.get('save_path') or 'temp'}")
    print(f"[{_ts()} KOKORO_INFER] → Device (Kokoro) : {d.get('kokoroDeviceSelect', 'cpu')}")
    print(f"[{_ts()} KOKORO_INFER] → Text Length     : {len(d.get('text',''))} chars")
    print(f"{'-' * 100}")

    raw_text = d.get("text", "").strip()

    # ——————— RECOVERY ———————
    if raw_text.strip().lower() == "##recover##":
        target = (d.get("save_path") or "").strip()
        if not target:
            return jsonify({"message": "Set save_path to folder"}), 400

        job_dir = PROJECTS_OUTPUT / target if "/" not in target and "\\" not in target else Path(target).expanduser().resolve()
        job_file = job_dir / "job.json"
        if not job_file.exists():
            return jsonify({"message": "No job.json found"}), 400

        job_data = json.load(open(job_file, encoding="utf-8"))
        if job_data.get("chunks_completed", 0) >= job_data.get("total_chunks", 0):
            return jsonify({"message": "Job already finished"}), 400

        d = job_data["parameters"].copy()
        text = job_data["input_text"]
        chunks = [c["text"] for c in job_data["chunks"]]
        start_from_chunk = job_data["chunks_completed"]
        save_path_raw = target
        output_format = job_data.get("output_format", "wav").lower()

        print(f"\n{'='*100}")
        print(f"[{_ts()} KOKORO_INFER] RECOVERY MODE — Resuming job: {target}")
        print(f"[{_ts()} KOKORO_INFER] Progress: {start_from_chunk}/{job_data['total_chunks']} chunks")
        print(f"{'='*100}")
        print(f"[{_ts()} KOKORO_INFER] → Voice           : {d.get('voice', 'MISSING')}")
        print(f"[{_ts()} KOKORO_INFER] → Language        : {d.get('language', 'en')}")
        print(f"[{_ts()} KOKORO_INFER] → Temperature     : {d.get('temperature', 0.65):.2f}")
        print(f"[{_ts()} KOKORO_INFER] → Speed           : {d.get('speed', 1.0):.2f}")
        print(f"[{_ts()} KOKORO_INFER] → Repetition Pen  : {d.get('repetition_penalty', 2.0):.3f}")
        print(f"[{_ts()} KOKORO_INFER] → De-ess          : {float(d.get('de_ess', 0))/100:.2f}")
        print(f"[{_ts()} KOKORO_INFER] → De-reverb       : {d.get('de_reverb', 0.7):.2f}")
        print(f"[{_ts()} KOKORO_INFER] → Tolerance       : {float(d.get('tolerance', 80))}%")
        print(f"[{_ts()} KOKORO_INFER] → Verify Whisper  : {d.get('verify_whisper', False)}")
        print(f"[{_ts()} KOKORO_INFER] → Output Format   : {output_format.upper()}")
        print(f"{'-'*100}\n")


    # ——————— NEW JOB ———————
    else:
        text = raw_text
        if not text:
            return jsonify({"error": "Missing text"}), 400
        chunks = split_text_kokoro(text.strip(), max_chars=500)
        start_from_chunk = 0
        save_path_raw = d.get("save_path") or None
        output_format = d.get("output_format", "wav").lower()

    # ——————— JOB DIR & JSON (old correct logic) ———————
    save_path_input = (save_path_raw or "").strip()

    if not save_path_input:
        stem = f"kokoro_{int(time.time())}"
        final_stem = stem
        job_dir = OUTPUT_DIR / f"temp_{stem}"
    else:
        if "/" in save_path_input or "\\" in save_path_input:
            full_path = Path(save_path_input).expanduser().resolve()
            job_dir = full_path.parent if full_path.suffix else full_path
            final_stem = full_path.stem if full_path.suffix else full_path.name
        else:
            job_dir = PROJECTS_OUTPUT / save_path_input
            final_stem = save_path_input
        stem = final_stem                                 # ← make sure stem always exists

    job_dir.mkdir(parents=True, exist_ok=True)
    job_file = job_dir / "job.json"

    if not job_file.exists():
        job_payload = {
            "job_id": str(uuid.uuid4()),
            "model": "kokoro",
            "timestamp": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
            "status": "running",
            "input_text": text,
            "total_chunks": len(chunks),
            "chunks_completed": start_from_chunk,
            "total_duration_sec": None,
            "sample_rate": 24000,
            "output_format": output_format,
            "final_file": None,
            "expected_files": [f"chunk_{i:03d}.wav" for i in range(len(chunks))] + [f"{stem}_final.{output_format}"],
            "missing_files": [f"chunk_{i:03d}.wav" for i in range(start_from_chunk, len(chunks))] + [f"{stem}_final.{output_format}"],
            "chunks": [
                {
                    "index": i,
                    "text": c,
                    "char_length": len(c),
                    "duration_sec": None,
                    "file": f"chunk_{i:03d}.wav",
                    "verification_passed": None,
                    "whisper_transcript": None,
                    "processing_error": None
                }
                for i, c in enumerate(chunks)
            ],
            "parameters": d.copy(), 
            "failure_reason": None  
        }
        with open(job_file, "w", encoding="utf-8") as f:
            json.dump(job_payload, f, ensure_ascii=False, indent=2)

    else:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            if "failure_reason" not in j:
                j["failure_reason"] = None

            j["status"] = "running"
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()




    speed = float(d.get("speed", 1.0))
    voice = d.get("voice", "af_heart")
    max_retries = int(d.get("auto_retry", KOKORO_AUTO_TRIGGER_JOB_RECOVERY_ATTEMPTS))
    tolerance = float(d.get("tolerance", 80))
    # ——————— GENERATION — SINGLE ATTEMPT ———————
    sr = 24000
    audio_parts = []
    try:
        # ← REQUIRED: disables gradients, saves VRAM, speeds up inference
        with torch.no_grad():
            # Load models once (lazy)
            if not kokoro_mod.model_loaded:
                dev = resolve_device(d.get("kokoroDeviceSelect") or "cpu")
                kokoro_mod.load_kokoro(dev)

            verify_whisper = d.get("verify_whisper", False)
            skip_post_process = d.get("skip_post_process", False)

            if verify_whisper:
                dev = resolve_device(d.get("whisperDeviceSelect") or "cpu")
                if whisper_mod.whisper_model is None or whisper_mod._current_device != dev:
                    print(f"[MODEL] verify_whisper=True → loading Whisper on {dev}")
                    whisper_mod.load_whisper(dev)
            else:
                if whisper_mod.whisper_model is not None:
                    print(f"[MODEL] verify_whisper=False → unloading Whisper to free VRAM")
                    whisper_mod.unload_whisper()

            for i in range(start_from_chunk, len(chunks)):
                if is_cancelled():
                    return jsonify({"error": "Cancelled"}), 499

                chunk = chunks[i]
                retry_count = 0

                while True:
                    try:
                        # ——— KOKORO INFERENCE ———
                        gen = kokoro_mod.pipeline(chunk, voice=voice, speed=speed)
                        raw_audio = np.concatenate([c for _, _, c in gen], axis=0)
                        raw_audio = np.concatenate([
                            np.zeros(int(sr * KOKORO_FRONT_PAD), dtype=np.float32),
                            raw_audio
                        ])

                        tmp_raw = OUTPUT_DIR / f"kokoro_raw_{i}_{uuid.uuid4().hex}.wav"
                        sf.write(tmp_raw, raw_audio, sr, subtype="PCM_16")

                        # ——— POST-PROCESSING ———
                        if not skip_post_process:
                            processed = post_process_kokoro(
                                str(tmp_raw),
                                speed,
                                float(d.get("de_reverb", 70)) / 100,
                                float(d.get("de_ess", 0)) / 100
                            )
                        else:
                            processed = str(tmp_raw)
                            _trim_silence_kokoro(processed)

                        # ——— WHISPER VERIFICATION (correct language + tolerance) ———
                        if verify_whisper:
                            if not verify_with_whisper(
                                processed, chunk,
                                d.get("language", "en"),   # ← fixed
                                tolerance,                 # ← fixed
                                job_file, i
                            ):
                                handle_save(processed, None, "kokoro", always_save_fails=True)
                                _record_chunk_error(job_file, i, "Whisper verification failed")
                                raise ValueError("Whisper verification failed")

                        # ——— SUCCESS ———
                        data, _ = sf.read(processed)
                        duration_sec = len(data) / sr
                        audio_parts.append(data)

                        final_chunk = job_dir / f"chunk_{i:03d}.wav"
                        Path(processed).replace(final_chunk)
                        print(f"[{_ts()} KOKORO] {i:03d} → {duration_sec:.2f}s (success)")

                        _update_chunk_success(job_file, i, duration_sec)

                        # Record retry count (optional but nice)
                        try:
                            with open(job_file, "r+", encoding="utf-8") as f:
                                j = json.load(f)
                                j.setdefault("chunk_retry_counts", {})[str(i)] = retry_count
                                f.seek(0)
                                json.dump(j, f, ensure_ascii=False, indent=2)
                                f.truncate()
                        except:
                            pass

                        break  # ← next chunk

                    except Exception as e:
                        retry_count += 1
                        error_msg = str(e) or "Unknown error"

                        if retry_count > max_retries:
                            print(f"[{_ts()} KOKORO FAILED] Chunk {i:03d} failed after {max_retries} retries")
                            _record_chunk_error(job_file, i, f"Permanently failed: {error_msg}")
                            _mark_job_failed(job_file, f"Chunk {i} failed after {max_retries} retries")
                            return jsonify({
                                "error": "generation_failed",
                                "reason": f"Chunk {i} failed after {max_retries} retries",
                                "failed_at_chunk": i,
                                "job_folder": str(job_dir.name),
                                "recover_command": f"##recover## (save_path: {job_dir.name})"
                            }), 200

                        print(f"[{_ts()} KOKORO RETRY] {i:03d} → {retry_count}/{max_retries} failed ({error_msg}) → retrying...")
                        time.sleep(1)

    except Exception as e:
        error_str = str(e) or "Unknown error"
        print(f"[{_ts()} KOKORO FAILED] Unexpected error: {error_str}")
        _mark_job_failed(job_file, error_str)
        return jsonify({
            "error": "generation_failed",
            "reason": error_str,
            "job_folder": str(job_dir.name)
        }), 200
        
    # ——————— FINAL ASSEMBLY ———————
    missing_chunks = [
        f"chunk_{i:03d}.wav" for i in range(len(chunks))
        if not (job_dir / f"chunk_{i:03d}.wav").exists()
    ]

    if missing_chunks:
        print(f"[{_ts()} KOKORO] ASSEMBLY SKIPPED — {len(missing_chunks)} chunk(s) missing")
        try:
            with open(job_file, "r+", encoding="utf-8") as f:
                j = json.load(f)
                j["missing_files"] = missing_chunks + [f"{final_stem}_final.{output_format}"]
                f.seek(0)
                json.dump(j, f, ensure_ascii=False, indent=2)
                f.truncate()
        except: pass

        return jsonify({
            "status": "incomplete",
            "message": f"Waiting for {len(missing_chunks)} missing chunk(s)",
            "missing_count": len(missing_chunks),
            "job_folder": str(job_dir.name),
            "recover_command": f"##recover## (save_path: {job_dir.name})"
        }), 200

    # ——— ALL CHUNKS PRESENT → BUILD FINAL AUDIO ———
    print(f"[{_ts()} KOKORO] All {len(chunks)} chunks ready → assembling...")

    parts = [sf.read(job_dir / f"chunk_{i:03d}.wav")[0] for i in range(len(chunks))]

    inter = np.zeros(int(sr * KOKORO_INTER_PAUSE), dtype=np.float32)
    pad = np.zeros(int(sr * KOKORO_PADDING_SECONDS), dtype=np.float32)
    final_wav = np.concatenate([pad, parts[0], *[np.concatenate([inter, p]) for p in parts[1:]], pad])

    tmp = OUTPUT_DIR / f"final_kokoro_{uuid.uuid4().hex}.wav"
    sf.write(tmp, final_wav, sr, subtype="PCM_16")

    final_path = tmp.with_suffix(f".{output_format}") if output_format != "wav" else tmp
    if output_format != "wav":
        subprocess.run([
            str(FFMPEG_BIN / "ffmpeg.exe"), "-i", str(tmp),
            "-y", str(final_path)
        ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        tmp.unlink()

    final_save = job_dir / f"{final_stem}_final.{output_format}"
    final_path.replace(final_save)

    try:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            j["status"] = "completed"
            j["total_duration_sec"] = round(len(final_wav)/sr, 3)
            j["final_file"] = final_save.name
            j["missing_files"] = []
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()
    except: pass

    resp = {
        "filename": final_save.name,
        "saved_to": str(final_save),
        "saved_rel": str(final_save.relative_to(Path.cwd())).replace("\\", "/"),
        "sample_rate": sr,
        "duration_sec": round(len(final_wav)/sr, 3),
        "format": output_format
    }
    if not save_path_raw:
        resp = {"audio_base64": base64.b64encode(final_save.read_bytes()).decode()}
        final_save.unlink(missing_ok=True)

    print(f"[{_ts()} KOKORO] DONE → {final_save.name}")
    return jsonify(resp)


# === job.json helpers ===
def _record_chunk_error(job_file: Path, idx: int, msg: str):
    try:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            j["chunks"][idx]["processing_error"] = msg
            j["chunks"][idx]["verification_passed"] = False
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()
    except: pass

def _update_chunk_success(job_file: Path, idx: int, duration: float):
    try:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            j["chunks_completed"] = idx + 1
            j["chunks"][idx]["duration_sec"] = round(duration, 3)
            j["chunks"][idx]["verification_passed"] = True
            j["chunks"][idx]["processing_error"] = None
            name = f"chunk_{idx:03d}.wav"
            if name in j["missing_files"]:
                j["missing_files"].remove(name)
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()
    except: pass

def _mark_job_failed(job_file: Path, reason: str):
    try:
        with open(job_file, "r+", encoding="utf-8") as f:
            j = json.load(f)
            j["status"] = "failed"
            j["failure_reason"] = reason
            f.seek(0)
            json.dump(j, f, ensure_ascii=False, indent=2)
            f.truncate()
    except: pass

FILE: E:\tts_0\static\js\production.js

================================================================================

// static/js/production.js

const $ = s => document.querySelector(s);

let currentProjectDir = '';
let currentAudioFile = null;

function setStatus(text, type = 'info') {
    const el = $('#uploadStatus');
    if (el) {
        el.textContent = text;
        el.className = `text-${type} small text-center mt-3`;
    }
}

function resolveProjectDir() {
    const input = $('#projectDir').value.trim();
    if (!input) return '';
    return 'projects_output/' + input.split(/[\\/]/).pop();
}

function getFileUrl(filename) {
    if (currentProjectDir) {
        const full = (currentProjectDir + '\\' + filename).replace(/\\/g, '/');
        return `/file/${encodeURIComponent(filename)}?rel=${encodeURIComponent(full)}`;
    }
    return `/file/${encodeURIComponent(filename)}`;
}

$('#mediaDropZone')?.addEventListener('click', () => $('#mediaInput')?.click());
$('#mediaInput')?.addEventListener('change', () => {
    if ($('#mediaInput').files.length) handleFiles($('#mediaInput').files);
    $('#mediaInput').value = '';
});

['dragover', 'dragenter'].forEach(ev => {
    $('#mediaDropZone')?.addEventListener(ev, e => {
        e.preventDefault();
        $('#mediaDropZone').classList.add('dragover');
    });
});
['dragleave', 'dragend', 'drop'].forEach(ev => {
    $('#mediaDropZone')?.addEventListener(ev, e => {
        e.preventDefault();
        $('#mediaDropZone').classList.remove('dragover');
    });
});
$('#mediaDropZone')?.addEventListener('drop', e => {
    e.preventDefault();
    if (e.dataTransfer.files.length) handleFiles(e.dataTransfer.files);
});


async function handleFiles(files) {
    currentProjectDir = resolveProjectDir();

    const fd = new FormData();
    Array.from(files).forEach(f => fd.append('file', f));
    if (currentProjectDir) fd.append('project_dir', currentProjectDir);

    setStatus(`Uploading ${files.length} file(s)...`, 'info');

    try {
        const r = await fetch('/production/upload_media', { method: 'POST', body: fd });
        const d = await r.json();
        if (!r.ok) throw new Error(d.error || 'Upload failed');

        setStatus(`Uploaded ${files.length} file(s)`, 'success');
        await refreshFileList();

        const audio = Array.from(files).find(f => /\.(wav|mp3|flac|ogg|m4a)$/i.test(f.name));
        if (audio && $('#prod_autoTranscribe')?.checked) {
            currentAudioFile = audio.name;
            $('#mediaSelect').value = currentAudioFile;
            selectMediaFile(currentAudioFile);
            await transcribeCurrent();  // await so it finishes
        }
    } catch (e) {
        setStatus(e.message, 'danger');
    }
}

async function refreshFileList() {
    const dir = currentProjectDir || '';
    const ts = Date.now();
    const [audioRes, imgRes] = await Promise.all([
        fetch(`/production/list_audio?dir=${encodeURIComponent(dir)}&t=${ts}`).then(r => r.json()),
        fetch(`/production/list_images?dir=${encodeURIComponent(dir)}&t=${ts}`).then(r => r.json())
    ]);

    const all = [...new Set([...audioRes, ...imgRes])].sort();
    $('#mediaSelect').innerHTML = all.map(f => `<option value="${f}">${f}</option>`).join('');
}

$('#mediaSelect')?.addEventListener('change', () => {
    const f = $('#mediaSelect').value;
    if (!f) return;
    selectMediaFile(f);

    if (/\.(wav|mp3|flac|ogg|m4a)$/i.test(f) && $('#prod_autoTranscribe')?.checked) {
        transcribeCurrent();
    }
});

function selectMediaFile(filename) {
    const url = getFileUrl(filename);
    const isVideo = /\.(mp4|webm|mov|avi|mkv)$/i.test(filename);
    const isAudio = /\.(wav|mp3|flac|ogg|m4a)$/i.test(filename);

    $('#previewName').textContent = filename;

    if (isVideo) {
        $('#previewArea').innerHTML = `<video controls loop autoplay class="img-fluid rounded shadow" style="max-height:100%;max-width:100%"><source src="${url}"></video>`;
    } else if (isAudio) {
        $('#previewArea').innerHTML = `
            <audio controls class="w-100 mb-3"><source src="${url}"></audio>
            <div class="text-center text-muted small">Selected audio — will be used for final video</div>`;
        currentAudioFile = filename;
        $('#transcribeBtn').disabled = false;
        $('#createVideoBtn').disabled = false;
    } else {
        $('#previewArea').innerHTML = `<img src="${url}" class="img-fluid rounded shadow" style="max-height:100%">`;
    }
}

$('#loadProjectBtn')?.addEventListener('click', () => {
    currentProjectDir = resolveProjectDir().replace(/\\+$/, '');
    refreshFileList();
    $('#previewArea').innerHTML = '<small class="text-muted">Click a file to preview</small>';
    $('#previewName').textContent = '';
    currentAudioFile = null;
    $('#transcribeBtn').disabled = true;
    $('#createVideoBtn').disabled = true;
    setStatus(currentProjectDir ? `Project: ${currentProjectDir}` : 'Temporary mode');
});

async function transcribeCurrent() {
    if (!currentAudioFile) return;

    $('#transcribeBtn').disabled = true;
    setStatus('Checking for existing transcription...', 'info');

    // Step 1: Fast cache check first
    try {
        const statusRes = await fetch('/production/transcribe_status', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ filename: currentAudioFile, project_dir: currentProjectDir })
        });
        const statusData = await statusRes.json();

        if (statusData.cached) {
            // Cache exists → load the text from the existing JSON (fast)
            const r = await fetch('/production/transcribe', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ filename: currentAudioFile, project_dir: currentProjectDir })
            });
            const d = await r.json();
            $('#transcriptionText').value = (d.text || '').trim();
            setStatus('Transcription loaded from cache', 'success');

            if ($('#prod_autoMakeVideo')?.checked) {
                setTimeout(() => $('#createVideoBtn').click(), 800);
            }
            return;
        }
    } catch (e) {
        console.warn("Cache check failed, continuing...", e);
    }

    setStatus('No cache found → ensuring Whisper is ready...', 'info');

    let whisperReady = false;
    try {
        const status = await fetch('/whisper_status').then(r => r.json());
        if (status.loaded) {
            console.log("[WHISPER] Already loaded in memory");
            setStatus('Whisper already loaded', 'success');
            whisperReady = true;
        }
    } catch (e) {}

    if (!whisperReady) {
        setStatus('Loading Whisper model...', 'info');
        const device = $('#whisperDeviceSelect').value;

        try {
            const r = await fetch('/whisper_load', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ device })
            });
            if (!r.ok) throw new Error();
            console.log("[WHISPER] Load request sent successfully");
            await new Promise(r => setTimeout(r, 800)); // give server a moment
        } catch (e) {
            setStatus('GPU failed → falling back to CPU', 'warning');
            await fetch('/whisper_load', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ device: 'cpu' })
            });
        }
    }

    setStatus('Transcribing audio...', 'info');
    try {
        const r = await fetch('/production/transcribe', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ filename: currentAudioFile, project_dir: currentProjectDir })
        });
        const d = await r.json();
        if (!r.ok) throw new Error(d.error || 'Failed');

        $('#transcriptionText').value = d.text.trim();
        setStatus('Transcription complete', 'success');

        if ($('#prod_autoMakeVideo')?.checked) {
            setTimeout(() => $('#createVideoBtn').click(), 800);
        }
    } catch (e) {
        setStatus('Transcription failed: ' + e.message, 'danger');
    } finally {
        $('#transcribeBtn').disabled = false;
    }
}

$('#transcribeBtn')?.addEventListener('click', transcribeCurrent);
$('#createVideoBtn')?.addEventListener('click', async () => {
    if (!currentAudioFile) return;
    $('#createVideoBtn').disabled = true;
    setStatus('Creating video...', 'info');

    // Parse the single dropdown value
    const bgValue = $('#bgModeSelect').value;    
    const [bg_mode, chroma_color] = bgValue.includes('|') 
        ? bgValue.split('|') 
        : [bgValue, 'red'];     

    const payload = {
        audio_file: currentAudioFile,
        project_dir: currentProjectDir,
        resolution: $('#resolution').value,
        bg_mode: bg_mode,    
        chroma_color: chroma_color  
    };

    try {
        const r = await fetch('/production/make_video', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(payload)
        });
        const d = await r.json();
        if (!r.ok) throw new Error(d.error || 'Failed');

        const url = getFileUrl(d.video_file);
        $('#resultVideo').src = url;
        $('#downloadLink').href = url;
        $('#downloadLink').download = d.video_file;
        $('#videoResult').style.display = 'block';
        setStatus('Video ready!', 'success');
        window.scrollTo({top: document.body.scrollHeight, behavior: 'smooth'});
    } catch (e) {
        setStatus('Video failed: ' + e.message, 'danger');
    } finally {
        $('#createVideoBtn').disabled = false;
    }
});


$('#folderLabel')?.addEventListener('click', () => $('#folderPicker').click());
$('#folderPicker')?.addEventListener('change', e => {
    const files = e.target.files;
    if (!files.length) return;

    const diskPath = files[0].path;
    if (diskPath && diskPath.includes('\\')) {
        $('#projectDir').value = diskPath.split('\\').slice(0, -1).join('\\');
    } else {
        $('#projectDir').value = files[0].webkitRelativePath.split('/')[0];
    }

    $('#loadProjectBtn').click();
    $('#folderPicker').value = '';
});

$('#transcribeBtn').disabled = true;
$('#createVideoBtn').disabled = true;
setStatus('Ready — drop files or scan folder');

fetch('/whisper_status')
    .then(r => r.json())
    .then(data => {
        if (data.loaded && $('#whisperStatusBadge')) {
            $('#whisperStatusBadge').className = 'badge bg-success';
            $('#whisperStatusBadge').textContent = 'Whisper ready';
        }
    })
    .catch(() => {});

FILE: E:\tts_0\static\js\app.js

================================================================================

// static/js/app.js

document.addEventListener("DOMContentLoaded", () => {
  const path = window.location.pathname;

  if (path === "/production") {
    import("./js/production.js");
    return;
  }

  document.querySelectorAll('.row-toggle').forEach(toggle => {
    toggle.addEventListener('click', () => {
      toggle.parentElement.classList.toggle('collapsed');
    });
  });

  if (path === "/" || path === "/index.html" || path === "") {
    Promise.all([
      import("./modules/ui-helpers.js"),
      import("./modules/upload.js"),
      import("./modules/settings.js"),
      import("./modules/model-xtts.js"),
      import("./modules/generate-xtts.js"),
      import("./modules/model-fish.js"),
      import("./modules/generate-fish.js"),
      import("./modules/model-kokoro.js"),
      import("./modules/generate-kokoro.js"),
      import("./modules/model-stable-audio.js"),
      import("./modules/generate-stable-audio.js"),
      import("./modules/model-ace-step.js"),
      import("./modules/generate-ace-step.js"),
      import("./modules/model-whisper.js"),
      import("./modules/chatbot.js"),
    ]).then(modules => {
      const [
        ui, upload, settings,
        xttsModel, xttsGen,
        fishModel, fishGen,
        kokoroModel, kokoroGen,
        saModel, saGen,
        aceModel, aceGen,
        whisper, chatbot
      ] = modules;

      settings.initSettings();
      ui.initUIHelpers();
      upload.initUpload();

      xttsModel.initXTTSModel();
      xttsModel.setMode("cloned");
      xttsGen.initXTTSGenerate();

      fishModel.initFishModel();
      fishGen.initFishGenerate();

      kokoroModel.initKokoroModel();
      kokoroGen.initKokoroGenerate();

      saModel.initStableAudioModel();
      saGen.initStableAudioGenerate();

      aceModel.initAceStepModel();
      aceGen.initAceStepGenerate();

      whisper.initWhisperModel();
      chatbot.initChatbot();

      if (document.querySelector('#mediaDropZone')) {
        import("./production.js");
      }

      document.addEventListener('click', e => {
        const gearBtn = e.target.closest('.show-settings');
        const backBtn = e.target.closest('.show-main');
        const leftSlider = e.target.closest('.card')?.querySelector('.settings-slider');
        if (gearBtn && leftSlider) leftSlider.classList.add('show-settings');
        if (backBtn && leftSlider) leftSlider.classList.remove('show-settings');

        const xtts   = document.querySelector('.xtts-center-slider');
        const fish   = document.querySelector('.fish-center-slider');
        const kokoro = document.querySelector('.kokoro-center-slider');
        const stable = document.querySelector('.stable-center-slider');
        const ace    = document.querySelector('.ace-center-slider');
        const upload = document.querySelector('.upload-center-slider');

        if (e.target.closest('.show-xtts-api'))        xtts?.classList.add('show-api');
        if (e.target.closest('.hide-xtts-api'))        xtts?.classList.remove('show-api');
        if (e.target.closest('.show-fish-api'))        fish?.classList.add('show-api');
        if (e.target.closest('.hide-fish-api'))        fish?.classList.remove('show-api');
        if (e.target.closest('.show-kokoro-api'))      kokoro?.classList.add('show-api');
        if (e.target.closest('.hide-kokoro-api'))      kokoro?.classList.remove('show-api');
        if (e.target.closest('.show-stable-api'))      stable?.classList.add('show-api');
        if (e.target.closest('.hide-stable-api'))      stable?.classList.remove('show-api');
        if (e.target.closest('.show-ace-api'))         ace?.classList.add('show-api');
        if (e.target.closest('.hide-ace-api'))         ace?.classList.remove('show-api');
        if (e.target.closest('.show-upload-audio-files')) upload?.classList.add('show-api');
        if (e.target.closest('.hide-upload-audio-files')) upload?.classList.remove('show-api');
      });

      document.getElementById('copyTranscriptionBtn')?.addEventListener('click', async () => {
        const text = document.getElementById('transcriptionOutput')?.value.trim();
        if (!text) return;
        try {
          await navigator.clipboard.writeText(text);
          const btn = document.getElementById('copyTranscriptionBtn');
          const old = btn.innerHTML;
          btn.innerHTML = 'Copied!';
          btn.classList.replace('btn-outline-light', 'btn-success');
          setTimeout(() => {
            btn.innerHTML = old;
            btn.classList.replace('btn-success', 'btn-outline-light');
          }, 1500);
        } catch (err) {
          alert('Copy failed');
        }
      });

    });
  }
});

FILE: E:\tts_0\static\js\modules\ui-helpers.js

================================================================================

// static/js/modules/ui-helpers.js
export function initUIHelpers() {
  // ── STOP BUTTON 
  const stopBtn = document.getElementById("stopBtn");
  if (stopBtn) {
    stopBtn.onclick = () => fetch("/cancel", { method: "POST" })
      .then(() => {
        const genStatus = document.getElementById("genStatus");
        const fishStatus = document.getElementById("fishGenStatus");
        if (genStatus) genStatus.textContent = "Stopped";
        if (fishStatus) fishStatus.textContent = "Stopped";
        stopBtn.disabled = true;
      });
  }

  // ── SHUTDOWN BUTTON (you kept this) ─────────────────────
  const shutdownBtn = document.getElementById("shutdownBtn");
  if (shutdownBtn) {
    shutdownBtn.onclick = () => fetch("/shutdown", { method: "POST" });
  }

  // ── CLEAR BUTTON (you probably removed this) ─────────────
  const clearBtn = document.getElementById("clearBtn");
  if (clearBtn) {
    clearBtn.onclick = () => fetch("/clear_output", { method: "POST" });
  }

  // ── REFRESH ALL VOICES BUTTON (you removed this) ────────
  const refreshAllBtn = document.getElementById("refreshAllBtn");
  if (refreshAllBtn) {
    refreshAllBtn.onclick = () => {
      import("./model-xtts.js").then(m => m.refreshAllVoices?.());
      import("./model-fish.js").then(m => m.refreshFishVoices?.());
    };
  }

  // ── SLIDERS (always safe) ─────────────────────────────────
  const tolerance = document.getElementById("tolerance");
  const tolVal = document.getElementById("tolVal");
  if (tolerance && tolVal) tolerance.oninput = e => tolVal.textContent = (+e.target.value).toFixed(2);

  const deReverb = document.getElementById("deReverb");
  const reverbVal = document.getElementById("reverbVal");
  if (deReverb && reverbVal) deReverb.oninput = e => reverbVal.textContent = e.target.value;

  const fishTolerance = document.getElementById("fishTolerance");
  const fishTolVal = document.getElementById("fishTolVal");
  if (fishTolerance && fishTolVal) fishTolerance.oninput = e => fishTolVal.textContent = (+e.target.value).toFixed(2);

  const fishDeReverb = document.getElementById("fishDeReverb");
  const fishReverbVal = document.getElementById("fishReverbVal");
  if (fishDeReverb && fishReverbVal) fishDeReverb.oninput = e => fishReverbVal.textContent = e.target.value;
}

FILE: E:\tts_0\static\js\modules\settings.js

================================================================================

// static/js/modules/settings.js  ←  FINAL, COMPLETE, WORKING VERSION
let currentPreset = "";

export function initSettings() {
  const select = document.getElementById("presetSelect");
  const nameInput = document.getElementById("presetNameInput");
  const saveBtn = document.getElementById("saveSettingsBtn");

  if (!select || !nameInput || !saveBtn) return;

  // AUTO-LOAD FIRST PRESET ON STARTUP
  refreshPresets().then(() => {
    const presets = Array.from(select.options).slice(1).map(o => o.value).filter(Boolean);
    if (presets.length > 0) {
      const first = presets.sort()[0];  // alphabetical first
      select.value = first;
      nameInput.value = first;
      currentPreset = first;
      loadPreset(first);
    }
  });

  // Load when user selects from dropdown
  select.onchange = () => {
    const name = select.value;
    if (!name) {
      nameInput.value = "";
      currentPreset = "";
      return;
    }
    nameInput.value = name;
    currentPreset = name;
    loadPreset(name);
  };

  // Save / Overwrite / Save As
  saveBtn.onclick = () => {
    let name = nameInput.value.trim();
    if (!name) return;

    const settings = captureAllSettings();
    if (Object.keys(settings).length === 0) return;

    fetch("/settings/save", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ name, settings })
    })
    .then(() => {
      currentPreset = name;
      refreshPresets().then(() => {
        select.value = name;  // instantly select the one just saved
      });
    });
  };
}

function refreshPresets() {
  const select = document.getElementById("presetSelect");
  return fetch("/settings/list")
    .then(r => r.json())
    .then(d => {
      const sorted = d.presets.length ? d.presets.sort() : [];
      select.innerHTML = `<option value="">— New preset —</option>` +
        sorted.map(p => `<option value="${p}">${p}</option>`).join("");
      return sorted;
    });
}

function loadPreset(name) {
  fetch("/settings/load", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ name })
  })
  .then(r => r.json())
  .then(d => {
    if (d.settings) applySettings(d.settings);
  });
}

// YOUR ORIGINAL FUNCTIONS — UNCHANGED & WORKING PERFECTLY
function captureAllSettings() {
  const settings = {};

  document.querySelectorAll("input[id], textarea[id], select[id]").forEach(el => {
    const id = el.id;
    if (!id) return;

    if (el.type === "checkbox") {
      settings[id] = el.checked;
    } else if (el.type === "radio" && el.checked) {
      settings[el.name] = el.value;
    } else {
      settings[id] = el.value;
    }
  });

  document.querySelectorAll("input[type=range]").forEach(slider => {
    const valEl = document.getElementById(slider.id + "Val");
    if (valEl) settings[slider.id + "_display"] = valEl.textContent;
  });

  return settings;
}

function applySettings(settings) {
  Object.keys(settings).forEach(key => {
    const el = document.getElementById(key);
    if (!el) {
      if (key.endsWith("_display")) {
        const baseId = key.replace("_display", "");
        const span = document.getElementById(baseId + "Val");
        if (span) span.textContent = settings[key];
      }
      return;
    }

    if (el.type === "checkbox") {
      el.checked = !!settings[key];
    } else if (el.type === "radio") {
      const radio = document.querySelector(`input[name="${el.name}"][value="${settings[key]}"]`);
      if (radio) radio.checked = true;
    } else {
      el.value = settings[key] ?? "";
    }

    if (el.type === "range") {
      const valEl = document.getElementById(el.id + "Val");
      if (valEl) {
        const displayKey = el.id + "_display";
        valEl.textContent = displayKey in settings ? settings[displayKey] : settings[key];
      }
    }
  });
}

FILE: E:\tts_0\static\js\modules\chatbot-core.js

================================================================================

// static/js/modules/chatbot-core.js
// Updated: all 6 sliders are now sent to every backend (local / lmstudio / openrouter)

let messages = [];
let currentBackend = "local";

export function getCurrentBackend() { return currentBackend; }
export function setCurrentBackend(b) { currentBackend = b; }

export function initChatbotCore() {
  loadBrain();

  const input = document.getElementById("llamaInput");
  const sendBtn = document.getElementById("sendLlamaBtn");

  sendBtn.onclick = async () => {
    const text = input.value.trim();
    if (!text) return;

    const loaded = await ensureModelLoaded();
    if (!loaded) {
      sendBtn.disabled = false;
      return;
    }

    messages.push({ role: "user", content: text });
    renderChat();
    saveHistory();
    input.value = "";
    sendBtn.disabled = true;

    const assistantMsg = { role: "assistant", content: "" };
    messages.push(assistantMsg);
    renderChat();

    // === READ ALL 6 SLIDERS ONCE ===
    const temperature       = parseFloat(document.getElementById("temperature_chat").value) || 0.8;
    const max_tokens        = parseInt(document.getElementById("max_tokens_chat").value) || 8192;
    const top_p             = parseFloat(document.getElementById("top_p_chat").value) || 0.95;
    const top_k             = parseInt(document.getElementById("top_k_chat").value) || 40;
    const presence_penalty  = parseFloat(document.getElementById("presence_penalty_chat").value) || 0.0;
    const frequency_penalty = parseFloat(document.getElementById("frequency_penalty_chat").value) || 0.0;

const currentSystemPrompt = await (async () => {
      try {
        const res = await fetch("/chatbot/brain/system_prompt");
        const data = await res.json();
        return data.content?.trim() || "You are a helpful assistant.";
      } catch (e) {
        console.warn("System prompt fetch failed, using fallback");
        return "You are a helpful assistant.";
      }
    })();

    const payloadMessages = [
      { role: "system", content: currentSystemPrompt },
      ...messages.filter(m => m.role !== "system").slice(0, -1)
    ];

    let endpoint = "";
    let requestBody = {
      messages: payloadMessages,
      temperature: temperature,
      max_tokens: max_tokens,
      top_p: top_p,
      top_k: top_k,
      presence_penalty: presence_penalty,
      frequency_penalty: frequency_penalty
    };

    if (currentBackend === "local") {
      endpoint = "/chatbot/infer";
    } else if (currentBackend === "lmstudio") {
      endpoint = "/lmstudio/infer";
    } else if (currentBackend === "openrouter") {
      endpoint = "/openrouter/infer";
      requestBody.model = document.getElementById("openrouterModelSelect").value || "openrouter/auto";
    }

    fetch(endpoint, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(requestBody)
    })
    .then(r => {
      if (!r.ok) throw new Error(`HTTP ${r.status}`);
      const reader = r.body.getReader();
      const decoder = new TextDecoder();
      function read() {
        reader.read().then(({ done, value }) => {
          if (done) {
            saveHistory();
            sendBtn.disabled = false;
            return;
          }
          assistantMsg.content += decoder.decode(value, { stream: true });
          renderChat();
          read();
        });
      }
      read();
    })
    .catch(err => {
      console.error("[INFER] Failed:", err);
      messages.pop();
      renderChat();
      alert("Generation failed – check model is loaded");
      sendBtn.disabled = false;
    });
  };

  input.addEventListener("keydown", e => {
    if (e.key === "Enter" && !e.shiftKey) {
      e.preventDefault();
      sendBtn.click();
    }
  });

  // ——— UI controls (unchanged) ———
  document.getElementById("saveHistoryBtn").onclick = async () => {
    if (messages.length <= 1) return alert("Nothing to save.");
    const name = sanitize(messages.find(m => m.role === "user")?.content || "chat");
    await fetch("/chatbot/brain/save_archive", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ filename: name, history: messages.filter(m => m.role !== "system") })
    });
    messages = [messages[0]];
    saveHistory();
    renderChat();
    refreshSavedList();
  };

  document.getElementById("deleteHistoryBtn").onclick = () => {
    if (confirm("Delete current chat? This cannot be undone.")) {
      messages = [messages[0]];
      saveHistory();
      renderChat();
    }
  };

document.getElementById("savedHistorySelect").onchange = async () => {
  const select = document.getElementById("savedHistorySelect");
  const file = select.value;

  // Ignore the placeholder option
  if (!file) {
    return;
  }

  try {
    const res = await fetch(`/chatbot/brain/load_archive?file=${file}`);
    if (!res.ok) throw new Error("Not found");
    const data = await res.json();

    if (data.history) {
      messages = [messages[0], ...data.history];  // keep system prompt
      renderChat();
      saveHistory();
    }
  } catch (e) {
    console.error("Failed to load archive:", e);
    alert("Could not load that chat");
  } finally {
    // This is the ONLY change you need:
    select.options[0].selected = true;   // instead of .value = ""
    // This keeps the placeholder text visible!
  }
};


}

async function loadBrain() {
  try {
    const sysRes = await fetch("/chatbot/brain/system_prompt");
    const system = await sysRes.json();
    messages = [{ role: "system", content: system.content || "You are a helpful assistant." }];

    const histRes = await fetch("/chatbot/brain/history");
    const history = await histRes.json();
    if (Array.isArray(history)) messages.push(...history);

    console.log("[SHARED] Brain loaded – messages:", messages.length);
  } catch (e) {
    console.error("[SHARED] Brain load failed:", e);
    messages = [{ role: "system", content: "You are a helpful assistant." }];
  }
  renderChat();
  refreshSavedList();
}

function saveHistory() {
  const clean = messages.filter(m => m.role !== "system");
  fetch("/chatbot/brain/history", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(clean)
  }).catch(() => {});
}

function renderChat() {
  const chatHistory = document.getElementById("chatHistory");
  chatHistory.innerHTML = "";
  messages.forEach(m => {
    if (m.role === "system") return;
    const div = document.createElement("div");
    div.className = `mb-3 p-3 rounded position-relative ${m.role === "user" ? "bg-primary text-white ms-auto" : "bg-secondary text-white"}`;
    div.style.maxWidth = "85%";
    const cleanText = m.content.replace(/\n/g, "<br>");
    div.innerHTML = `
      <strong>${m.role === "user" ? "You" : "Assistant"}</strong><br>
      <div class="message-text">${cleanText}</div>
    `;

    if (m.role === "assistant") {
      const buttonRow = document.createElement("div");
      buttonRow.className = "mt-2 d-flex flex-wrap gap-1";
      buttonRow.style.fontSize = "0.75rem";

        const buttons = [
          { label: "XTTS", icon: "🎙️", target: "#textInput" },
          { label: "Fish", icon: "🐟", target: "#fishTextInput" },
          { label: "Kokoro", icon: "❤️", target: "#kokoroTextInput" },
          { label: "Stable", icon: "🎵", target: "#stablePrompt" },
          { label: "ACE", icon: "⚡", target: "#acePrompt" }
        ];

      buttons.forEach(btn => {
        const b = document.createElement("button");
        b.type = "button";
        b.className = "btn btn-outline-light btn-sm py-0 px-2";
        b.innerHTML = `${btn.icon} ${btn.label}`;
        b.title = `Send to ${btn.label}`;
        b.onclick = e => {
          e.stopPropagation();
          const target = document.querySelector(btn.target);
          if (target) {
            target.value = m.content.trim();
            target.dispatchEvent(new Event('input', { bubbles: true }));
            target.focus();
          }
        };
        buttonRow.appendChild(b);
      });
      div.appendChild(buttonRow);
    }
    chatHistory.appendChild(div);
  });
  chatHistory.scrollTop = chatHistory.scrollHeight;
}

function sanitize(str) {
  return (str || "").replace(/[^a-zA-Z0-9\s]/g, "").replace(/\s+/g, "_").substring(0, 40) || "chat";
}

async function refreshSavedList() {
  try {
    const res = await fetch("/chatbot/brain/list_archives");
    const files = await res.json();
    document.getElementById("savedHistorySelect").innerHTML = `<option>— Load old chat —</option>` +
      files.map(f => `<option value="${f}">${f.replace(".json", "")}</option>`).join("");
  } catch (e) {
    console.error("[ARCHIVE] List failed:", e);
  }
}

async function ensureModelLoaded() {
  if (currentBackend === "local") {
    const status = await fetch("/chatbot/status").then(r => r.json()).catch(() => ({ loaded: false }));
    if (!status.loaded) {
      const modelPath = document.getElementById("llamaModelSelect").value;
      if (!modelPath) {
        alert("Please select and load a local model first!");
        return false;
      }
      await fetch("/chatbot/load", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          model_path: modelPath,
          n_ctx: +document.getElementById("llamaCtx").value,
          n_gpu_layers: document.getElementById("llamaLayers").value === "99" ? -1 : +document.getElementById("llamaLayers").value
        })
      });
    }
  }

  // LM STUDIO: Do NOTHING — just trust whatever is loaded in the app
  // OpenRouter: No load needed

  return true;
}

FILE: E:\tts_0\static\js\modules\backend-lmstudio.js

================================================================================

// static/js/modules/chatbot/backend-lmstudio.js

let lmstudioSetupDone = false;
let pollInterval = null;

export function setupLMStudioBackend() {
  if (lmstudioSetupDone) return;
  lmstudioSetupDone = true;

  const statusBadge = document.getElementById("lmstudioStatusBadge");
  const currentModelDiv = document.getElementById("lmstudioCurrentModel");

  const updateStatus = async () => {
    try {
      const res = await fetch("/lmstudio/status");
      const data = await res.json();

      if (data.loaded && data.model && data.model !== "—") {
        statusBadge.textContent = "Loaded";
        statusBadge.className = "badge bg-success fs-6";
        currentModelDiv.textContent = data.model;
      } else {
        statusBadge.textContent = "No model loaded";
        statusBadge.className = "badge bg-secondary fs-6";
        currentModelDiv.textContent = "Load in LM Studio";
      }
    } catch {
      statusBadge.textContent = "LM Studio offline";
      statusBadge.className = "badge bg-danger fs-6";
      currentModelDiv.textContent = "Start server";
    }
  };

  const startPolling = () => {
    if (pollInterval) clearInterval(pollInterval);
    updateStatus(); // immediate update
    pollInterval = setInterval(updateStatus, 4000);
  };

  const stopPolling = () => {
    if (pollInterval) {
      clearInterval(pollInterval);
      pollInterval = null;
    }
  };

  // Check current backend on load
  if (document.getElementById("llmBackendSelect").value === "lmstudio") {
    startPolling();
  }

  // React to backend changes
  document.getElementById("llmBackendSelect").addEventListener("change", (e) => {
    if (e.target.value === "lmstudio") {
      startPolling();
    } else {
      stopPolling();
    }
  });
}

